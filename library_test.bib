%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Feng Yu at 2022-02-01 10:15:16 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{carpenter2000bootstrap,
	author = {Carpenter, James and Bithell, John},
	date-added = {2022-01-26 16:21:27 -0500},
	date-modified = {2022-01-26 16:22:14 -0500},
	journal = {Statistics in medicine},
	keywords = {bootstrap, confidence interval},
	number = {9},
	pages = {1141--1164},
	publisher = {Wiley Online Library},
	title = {Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians},
	volume = {19},
	year = {2000},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxDBLi4vTGlicmFyeS9DbG91ZFN0b3JhZ2UvT25lRHJpdmUtWW91bmdzdG93blN0YXRlVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvQ2FycGVudGVyIC0gMjAwMCAtIEJvb3RzdHJhcCBjb25maWRlbmNlIGludGVydmFscyB3aGVuLCB3aGljaCwgd2hhdCBBIHByYWN0aWNhbCBndWlkZSBmb3IgbWVkaWNhbCBzdGF0aXN0aWNpYW5zLnBkZk8RA1IAAAAAA1IAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9DYXJwZW50ZXIgLSAyMDAwIC0jRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABwAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAMovOlVzZXJzOmZ5dTpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Zb3VuZ3N0b3duU3RhdGVVbml2ZXJzaXR5OmRvYzpteV9saWJyYXJ5OnBkZjpDYXJwZW50ZXIgLSAyMDAwIC0gQm9vdHN0cmFwIGNvbmZpZGVuY2UgaW50ZXJ2YWxzIHdoZW4sIHdoaWNoLCB3aGF0IEEgcHJhY3RpY2FsIGd1aWRlIGZvciBtZWRpY2FsIHN0YXRpc3RpY2lhbnMucGRmAA4A6ABzAEMAYQByAHAAZQBuAHQAZQByACAALQAgADIAMAAwADAAIAAtACAAQgBvAG8AdABzAHQAcgBhAHAAIABjAG8AbgBmAGkAZABlAG4AYwBlACAAaQBuAHQAZQByAHYAYQBsAHMAIAB3AGgAZQBuACwAIAB3AGgAaQBjAGgALAAgAHcAaABhAHQAIABBACAAcAByAGEAYwB0AGkAYwBhAGwAIABnAHUAaQBkAGUAIABmAG8AcgAgAG0AZQBkAGkAYwBhAGwAIABzAHQAYQB0AGkAcwB0AGkAYwBpAGEAbgBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgDIVXNlcnMvZnl1L0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVlvdW5nc3Rvd25TdGF0ZVVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL0NhcnBlbnRlciAtIDIwMDAgLSBCb290c3RyYXAgY29uZmlkZW5jZSBpbnRlcnZhbHMgd2hlbiwgd2hpY2gsIHdoYXQgQSBwcmFjdGljYWwgZ3VpZGUgZm9yIG1lZGljYWwgc3RhdGlzdGljaWFucy5wZGYAEwABLwAAFQACAAr//wAAAAgADQAaACQA6AAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAQ+}}

@inproceedings{cal-sede21,
	//bibsource = {EasyChair, https://easychair.org},
	//editor = {Frederick Harris and Rui Wu and Alex Redei},
	//publisher = {EasyChair},
	//url = {https://easychair.org/publications/paper/v1b5},
	author = {Semih Cal and En Cheng and Feng Yu},
	booktitle = {Proc. of ISCA 30th International Conference on Software Engineering and Data Engineering},
	date-added = {2022-01-10 15:15:14 -0500},
	date-modified = {2022-01-10 15:16:39 -0500},
	doi = {10.29007/bkw9},
	issn = {2398-7340},
	keywords = {database;bootstrap;sampling},
	pages = {144--153},
	title = {Optimized Bootstrap Sampling for -{AQP} Error Estimation: A Pilot Study},
	volume = {77},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.29007/bkw9}}

@inproceedings{haas-97-sigmod,
	author = {Haas, P.J.},
	booktitle = {Proceedings. Ninth International Conference on Scientific and Statistical Database Management (Cat. No.97TB100150)},
	date-added = {2022-01-10 13:59:49 -0500},
	date-modified = {2022-01-10 14:17:59 -0500},
	doi = {10.1109/SSDM.1997.621151},
	keywords = {database;sampling;confidence interval;aqp},
	pages = {51-62},
	title = {Large-sample and deterministic confidence intervals for online aggregation},
	year = {1997},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxEBAi4uL0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL2hhYXMgLSA5NyAtIExhcmdlLXNhbXBsZSBhbmQgZGV0ZXJtaW5pc3RpYyBjb25maWRlbmNlIGludGVydmFscyBmb3Igb25saW5lIGFnZ3JlZ2F0aW9uLnBkZk8RA6QAAAAAA6QAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9oYWFzIC0gOTcgLSBMYXJnZS0jRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEACQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAQsvOlVzZXJzOmZ5dTpMaWJyYXJ5Okdyb3VwIENvbnRhaW5lcnM6VUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZTpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4Ok9uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5OmRvYzpteV9saWJyYXJ5OnBkZjpoYWFzIC0gOTcgLSBMYXJnZS1zYW1wbGUgYW5kIGRldGVybWluaXN0aWMgY29uZmlkZW5jZSBpbnRlcnZhbHMgZm9yIG9ubGluZSBhZ2dyZWdhdGlvbi5wZGYAAA4AtgBaAGgAYQBhAHMAIAAtACAAOQA3ACAALQAgAEwAYQByAGcAZQAtAHMAYQBtAHAAbABlACAAYQBuAGQAIABkAGUAdABlAHIAbQBpAG4AaQBzAHQAaQBjACAAYwBvAG4AZgBpAGQAZQBuAGMAZQAgAGkAbgB0AGUAcgB2AGEAbABzACAAZgBvAHIAIABvAG4AbABpAG4AZQAgAGEAZwBnAHIAZQBnAGEAdABpAG8AbgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIBCVVzZXJzL2Z5dS9MaWJyYXJ5L0dyb3VwIENvbnRhaW5lcnMvVUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZS9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4L09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5L2RvYy9teV9saWJyYXJ5L3BkZi9oYWFzIC0gOTcgLSBMYXJnZS1zYW1wbGUgYW5kIGRldGVybWluaXN0aWMgY29uZmlkZW5jZSBpbnRlcnZhbHMgZm9yIG9ubGluZSBhZ2dyZWdhdGlvbi5wZGYAABMAAS8AABUAAgAK//8AAAAIAA0AGgAkASoAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAE0g==},
	bdsk-url-1 = {https://doi.org/10.1109/SSDM.1997.621151}}

@inproceedings{shi2021time,
	author = {Shi, Benwei and Zhao, Zhuoyue and Peng, Yanqing and Li, Feifei and Phillips, Jeff M},
	booktitle = {Proceedings of the 2021 International Conference on Management of Data},
	date-added = {2022-01-10 07:36:41 -0500},
	date-modified = {2022-01-10 07:36:41 -0500},
	pages = {1623--1636},
	title = {At-the-time and Back-in-time Persistent Sketches},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBILi4vRG9jdW1lbnRzL1BhcGVycy9BdC10aGUtdGltZSBhbmQgQmFjay1pbi10aW1lIFBlcnNpc3RlbnQgU2tldGNoZXMucGRmTxEB5gAAAAAB5gACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0F0LXRoZS10aW1lIGFuZCBCYSNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgBRLzpVc2VyczpmeXU6RG9jdW1lbnRzOlBhcGVyczpBdC10aGUtdGltZSBhbmQgQmFjay1pbi10aW1lIFBlcnNpc3RlbnQgU2tldGNoZXMucGRmAAAOAGoANABBAHQALQB0AGgAZQAtAHQAaQBtAGUAIABhAG4AZAAgAEIAYQBjAGsALQBpAG4ALQB0AGkAbQBlACAAUABlAHIAcwBpAHMAdABlAG4AdAAgAFMAawBlAHQAYwBoAGUAcwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAT1VzZXJzL2Z5dS9Eb2N1bWVudHMvUGFwZXJzL0F0LXRoZS10aW1lIGFuZCBCYWNrLWluLXRpbWUgUGVyc2lzdGVudCBTa2V0Y2hlcy5wZGYAABMAAS8AABUAAgAK//8AAAAIAA0AGgAkAG8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACWQ==}}

@inproceedings{Shi_2021,
	author = {Benwei Shi and Zhuoyue Zhao and Yanqing Peng and Feifei Li and Jeff M. Phillips},
	booktitle = {Proceedings of the 2021 International Conference on Management of Data},
	date-added = {2022-01-10 07:35:54 -0500},
	date-modified = {2022-01-10 07:35:54 -0500},
	doi = {10.1145/3448016.3452802},
	month = {jun},
	publisher = {{ACM}},
	title = {At-the-time and Back-in-time Persistent Sketches},
	url = {https://doi.org/10.1145%2F3448016.3452802},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1145%2F3448016.3452802},
	bdsk-url-2 = {https://doi.org/10.1145/3448016.3452802}}

@inproceedings{Dai_2021,
	author = {Zhenwei Dai and Aditya Desai and Reinhard Heckel and Anshumali Shrivastava},
	booktitle = {Proceedings of the 2021 International Conference on Management of Data},
	date-added = {2022-01-10 07:32:57 -0500},
	date-modified = {2022-01-10 07:32:57 -0500},
	doi = {10.1145/3448016.3457327},
	month = {jun},
	publisher = {{ACM}},
	title = {Active Sampling Count Sketch ({ASCS}) for Online Sparse Estimation of a Trillion Scale Covariance Matrix},
	url = {https://doi.org/10.1145%2F3448016.3457327},
	year = 2021,
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCTLi4vLi4vLi4vcHJvamVjdHMvIW5ldyBwYXBlci9zaWdtb2QgMjAyMS9BY3RpdmUgU2FtcGxpbmcgQ291bnQgU2tldGNoIChBU0NTKSBmb3IgT25saW5lIFNwYXJzZSBFc3RpbWF0aW9uIG9mIGEgVHJpbGxpb24gU2NhbGUgQ292YXJpYW5jZSBNYXRyaXgucGRmTxEDHgAAAAADHgACAAADbWFjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0FjdGl2ZSBTYW1wbGluZyBDbyNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwAEAAAKIGN1AAAAAAAAAAAAAAAAAAtzaWdtb2QgMjAyMQAAAgC9LzpVc2VyczpmeXU6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHk6cHJvamVjdHM6IW5ldyBwYXBlcjpzaWdtb2QgMjAyMTpBY3RpdmUgU2FtcGxpbmcgQ291bnQgU2tldGNoIChBU0NTKSBmb3IgT25saW5lIFNwYXJzZSBFc3RpbWF0aW9uIG9mIGEgVHJpbGxpb24gU2NhbGUgQ292YXJpYW5jZSBNYXRyaXgucGRmAAAOANYAagBBAGMAdABpAHYAZQAgAFMAYQBtAHAAbABpAG4AZwAgAEMAbwB1AG4AdAAgAFMAawBlAHQAYwBoACAAKABBAFMAQwBTACkAIABmAG8AcgAgAE8AbgBsAGkAbgBlACAAUwBwAGEAcgBzAGUAIABFAHMAdABpAG0AYQB0AGkAbwBuACAAbwBmACAAYQAgAFQAcgBpAGwAbABpAG8AbgAgAFMAYwBhAGwAZQAgAEMAbwB2AGEAcgBpAGEAbgBjAGUAIABNAGEAdAByAGkAeAAuAHAAZABmAA8ACAADAG0AYQBjABIAu1VzZXJzL2Z5dS9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9wcm9qZWN0cy8hbmV3IHBhcGVyL3NpZ21vZCAyMDIxL0FjdGl2ZSBTYW1wbGluZyBDb3VudCBTa2V0Y2ggKEFTQ1MpIGZvciBPbmxpbmUgU3BhcnNlIEVzdGltYXRpb24gb2YgYSBUcmlsbGlvbiBTY2FsZSBDb3ZhcmlhbmNlIE1hdHJpeC5wZGYAABMAAS8AABUAAgAK//8AAAAIAA0AGgAkALoAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAD3A==},
	bdsk-url-1 = {https://doi.org/10.1145%2F3448016.3457327},
	bdsk-url-2 = {https://doi.org/10.1145/3448016.3457327}}

@inproceedings{Zhang2009,
	abstract = {Many queries, such as those involve words like "every", "only", "some", "at least", etc., are very difficult to formulate in SQL. In this paper, we analyze the problems and propose a solution by enhancing SQL with set-comparison operators. These operators would allow users to express such difficult to formulate queries in a simple and natural manner, greatly enhancing the user-friendliness of SQL.},
	author = {J. Zhang and W.-C. Hou and F. Yu and D. Che},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781615670147},
	journal = {24th International Conference on Computers and Their Applications 2009, CATA 2009},
	keywords = {Databases,Query language,SQL},
	title = {Enhancing SQL with set-comparison operators},
	year = {2009}}

@inproceedings{Luo2009,
	abstract = {As the Extensible Markup Language (XML) rapidly establishes itself as the de facto standard for presenting, storing, and exchanging data on the Internet, large volume of XML data and their supporting facilities start to surface. A fast and accurate selectivity estimation mechanism is of practical importance because selectivity estimation plays a fundamental role in XML query optimization. Recently proposed techniques are all based on some forms of structure synopses that could be time-consuming to build and not effective for summarizing complex structure relationships. In this research, we propose an innovative sampling method that can capture the tree structures and intricate relationships among nodes in a simple and effective way. The derived sample tree is stored as a synopsis for selectivity estimation. Extensive experimental results show that, in comparison with the state-of-the-art structure synopses, specifically the TreeSketch and Xseed synopses, our sample tree synopsis applies to a broader range of query types, requires several orders of magnitude less construction time, and generates estimates with considerably better precision for complex datasets. Copyright 2009 ACM.},
	author = {C. Luo and Z. Jiang and W.-C. Hou and F. Yu and Q. Zhu},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1145/1516360.1516400},
	isbn = {9781605584225},
	journal = {Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology, EDBT'09},
	title = {A sampling approach for XML query selectivity estimation},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1145/1516360.1516400}}

@inproceedings{Luo2011,
	abstract = {Reverse top-k queries are recently proposed to help producers (or manufacturers) predict the popularity of a particular product. They can also help them design effective marketing strategies to advertise their products to a target audience. This paper designs an innovative algorithm, termed IRTA (Improved Reverse top-k Threshold Algorithm), to answer reverse top-k queries efficiently. Compared with the state-of-the-art RTA algorithm, it further reduces the number of expensive top-k queries. Besides, it utilizes the dominance and reverse-dominance relationships between the query product and the other products to cut down the cost of each top-k query. Comprehensive theoretical analyses and experimental studies show that IRTA is a more effective algorithm than RTA.},
	author = {C. Luo and F. Yu and W.-C. Hou and Z. Jiang and D. Che and S. He},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9789898425539},
	journal = {ICEIS 2011 - Proceedings of the 13th International Conference on Enterprise Information Systems},
	keywords = {RTA,Reverse top-k queries},
	title = {IRTA: An improved threshold algorithm for reverse top-k queries},
	volume = {1 DISI},
	year = {2011}}

@inproceedings{yu-dexa11,
	author = {Feng Yu and Wen-Chi Hou and Cheng Luo and Qiang Zhu and Dunren Che},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	institution = {Springer},
	journal = {International Conference on Database and Expert Systems Applications},
	pages = {420-427},
	title = {Join selectivity re-estimation for repetitive queries in databases},
	year = {2011}}

@inproceedings{Luo2012,
	abstract = {Caching has been widely used in the relational database paradigm to facilitate query evaluations. Compared with relational queries, XML queries are in general more complex and time-consuming since they involve not only data values but also structural constraints. Therefore, caching can potentially play an even more significant and beneficial role in expediting XML query evaluations. In this research, we thoroughly examine the possible relationships between an incoming XML query and a cached XML query. Depending on their relationships, different algorithms have been proposed to utilize the cached XML query results, so the time and scope for the evaluation of the incoming XML query can be reduced. Copyright {\copyright} 2012 by International Society of Computers and Their Applications (ISCA).},
	author = {C. Luo and W.-C. Hou and S. He and F. Yu},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781880843888},
	journal = {25th International Conference on Computer Applications in Industry and Engineering, CAINE 2012 and 4th International Symposium on Sensor Network and Application, SNA 2012},
	title = {Using cached results to expedite XML query evaluations},
	year = {2012}}

@inproceedings{Yu2012,
	abstract = {Optimizing executions of queries is the ultimate goal of a query optimizer. Unfortunately, due to the complexities of queries, accuracy of statistics, validities of assumptions, etc., query optimizers often cannot find the best execution plans in their search spaces, conveniently called the optimal plans, for the queries. In this paper, we consider gathering statistics for re-optimization of a large and useful set of queries, called repetitive queries. Repetitive queries refer to those queries that are likely to be used repeatedly or frequently in the future. They are usually stored in the database for convenient reuse over the long term. They deserve more optimization efforts than ordinary ad hoc queries. In this research, we identify statistics, called sufficient statistics, that are sufficient to compute (the exact frequency distributions of) the intermediate results of all plans of a query. The finding of the sufficient statistics makes it entirely possible for an optimizer to find a plan that is truly the best in its search space for the query.},
	author = {F. Yu and W.-C. Hou and M. Wainer and C. Luo and D. Che},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781880843840},
	journal = {Proceedings of the ISCA 27th International Conference on Computers and Their Applications, CATA 2012},
	keywords = {Query optimization,Query size estimation},
	title = {Sufficient statistics for re-optimizing repetitive queries},
	year = {2012}}

@article{Yu2013a,
	abstract = {Optimizing executions of queries is the ultimate goal of a query optimizer. Unfortunately, due to the complexities of queries, accuracy of statistics, validities of assumptions, etc., query optimizers often cannot find the best execution plans in their search spaces for the queries. In this paper, we consider gathering statistics for re-optimization of a large and useful set of queries, called repetitive queries. They are likely to be posted repeatedly in the future and deserve more optimization efforts than ordinary ad hoc queries. In this research, we identify a minimal set of statistics, called the sufficient statistics, which is sufficient and necessary to compute the exact frequency distributions of intermediate results of all plans of a query. The finding of the sufficient statistics makes it entirely possible for an optimizer to find a plan that is truly the best in its search space for the query. Copyright {\copyright} 2013 ISCA.},
	author = {F. Yu and W.-C. Hou and M. Wainer and C. Luo},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	issn = {10765204},
	issue = {1},
	journal = {International Journal of Computers and their Applications},
	keywords = {Query optimization,Query size estimation,Re-optimization,Repetitive query,Sufficient statistics},
	title = {Sufficient statistics for re-optimizing repetitive queries},
	volume = {20},
	year = {2013}}

@inproceedings{Yu2013,
	abstract = {In this paper, we propose a concurrency control protocol, called the prudent-precedence concurrency control (PPCC) protocol, for high data contention databases. PPCC is able to recognize more serializable schedules than two-phase locking and commit transactions following the serialization order established during executions. A detailed simulation model has been constructed and a series of experiments have been conducted to evaluate the performance of the proposed approach. The results demonstrate that the proposed algorithm outperforms the two-phase locking over a wide range of system workload. Copyright {\copyright} (2013) by the International Society for Computers and Their Applications.},
	author = {F. Yu and W.-C. Hou and M. Wainer and C. Luo},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781622769728},
	journal = {28th International Conference on Computers and Their Applications 2013, CATA 2013},
	keywords = {2PL,Concurrency control,Serialization graph},
	title = {A prudent-precedence concurrency control protocol for high data contention database environments},
	year = {2013}}

@inproceedings{Yu2014,
	abstract = {The column-store database features a faster data reading speed and higher data compression efficiency compared with traditional row-based databases. However, optimizing write operations in the column-store database is one of the well-known challenges. Most existing works on write performance optimization focus on main-memory column-store databases. In this work, we investigate optimizing write operations on out-of-core (or external memory) column-store databases. We propose an innovative data storage format called Timestamped Binary Association Table (or TBAT). Based on TBAT, a new update method, called Asynchronous Out-of-Core Update (or AOC Update), is designed to replace the traditional update. A significant improvement in speed performance is shown in experiments when comparing the AOC update with the traditional update. Copyright ISCA, CAINE 2014.},
	author = {F. Yu and C. Luo and W.-C. Hou and E.S. Jones},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781880843970},
	journal = {27th International Conference on Computer Applications in Industry and Engineering, CAINE 2014},
	title = {Asynchronous update on out-of-core column-store databases utilizing the timestamped binary association table},
	year = {2014}}

@article{Yu2014b,
	abstract = {Copyright {\copyright} 2014. Histograms are commonly used in databases to store statistics for query size estimation. Unfortunately, the sizes of histograms can grow dramatically with the number of attributes, and thus may not be suitable for multi-dimensional range queries. We take up the challenge to perform an estimation error analysis of histogram for multi-dimensional range selection queries. By employing the multidimensional Random Shuffling Assumption and Fixed Storage Assumption, we prove that the averaged estimation (square) errors of multi-dimensional (equi-width) histograms increase faster than linearly when dimension of the domain increases, with respect to the selection proportion of the range query; however, the corresponding errors of sample synopses increase only linearly with the increase in dimensionality. ISCA},
	author = {F. Yu and W.-C. Hou and C. Luo},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	issn = {10765204},
	issue = {3},
	journal = {International Journal of Computers and their Applications},
	keywords = {Estimation error,Histogram,Sample},
	title = {Estimation error analysis of range selection queries using histogram and sample in low dimensional spaces},
	volume = {21},
	year = {2014}}

@article{Xu2014a,
	abstract = {{\copyright}, 2014, South China University of Technology. All right reserved. In the electro-hydraulic servo system, extra sensors are often added for fault detection and isolation (FDI), but the sensor fault can cause a false alarm. The electro-hydraulic servo system itself is a typical nonlinear system and is often subjected to time-varying and unknown disturbances, which brings about great challenges to the sensor FDI. In order to solve these problems, a FDI scheme based on a nonlinear robust observer is proposed. In this scheme, the robust observer is used to handle the system nonlinearity as well as unknown disturbances and a linear matrix inequality method is adopted to facilitate the observer design. For the sensor fault isolation, a batch of robust observers is designed and some logic rules are made. Then, the proposed FDI scheme is verified by simulations and experiments, and an adaptive threshold is designed to make decision according to the characteristics of experimental data. Both simulation and experimental results show that the proposed FDI scheme is effective.},
	author = {Q.-N. Xu and F. Yu and H. Zhou and H.-Y. Yang},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.3969/j.issn.1000-565X.2014.11.006},
	issn = {1000565X},
	issue = {11},
	journal = {Huanan Ligong Daxue Xuebao/Journal of South China University of Technology (Natural Science)},
	keywords = {Adaptive threshold,Electro-hydraulic servo system,Fault detection,Fault isolation,Robust observer},
	title = {Sensor fault detection and isolation for an electro-hydraulic servo system based on robust observer},
	volume = {42},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.3969/j.issn.1000-565X.2014.11.006}}

@inproceedings{fyu-caine14,
	author = {Feng Yu and Cheng Luo and Wen-Chi Hou and Eric S Jones},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	journal = {27th International Conference on. Computer Applications in Industry and Engineering (CAINE'14)},
	pages = {215-220},
	title = {Asynchronous Update on Out-of-Core Column-Store Databases Utilizing the Time stamped Binary Association Table},
	year = {2014}}

@inproceedings{Yu2014a,
	abstract = {Histograms are commonly used in databases to store statistics for query size estimation. Unfortunately, the sizes of histograms can grow dramatically with the number of attributes, and thus may not be suitable for multi-dimensional range queries. In this paper, we compare analytically and empirically the estimation errors of using sample synopses and histograms for multi-dimensional range query size estimation. We prove that given the same amount of storage space, the estimation (square) errors of sample synopses increase linearly with increase in dimensionality, while faster than linearly for (equi-length) histograms.},
	author = {F. Yu and W.-C. Hou and C. Luo},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781632665133},
	journal = {29th International Conference on Computers and Their Applications, CATA 2014},
	keywords = {Estimation error,Histogram,Sample},
	title = {Histogram and sample revisited: Estimation error analysis for multidimensional range selection queries},
	year = {2014}}

@inproceedings{Luo2014,
	abstract = {Checking containment relationships between XML data nodes is the most essential task for processing XML queries. The result of an XML query is built upon the containment relationships between the XML data nodes that participate in the query. Hence, it is of vital practical importance to design efficient algorithms to perform containment joins. In addition, some XML data tree contains self-nested nodes. When these nodes participate in containment joins, previously valid algorithms may become erroneous. This paper proposes an innovative containment join algorithm that can efficiently handle nested data nodes. Theoretical analyses and empirical studies both confirm its correctness and efficiency. Copyright ISCA, CAINE 2014.},
	author = {C. Luo and F. Yu and W.-C. Hou and S. He},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781880843970},
	journal = {27th International Conference on Computer Applications in Industry and Engineering, CAINE 2014},
	title = {Containment join in the presence of nested data nodes},
	year = {2014}}

@article{Xu2014,
	abstract = {{\copyright} 2014, Central South University Press and Springer-Verlag Berlin Heidelberg. The inherent nonlinearities of the rudder servo system (RSS) and the unknown external disturbances bring great challenges to the practical application of fault detection technology. Modeling of whole rudder system is a challenging and difficult task. Quite often, models are too inaccurate, especially in transient stages. In model based fault detection, these inaccuracies might cause wrong actions. An effective approach, which combines nonlinear unknown input observer (NUIO) with an adaptive threshold, is proposed. NUIO can estimate the states of RSS asymptotically without any knowledge of external disturbance. An adaptive threshold is used for decision making which helps to reduce the influence of model uncertainty. Actuator and sensor faults that occur in RSS are considered both by simulation and experimental tests. The observer performance, robustness and fault detection capability are verified. Simulation and experimental results show that the proposed fault detection scheme is efficient and can be used for on-line fault detection.},
	author = {Q.-N. Xu and H. Zhou and F. Yu and X.-Q. Wei and H.-Y. Yang},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1007/s11771-014-2413-6},
	issn = {22275223},
	issue = {11},
	journal = {Journal of Central South University},
	keywords = {adaptive threshold,fault detection,nonlinear unknown input observer,rudder servo system},
	title = {Effective model based fault detection scheme for rudder servo system},
	volume = {21},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s11771-014-2413-6}}

@book_section{Yu2015-dexa,
	abstract = {{\copyright} Springer International Publishing Switzerland 2015. The column-store database features a faster data reading speed and higher data compression efficiency compared with traditional row-based databases. However, optimizing write operations in the column-store database is one of the well-known challenges. Most existing works on write performance optimization focus on main-memory column-store databases. In this work, we investigate optimizing write operation (update and deletion) on out-of-core (OOC, or external memory) column-store databases. We propose a general framework to work for both normal OOC storage or big data storage, such as Hadoop Distributed File System (HDFS). On normal OOC storage, we propose an innovative data storage format called Timestamped Binary Association Table (or TBAT). Based on TBAT, a new update method, called Asynchronous Out-of-Core Update (or AOC Update), is designed to replace the traditional update. On big data storage, we further extend TBAT onto HDFS and propose the Asynchronous Map-Only Update (or AMO Update) to replace the traditional update. Fast selection methods are developed in both contexts to improve data retrieving speed. A significant improvement in speed performance is shown in the extensive experiments when performing write operations on TBAT in normal and Map-Reduce environment.},
	author = {Feng Yu and W.-C. Wen-Chi Hou},
	city = {Cham},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1007/978-3-319-22849-5_12},
	editor = {Qiming Chen and Abdelkader Hameurlain and Farouk Toumani and Roland Wagner and Hendrik Decker},
	isbn = {9783319228488},
	issn = {16113349},
	journal = {Database and Expert Systems Applications: 26th International Conference, DEXA 2015, Valencia, Spain, September 1-4, 2015, Proceedings, Part I},
	pages = {155-169},
	publisher = {Springer International Publishing},
	title = {A Framework of Write Optimization on Read-Optimized Out-of-Core Column-Store Databases},
	volume = {9261},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-22849-5_12}}

@article{Xu2015,
	abstract = {{\copyright}, 2015, Huazhong University of Science and Technology. All right reserved. For the inherent nonlinearities and unknown external disturbances of the ship rudder servo system (RSS), a robust observer based fault detection scheme was proposed, which can detect the faults of the RSS efficiently without any knowledge of the disturbances. The design steps and the existing conditions for the observer were given after system modeling, and the linear matrix inequality method was used to solve the design problem. A NI-PXI based test rig was built for online RSS fault detection. The observer performance in normal state, robustness and fault detection capability were all verified. In order to avoid false alarms during transient response, an adaptive threshold was used for fault decision. Experimental results show that the proposed fault detection scheme is efficient and can be used for online fault detection, and is better than some existing fault detection methods for similar systems.},
	author = {Q. Xu and F. Yu and H. Zhou and H. Yang},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.13245/j.hust.150121},
	issn = {16714512},
	issue = {1},
	journal = {Huazhong Keji Daxue Xuebao (Ziran Kexue Ban)/Journal of Huazhong University of Science and Technology (Natural Science Edition)},
	keywords = {Fault detection,Observer,Robustness,Rudder,Servo system,Threshold},
	title = {Robust observer based fault detection for a ship rudder servo system},
	volume = {43},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.13245/j.hust.150121}}

@inproceedings{Neizer-Ashun2015,
	abstract = {Copyright ISCA, BICOB 2015. Proteins execute their biological functions if located at a proper cellular compartment. Knowledge of the subcellular location of a protein is crucial for understanding its functions. Many algorithms have been developed to help build up tools to predict most subcellular locations of proteins but only a small fraction has a high level of prediction accuracy. This research is to compare methods for predicting the subcellular location of plant proteins, based on the physiochemical properties of proteins that can be calculated from their primary sequences. Although there have been different approaches to predict the subcellular location of proteins, this research takes the approach of using Pseudo amino acid composition as a numerical representation of the proteins and uses Random forest, AdaBoost (Adaptive Boosting) and SAMME (Stage-wise Additive Model using Multi-class Exponential loss function) algorithms to build models for prediction. The algorithms were compared to determine the best model to use for our protein predictions. SAMME, which is a little similar to AdaBoost had a lot of improvement in predicting proteins to their subcellular locations compared to the other algorithms.},
	author = {K. Neizer-Ashun and F. Yu and J. Meinken and X. Min and G.-H. Chang},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781880843994},
	journal = {Proceedings of the 7th International Conference on Bioinformatics and Computational Biology, BICOB 2015},
	keywords = {AdaBoost,Matthews Correlation Coefficient,Plant protein,Random Forest,SAMME,Subcellular locations},
	title = {Prediction of plant protein subcellular locations},
	year = {2015}}

@article{Min2015,
	abstract = {{\copyright} 2015 Min et al. Background: Protein functional diversity at the post-transcriptional level is regulated through spliceosome mediated pre-mRNA alternative splicing (AS) events and that has been widely demonstrated to be a key player in regulating the functional diversity in plants. Identification and analysis of AS genes in cereal crop plants are critical for crop improvement and understanding regulatory mechanisms. Results: We carried out the comparative analyses of the functional landscapes of the AS using the consensus assembly of expressed sequence tags and available mRNA sequences in four cereal plants. We identified a total of 8,734 in Oryza sativa subspecies (ssp) japonica, 2,657 in O. sativa ssp indica, 3,971 in Sorghum bicolor, and 10,687 in Zea mays AS genes. Among the identified AS events, intron retention remains to be the dominant type accounting for 23.5 % in S. bicolor, and up to 55.8 % in O. sativa ssp indica. We identified a total of 887 AS genes that were conserved among Z. mays, S. bicolor, and O. sativa ssp japonica; and 248 AS genes were found to be conserved among all four studied species or ssp. Furthermore, we identified 53 AS genes conserved with Brachypodium distachyon. Gene Ontology classification of AS genes revealed functional assignment of these genes in many biological processes with diverse molecular functions. Conclusions: AS is common in cereal plants. The AS genes identified in four cereal crops in this work provide the foundation for further studying the roles of AS in regulation of cereal plant growth and development. The data can be accessed at Plant Alternative Splicing Database},
	author = {X.J. Min and B. Powell and J. Braessler and J. Meinken and F. Yu and G. Sablok},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1186/s12864-015-1914-5},
	issn = {14712164},
	issue = {1},
	journal = {BMC Genomics},
	keywords = {Alternative splicing,Cereal crops,Expressed sequence tags,MRNA},
	title = {Genome-wide cataloging and analysis of alternatively spliced genes in cereal crops},
	volume = {16},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1186/s12864-015-1914-5}}

@inproceedings{Yu2015,
	abstract = {Copyright {\copyright} 2015 by The International Society for Computers and Their Applications (ISCA). Optimizing write operations in out-of-core (or external memory) column-store databases is a wellknown challenge. By the employment of Timestamped Binary Association Table (or TBAT) and Asynchronous Out-of-Core Update (or AOC Update), updates on the out-of-core column-store database can be significantly improved. After a time period of AOC updates, the selection query performance on TBAT gradually decreases. In this work, we propose innovative online data cleaning approaches to solve the selection performance problem without the need of locking the table. Two different methods are introduced, an eager approach and a progressive approach to satisfy the cleaning speed priority and memory usage priority, respectively. Preliminary experiments show that the selection speed is significantly improved using the progressive online data cleaning approaches. In addition, the more the AOC updates, the greater the improvement is observed.},
	author = {F. Yu and C. Luo and W.-C. Hou and E.S. Jones},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781880843987},
	journal = {Proceedings of the 30th International Conference on Computers and Their Applications, CATA 2015},
	title = {Online data cleaning for out-of-core column-store databases with timestamped binary association tables},
	year = {2015}}

@inproceedings{Yu2015a,
	abstract = {{\copyright} 2015 IEEE. Column-store databases feature a faster data reading speed compared with traditional row-based databases. However, optimizing write operations in a column-store database is a well-known challenge. Most existing works on write performance optimization focus on main-memory column-store databases. In this work, we extend the research on column-store databases in the Map-Reduce environment. We propose a data storage format called Time stamped Binary Association Table (or TBAT) without the need of global indexing. Based on TBAT, a new update method, called Asynchronous Map-Only Update (or AMO Update), is designed to replace the traditional update. A significant improvement in speed performance is shown in experiments when comparing the AMO update with the traditional update.},
	author = {F. Yu and E.S. Jones and W.-C. Hou},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1109/BigDataCongress.2015.117},
	isbn = {9781467372787},
	journal = {Proceedings - 2015 IEEE International Congress on Big Data, BigData Congress 2015},
	keywords = {column-store,map-reduce,write optimization},
	title = {Write Optimization Using Asynchronous Update on Out-of-Core Column-Store Databases in Map-Reduce},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/BigDataCongress.2015.117}}

@inproceedings{Yu2015b,
	abstract = {Write optimization in out-of-core (or external memory) column-store databases is a well-known challenge. Timestamped Binary Association Table (or TBAT) and Asynchronous Out-of-Core Update (or AOC Update) have shown significant improvements for this problem. However, after a time period of AOC updates, the selection query performance on TBAT gradually decreases. In this work, we introduce a new index, called Offset B + -tree (or OB-tree), for fast data retrieving speed on the updated record in the appendix part of a TBAT. Compared with traditional B + -tree, OB-tree uses a special pointer elimination strategy to save more space and allows duplicated keys to be inserted. These features allows the OB-tree to be easily implemented with the existing TBAT data structure. Experiment results show the searching speed is significantly improved after indexing by an OB-tree.},
	author = {F. Yu and E.S. Jones},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781510812284},
	journal = {28th International Conference on Computer Applications in Industry and Engineering, CAINE 2015},
	keywords = {B -tree +,Column-store database,Index},
	title = {Hastening data retrieval on out-of-core column-store databases using offset B<sup>+</sup>-tree},
	year = {2015}}

@inproceedings{Xiong2016,
	abstract = {Copyright ISCA, CAINE 2016. In this paper, we propose a family of concurrency control protocols, called the Hierarchical Precedence Concurrency Control (HPCC) protocols, for high data contention database environments. HPCC attempts to be more aggressive by permitting more serializable schedules than the two-phase locking (2PL). It maintains cycle-free precedence hierarchies for conflicting transactions. Conflicting operations are allowed to proceed only if the hierarchical orderings of precedence is not violated. Transactions commit based on the serialization order established during the executions. A detailed simulation model has been implemented and extensive experiments have been conducted to evaluate the performance of the proposed approach. The results demonstrate that the proposed algorithm outperforms the two-phase locking over a wide range of system workloads.},
	author = {W. Xiong and M. Hamdi and F. Yu and W.-C. Hou},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781943436040},
	journal = {29th International Conference on Computer Applications in Industry and Engineering, CAINE 2016},
	keywords = {2PL,Concurrency control,Serializability,Serialization graph},
	title = {A hierarchical precedence concurrency control protocol for high data contention database environments},
	year = {2016}}

@inproceedings{Yu2017,
	abstract = {{\copyright} 2017 IEEE. The column-store database, featuring a column-by-column data layout and a fast data retrieving speed, is a representative of next-generation database management systems in this big data era. Optimizing the write performance is a well-known challenge in out-of-core (or external memory) column-store databases. Data cleaning helps to cleanse redundant data and improve the overall performance of the databases. Previously proposed data cleaning methods require a long execution time and additional computing resources which are inefficient for column-store databases with large-volume data. This work introduces an auxiliary tree index and high-speed data cleaning methods to improve the overall processing speed of columnar data. The proposed index called OB-tree comes with a rich set of operations and possesses multiple advantages in working with a wide-range of column-store databases. We introduce new data cleaning methods utilizing OB-tree to efficiently identify target records and their locations. Extensive experiments show that the proposed methods enable significant performance improvements for data cleaning on column-store databases.},
	author = {F. Yu and B.J. Latronics and T. Matacic and E.S. Jones},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1109/BigDataCongress.2017.33},
	isbn = {9781538619964},
	journal = {Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017},
	keywords = {B+ -Tree,Column-Store Database,Index,Write Optimization},
	title = {OB-Tree: Accelerating Data Cleaning in Out-of-Core Column-Store Databases},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/BigDataCongress.2017.33}}

@book{Hamdi2017,
	abstract = {{\copyright} 2017, Springer International Publishing AG. We propose to store equi-join relationships of tuples on inexpensive and space abundant devices, such as disks, to facilitate query processing. The equi-join relationships are captured, grouped, and stored as various tables, which are collectively called the Join Core. Queries involving arbitrary legitimate sequences of equi-joins, semi-joins, outer-joins, anti-joins, unions, differences, and intersections can all be answered quickly by merely merging these tables. The Join Core can also be updated dynamically. Experimental results showed that all test queries began to generate results instantly, and many completed instantly too. The proposed methodology can be very useful for queries with complex joins of large relations as there are fewer or even no relations or intermediate results needed to be retrieved and generated.},
	author = {M. Hamdi and F. Yu and S. Alswedani and W.-C. Hou},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1007/978-3-319-64468-4_13},
	isbn = {9783319644677},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Anti-join,Equi-join,Join queries,Outer-join,Query processing,Semi-join,Set operations},
	title = {Storing join relationships for fast join query processing},
	volume = {10438 LNCS},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-64468-4_13}}

@inproceedings{Almutairi2018,
	abstract = {{\copyright} 2018 The International Society for Computers and Their Applications (ISCA). All rights reserved. Much research has been devoted to designing efficient join algorithms in the last few decades. In this paper, we compare two fastest join methods: Join indices and Join Core. Join indices generate index tables that contain tuples identifiers for matching tuples. Joins can be performed by scaning each input relation only once. On the other hand, Join Core is a data structure that stores join relationships to facilitate join query processing. With Join Core, join queries can be answered without having to perform costly join operations. We have implemented both methods and performed extensive experiments on TPC-H benchmark datasets and queries. Our experimental result shows that while both methods are much faster than conventional systems, such as MySQL, Join Core is the fastest.},
	author = {R.M. Almutairi and M. Hamdi and F. Yu and W.-C. Hou},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	isbn = {9781943436101},
	journal = {Proceedings of the 33rd International Conference on Computers and Their Applications, CATA 2018},
	title = {Performance evaluations of two fast join query processing methods: Join core and join indices},
	volume = {2018-March},
	year = {2018}}

@article{Yu2018,
	abstract = {Copyright {\copyright} 2018 Inderscience Enterprises Ltd. The column-store database is a representative of next generation databases featuring a high reading speed. Write optimisation in the out-of-core column-store database remains a well-known challenge. Timestamped binary association table (or TBAT) and asynchronous out-of-core update (or AOC update) have shown improvements in write performance. However, a common restriction shared by the timestamp-based approaches is that, after a time period of updates, the searching performance will gradually decrease. In this work, we introduce a new index, called Offset B + -tree (or OB-tree), to further improve the data retrieval speed after many updates have taken place. OB-tree is a flexible and robust index that employs a special pointer elimination strategy to reduce the storage costs. Succinctly designed, OB-tree can be easily integrated into existing timestamp-based column-store databases. Extensive experiments show that OB-tree can be efficiently constructed and significantly improves the data retrieval speed on the TBAT even when a large number of updates occurred.},
	author = {F. Yu and T.J. Matacic and B.J. Latronica and W.-C. Hou},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1504/IJIIDS.2018.10012700},
	issn = {17515866},
	issue = {1},
	journal = {International Journal of Intelligent Information and Database Systems},
	keywords = {B+-tree,Column-store database,Index,Write optimisation},
	title = {OB-tree: A new write optimisation index on out-of-core column-store databases},
	volume = {11},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1504/IJIIDS.2018.10012700}}

@inproceedings{Hamdi2018,
	abstract = {{\copyright} 2017 IEEE. This research presents an innovative way to process queries without having to perform expensive join and set operations. We propose to store the equi-join relationships of tuples on mass storage devices, such as disks, to facilitate query processing. The equi-join relationships are captured, grouped, and stored as various tables on disks, which are collectively called the Join Core. Queries involving arbitrary legitimate sequences of equi-joins, semi-joins, outer-joins, anti-joins, unions, differences, and intersections can all be answered quickly by merely merging these tables. Without having to perform joins, memory consumptions are dramatically reduced. The Join Core can also be updated dynamically. Preliminary experimental results showed that all test queries began to generate results instantly, and many completed instantly too. The proposed methodology can be very useful for queries with complex joins of large relations, and can be even more advantageous to distributed query processing, as there are fewer or even no relations or intermediate results needed to be retrieved, generated or transferred over the networks.},
	author = {M. Hamdi and F. Yu and W.-C. Hou},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.1109/SAI.2017.8252126},
	isbn = {9781509054435},
	journal = {Proceedings of Computing Conference 2017},
	keywords = {Anti-Join,Equi-Join,Join Queries,Outer-Join,Query Processing,Semi-Join,Set Operations},
	title = {Fast processing of join queries with instant response},
	volume = {2018-Janua},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/SAI.2017.8252126}}

@article{Clark2019,
	abstract = {{\copyright} 2019 Clark, Yu, Gu and Min. Tomato (Solanum lycopersicum) is an important vegetable and fruit crop. Its genome was completely sequenced and there are also a large amount of available expressed sequence tags (ESTs) and short reads generated by RNA sequencing (RNA-seq) technologies. Mapping transcripts including mRNA sequences, ESTs, and RNA-seq reads to the genome allows identifying pre-mRNA alternative splicing (AS), a post-transcriptional process generating two or more RNA isoforms from one pre-mRNA transcript. We comprehensively analyzed the AS landscape in tomato by integrating genome mapping information of all available mRNA and ESTs with mapping information of RNA-seq reads which were collected from 27 published projects. A total of 369,911 AS events were identified from 34,419 genomic loci involving 161,913 transcripts. Within the basic AS events, intron retention is the prevalent type (18.9%), followed by alternative acceptor site (12.9%) and alternative donor site (7.3%), with exon skipping as the least type (6.0%). Complex AS types having two or more basic event accounted for 54.9% of total AS events. Within 35,768 annotated protein-coding gene models, 23,233 gene models were found having pre-mRNAs generating AS isoform transcripts. Thus the estimated AS rate was 65.0% in tomato. The list of identified AS genes with their corresponding transcript isoforms serves as a catalog for further detailed examination of gene functions in tomato biology. The post-transcriptional information is also expected to be useful in improving the predicted gene models in tomato. The sequence and annotation information can be accessed at plant alternative splicing database (http://proteomics.ysu.edu/altsplice).},
	author = {S. Clark and F. Yu and L. Gu and X.J. Min},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 07:29:47 -0500},
	doi = {10.3389/fpls.2019.00689},
	issn = {1664462X},
	journal = {Frontiers in Plant Science},
	keywords = {Alternative splicing,Gene expression,MRNA,Plant,Solanum lycopersicum,Tomato,Transcriptome},
	title = {Expanding alternative splicing identification by integrating multiple sources of transcription data in tomato},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3389/fpls.2019.00689}}

@inproceedings{Yu2019,
	author = {F. Yu and W.-C. Hou},
	city = {Los Alamitos, CA, USA},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 10:23:15 -0500},
	doi = {10.1109/BigData47090.2019.9006440},
	journal = {2019 IEEE International Conference on Big Data (Big Data)},
	keywords = {estimation;big data;histograms;indexes;query processing;aggregates},
	month = {12},
	pages = {583-592},
	publisher = {IEEE Computer Society},
	title = {CS*: Approximate Query Processing on Big Data using Scalable Join Correlated Sample Synopsis},
	url = {https://doi.ieeecomputersociety.org/10.1109/BigData47090.2019.9006440},
	year = {2019},
	bdsk-url-1 = {https://doi.ieeecomputersociety.org/10.1109/BigData47090.2019.9006440},
	bdsk-url-2 = {https://doi.org/10.1109/BigData47090.2019.9006440}}

@inproceedings{Wilson2019,
	author = {David Wilson and Wen-Chi Hou and Feng Yu},
	date-added = {2022-01-10 07:29:47 -0500},
	date-modified = {2022-01-10 13:36:25 -0500},
	doi = {10.29007/87vt},
	editor = {Frederick Harris and Sergiu Dascalu and Sharad Sharma and Rui Wu},
	issn = {2398-7340},
	journal = {Proc. of 28th International Conference on Software Engineering and Data Engineering},
	keywords = {aqp;database},
	pages = {41-50},
	publisher = {EasyChair},
	title = {Scalable Correlated Sampling for Join Query Estimations on Big Data},
	url = {https://easychair.org/publications/paper/RB13},
	volume = {64},
	year = {2019},
	bdsk-url-1 = {https://easychair.org/publications/paper/RB13},
	bdsk-url-2 = {https://doi.org/10.29007/87vt}}

@article{Dobra,
	author = {Alin Dobra and Johannes Gehrke},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	isbn = {1581134975},
	keywords = {aqp; database; sampling},
	title = {SIGMOD2003_Processing Complex Aggregate Queries over Data Streams.pdf}}

@article{horvitz1952ht,
	author = {Daniel G Horvitz and Donovan J Thompson},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {260},
	journal = {Journal of the American statistical Association},
	keywords = {aqp; database; sampling},
	pages = {663-685},
	publisher = {Taylor & Francis Group},
	title = {A generalization of sampling without replacement from a finite universe},
	volume = {47},
	year = {1952}}

@inproceedings{Lynch1988,
	author = {Clifford A Lynch},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proc. VLDB'88},
	keywords = {aqp; database; sampling},
	pages = {240-251},
	title = {Selectivity Estimation and Query Optimization in Large Databases with Highly Skewed Distributions of Column Values},
	year = {1988}}

@inproceedings{Hou1989,
	abstract = {We consider those database environments in which queries have strict timing constraints, and develop a time-constrained query evaluation methodology. For aggregate relational algebra queries, we describe a time constrained query evaluation algorithm. ...},
	author = {Wen Chi Hou and Gultekin Ozsoyoglu and Baldeo K. Taneja},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proc. SIGMOD'89},
	keywords = {aqp; database; sampling},
	pages = {68-77},
	title = {Processing aggregate relational queries with hard time constraints},
	year = {1989}}

@article{Hellerstein1997,
	author = {Joseph M Hellerstein and Peter J Haas and Helen J Wang},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {aqp; database; sampling},
	month = {6},
	pages = {171-182},
	publisher = {ACM},
	title = {Online Aggregation},
	volume = {26},
	year = {1997}}

@article{poosala99-DEB,
	author = {Viswanath Poosala and Venkatesh Ganti and Yannis E Ioannidis},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {4},
	journal = {IEEE Data Eng. Bull.},
	keywords = {aqp; database; sampling},
	pages = {5-14},
	title = {Approximate query answering using histograms},
	volume = {22},
	year = {1999}}

@article{Ling1999,
	author = {Yibei Ling and Wei Sun and Naphtali D. Rishe and Xianjing Xiang},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1109/69.761667},
	issn = {10414347},
	issue = {2},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Estimation accuracy,Estimation reliability,Hybrid estimator,Parametric estimator,Query optimization,Sampling estimator,Table-based estimator; aqp; database; sampling},
	pages = {338-354},
	title = {A hybrid estimator for selectivity estimation},
	volume = {11},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/69.761667}}

@inproceedings{acha99js,
	author = {Swarup Acharya and Phillip B. Gibbons and Viswanath Poosala and Sridhar Ramaswamy},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proc. SIGMOD'99},
	keywords = {aqp; database; sampling},
	pages = {275-286},
	publisher = {ACM},
	title = {Join Synopses for Approximate Query Answering},
	year = {1999}}

@article{breidt2000local,
	author = {F Jay Breidt and Jean D Opsomer and others},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {4},
	journal = {The annals of statistics},
	keywords = {aqp; database; sampling},
	pages = {1026-1053},
	publisher = {Institute of Mathematical Statistics},
	title = {Local polynomial regresssion estimators in survey sampling},
	volume = {28},
	year = {2000}}

@article{10.5555/767141.767147,
	abstract = {Approximate query processing has emerged as a cost-effective approach for dealing
with the huge data volumes and stringent response-time requirements of today's decision
support systems (DSS). Most work in this area, however, has so far been limited in
its query processing scope, typically focusing on specific forms of aggregate queries.
Furthermore, conventional approaches based on sampling or histograms appear to be
inherently limited when it comes to approximating the results of complex queries over
high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional
wavelets as an effective tool for general-purpose approximate query processing in
modern, high-dimensional applications. Our approach is based on building wavelet-coefficient
synopses of the data and using these synopses to provide approximate answers to queries.
We develop novel query processing algorithms that operate directly on the wavelet-coefficient
synopses of relational tables, allowing us to process arbitrarily complex queries
entirely in the wavelet-coefficient domain. This guarantees extremely fast response
times since our approximate query execution engine can do the bulk of its processing
over compact sets of wavelet coefficients, essentially postponing the expansion into
relational tuples until the end-result of the query. We also propose a novel wavelet
decomposition algorithm that can build these synopses in an I/O-efficient manner.
Finally, we conduct an extensive experimental study with synthetic as well as real-life
data sets to determine the effectiveness of our wavelet-based approach compared to
sampling and histograms. Our results demonstrate that our techniques: (1) provide
approximate answers of better quality than either sampling or histograms; (2) offer
query execution-time speedups of more than two orders of magnitude; and (3) guarantee
extremely fast synopsis construction times that scale linearly with the size of the
data.},
	author = {Kaushik Chakrabarti and Minos Garofalakis and Rajeev Rastogi and Kyuseok Shim},
	city = {Berlin, Heidelberg},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issn = {1066-8888},
	issue = {2--3},
	journal = {The VLDB Journal},
	keywords = {Approximate query answers,Data synopses,Query processing,Wavelet decomposition,approximate,data synopses,query answers,query processing,wavelet decomposition; aqp; database; sampling},
	month = {9},
	pages = {199--223},
	publisher = {Springer-Verlag},
	title = {Approximate Query Processing Using Wavelets},
	volume = {10},
	year = {2001}}

@article{Bernardino2001,
	abstract = {This paper presents an approach to implement large data warehouses on an arbitrary number of computers, achieving very high query execution performance and scalability. The data is distributed and processed in a potentially large number of autonomous computers using our technique called data warehouse striping (DWS). The major problem of DWS technique is that it would require a very expensive cluster of computers with fault tolerant capabilities to prevent a fault in a single computer to stop the whole system. In this paper, we propose a radically different approach to deal with the problem of the unavailability of one or more computers in the cluster, allowing the use of DWS with a very large number of inexpensive computers. The proposed approach is based on approximate query answering techniques that make it possible to deliver an approximate answer to the user even when one or more computers in the cluster are not available. The evaluation presented in the paper shows both analytically and experimentally that the approximate results obtained this way have a very small error that can be negligible in most of the cases.},
	author = {Jorge Bernardino and Pedro Furtado and Henrique Madeira},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1007/3-540-44801-2_34},
	isbn = {3540425535},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {approximate query answering,data partitioning,data warehousing,distributed query optimization,performance optimization; aqp; database; sampling},
	pages = {349-359},
	title = {Approximate query answering using data warehouse striping},
	volume = {2114},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-44801-2_34}}

@article{Lu2002,
	author = {H Lu and R Setiono},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Applied Intelligence},
	keywords = {cost based optimization,neural,query processing,query size estimation,relational algebra operations; aqp; database; sampling},
	pages = {173-183},
	title = {Effective query size estimation using neural networks},
	url = {http://link.springer.com/article/10.1023/A:1014333932021},
	year = {2002},
	bdsk-url-1 = {http://link.springer.com/article/10.1023/A:1014333932021}}

@inproceedings{dobra2002processing,
	author = {Alin Dobra and Minos Garofalakis and Johannes Gehrke and Rajeev Rastogi},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	institution = {ACM},
	journal = {Proceedings of the 2002 ACM SIGMOD international conference on Management of data},
	keywords = {aqp; database; sampling},
	pages = {61-72},
	title = {Processing complex aggregate queries over data streams},
	year = {2002}}

@article{Jin2006,
	abstract = {One important way in which sampling for approximate query processing in a database environment differs from traditional applications of sampling is that in a database, it is feasible to collect accurate summary statistics from the data in addition to.....},
	author = {Ruoming Jin and Leo Glimcher and Chris Jermaine and Gagan Agrawal},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1109/ICDE.2006.106},
	isbn = {0769525709},
	issn = {10844627},
	journal = {Proceedings - International Conference on Data Engineering},
	keywords = {aqp; database; sampling},
	pages = {18},
	title = {New sampling-based estimators for OLAP queries},
	volume = {2006},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2006.106}}

@inproceedings{Joshi08,
	author = {Shantanu Joshi and Christopher Jermaine},
	city = {Washington, DC, USA},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proc. ICDE'08},
	keywords = {aqp; database; sampling},
	pages = {199-208},
	publisher = {IEEE Computer Society},
	title = {Robust Stratified Sampling Plans for Low Selectivity Queries},
	year = {2008}}

@article{Jermaine2008,
	abstract = {This paper describes query processing in the DBO database system. Like other database systems designed for ad-hoc, analytic processing, DBO is able to compute the exact answer to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more information, the guess gets more and more accurate, until it is 100% accurate as the query is completed. This allows users to stop the execution at any time that they are happy with the query accuracy, and encourages exploratory data analysis.},
	author = {Christopher Jermaine and Subramanian Arumugam and Abhijit Pol and Alin Dobra},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1412331.1412335},
	isbn = {9781595936868},
	issn = {03625915},
	issue = {4},
	journal = {ACM Trans. Database Syst.},
	keywords = {dbo,online aggregation,randomized algorithms,sampling; aqp; database},
	pages = {1-54},
	title = {Scalable approximate query processing with the DBO engine},
	volume = {33},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1145/1412331.1412335}}

@article{cormode2011synopses,
	abstract = {Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance. We also discuss the trade-offs between the different synopsis types.},
	author = {Graham Cormode and Minos Garofalakis and Peter J Haas and Chris Jermaine and others},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1561/1900000004},
	issn = {1931-7883},
	issue = {1--3},
	journal = {Foundations and Trends{\textregistered}in Databases},
	keywords = {aqp; database; sampling},
	pages = {1-294},
	publisher = {Now Publishers, Inc.},
	title = {Synopses for massive data: Samples, histograms, wavelets, sketches},
	volume = {4},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1561/1900000004}}

@article{Berkeley2012,
	author = {U C Berkeley and Barzan Mozafari},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.14778/2367502.2367533},
	isbn = {9781450312479},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {aqp; database; sampling},
	pages = {1902-1905},
	title = {Blink and It ' s Done : Interactive Queries on Very Large Data},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.14778/2367502.2367533}}

@inproceedings{yu13-cs2,
	author = {Feng Yu and Wen-Chi Hou and Cheng Luo and Dunren Che and Mengxia Zhu},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-14 10:18:29 -0500},
	doi = {10.1145/2463676.2463701},
	isbn = {978-1-4503-2037-5},
	journal = {Proc. SIGMOD'13},
	keywords = {database synopsis,query optimization,selectivity estimation; aqp; database; sampling},
	pages = {469-480},
	publisher = {ACM},
	title = {{CS2}: A New Database Synopsis for Query Estimation},
	url = {http://doi.acm.org/10.1145/2463676.2463701},
	year = {2013},
	bdsk-url-1 = {http://doi.acm.org/10.1145/2463676.2463701},
	bdsk-url-2 = {https://doi.org/10.1145/2463676.2463701}}

@inproceedings{Agarwal2014,
	author = {Sameer Agarwal and Henry Milner and Ariel Kleiner and Ameet Talwalkar and Michael Jordan and Samuel Madden and Barzan Mozafari and Ion Stoica},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1145/2588555.2593667},
	isbn = {9781450323765},
	issn = {07308078},
	journal = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data - SIGMOD},
	keywords = {approximate query processing,diagnostics,error estimation; aqp; database; sampling},
	pages = {481-492},
	title = {Knowing When You're Wrong: Building Fast and Reliable Approximate Query Processing Systems},
	url = {http://dl.acm.org/citation.cfm?doid=2588555.2593667$%5C$nhttp://web.eecs.umich.edu/%7B~%7Dmozafari/papers/sigmod%7B_%7D2014%7B_%7Ddiagnosis.pdf},
	year = {2014},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2588555.2593667$%5C$nhttp://web.eecs.umich.edu/%7B~%7Dmozafari/papers/sigmod%7B_%7D2014%7B_%7Ddiagnosis.pdf},
	bdsk-url-2 = {https://doi.org/10.1145/2588555.2593667}}

@article{LiFF16-wander,
	author = {Feifei Li and Bin Wu and Ke Yi and Zhuoyue Zhao and Lihong Li and Shawna Miles and Zephan Melville and Amalthiya Prasad and Linda L Breeden},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proc. SIGMOD'16},
	keywords = {asymmetric cell division,cell wall,joins,lsm1,mpt5,online aggregation,quiescence,random walks,ssd1; aqp; database; sampling},
	pages = {615-629},
	title = {Wander Join: Online Aggregation via Random Walks},
	year = {2016}}

@article{van2017query,
	author = {Guy den Broeck and Dan Suciu and others},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {3-4},
	journal = {Foundations and Trends{\textregistered}in Databases},
	keywords = {aqp; database; sampling},
	pages = {197-341},
	publisher = {Now Publishers, Inc.},
	title = {Query processing on probabilistic data: A survey},
	volume = {7},
	year = {2017}}

@inproceedings{chaudhuri2017approximate,
	author = {Surajit Chaudhuri and Bolin Ding and Srikanth Kandula},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proc. SIGMOD'17},
	keywords = {aqp; database; sampling},
	pages = {511-519},
	title = {Approximate query processing: No silver bullet},
	year = {2017}}

@inproceedings{Quoc2018-approxjoin,
	author = {Do Le Quoc and Istemi Ekin Akkus and Pramod Bhatotia and Spyros Blanas and Ruichuan Chen and Christof Fetzer and Thorsten Strufe},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	isbn = {9781450360111},
	journal = {Socc'18},
	keywords = {approximate computing and distributed,approximate join processing,approximate query processing,multi-way joins,stratified sampling; aqp; database; sampling},
	title = {ApproxJoin : Approximate Distributed Joins},
	year = {2018}}

@article{kulessa2018,
	abstract = {Interactive visualizations are arguably the most important tool to explore, understand and convey facts about data. In the past years, the database community has been working on different techniques for Approximate Query Processing (AQP) that aim to deliver an approximate query result given a fixed time bound to support interactive visualizations better. However, classical AQP approaches suffer from various problems that limit the applicability to support the ad-hoc exploration of a new data set: (1) Classical AQP approaches that perform online sampling can support ad-hoc exploration queries but yield low quality if executed over rare subpopulations. (2) Classical AQP approaches that rely on offline sampling can use some form of biased sampling to mitigate these problems but require a priori knowledge of the workload, which is often not realistic if users want to explore a new database. In this paper, we present a new approach to AQP called Model-based AQP that leverages generative models learned over the complete database to answer SQL queries at interactive speeds. Different from classical AQP approaches, generative models allow us to compute responses to ad-hoc queries and deliver high-quality estimates also over rare subpopulations at the same time. In our experiments with real and synthetic data sets, we show that Model-based AQP can in many scenarios return more accurate results in a shorter runtime. Furthermore, we think that our techniques of using generative models presented in this paper can not only be used for AQP in databases but also has applications for other database problems including Query Optimization as well as Data Cleaning.},
	author = {Moritz Kulessa and Alejandro Molina and Carsten Binnig and Benjamin Hilprecht and Kristian Kersting},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {arXiv e-prints},
	keywords = {Computer Science - Databases,Computer Science - Machine Learning; aqp; database; sampling},
	month = {11},
	pages = {arXiv:1811.06224},
	title = {Model-based Approximate Query Processing},
	url = {http://arxiv.org/abs/1811.06224},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1811.06224}}

@inproceedings{Zhao2018,
	author = {Zhuoyue Zhao and Robert Christensen and Feifei Li and Xiao Hu and Ke Yi},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1145/3183713.3183739},
	isbn = {9781450347037},
	journal = {Proc. SIGMOD'18},
	keywords = {2018,acm reference format,and ke yi,feifei li,join sampling framework,multi-way joins,random sampling,robert christensen,xiao hu,zhuoyue zhao; aqp; database; sampling},
	note = {https://www.evernote.com/shard/s13/nl/1480559/daebf8a1-5d4e-4d9c-9833-541b0b951d32/},
	pages = {1525-1539},
	title = {Random Sampling over Joins Revisited},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3183713.3183739}}

@article{su2018miss,
	author = {Xuebin Su and Hongzhi Wang and Jianzhong Li and Hong Gao},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {arXiv preprint arXiv:1807.11054},
	keywords = {aqp; database; sampling},
	title = {MISS: Finding Optimal Sample Sizes for Approximate Analytics},
	year = {2018}}

@article{han2018efficiently,
	author = {Xixian Han and Bailing Wang and Jianzhong Li and Hong Gao},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {2},
	journal = {Knowledge and Information Systems},
	keywords = {aqp; database; sampling},
	pages = {437-473},
	publisher = {Springer},
	title = {Efficiently processing deterministic approximate aggregation query on massive data},
	volume = {57},
	year = {2018}}

@article{wang-skew-aware-2018,
	abstract = {Summary Online aggregation is a query processing technique that returns approximate answers with error guarantees (in the form of confidence intervals) continuously during the query execution process. This approach offers users a suitable tradeoff between query efficiency and accuracy. The key issue of online aggregation is how to ensure a random sample collection's efficiency and effectiveness. However, the often-used ``blind'' sampling method does not adequately consider dataset statistics and other useful information, leading to inefficient sampling and poor sample quality. This becomes a glaring performance issue for skewed data distribution over joins. To alleviate this problem, we utilize dataset statistics to propose a new ``guided'' sampling approach, which consists of a logic-partition-based weighted Gaussian sampling method tailored for the skewed join key, as well as a two-level sample allocation method that applies to the skewed measured value. Extensive experiments using the TPC-H benchmark for skewed data distribution demonstrate our solution's superior performance.},
	author = {Yuxiang Wang and Jiahui Jin and Xiaoliang Xu and Longbin Zhang},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1002/cpe.4695},
	issue = {20},
	journal = {Concurrency and Computation: Practice and Experience},
	keywords = {approximate query processing,online aggregation,random sampling,skewed distribution; aqp; database; sampling},
	note = {e4695 cpe.4695},
	pages = {e4695},
	title = {Skew-aware online aggregation over joins through guided sampling},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4695},
	volume = {30},
	year = {2018},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4695},
	bdsk-url-2 = {https://doi.org/10.1002/cpe.4695}}

@article{zhou-hermes18,
	abstract = {We propose a sampling-based framework for privacy-preserving approximate data search in the context of big data. The framework is designed to bridge multi-target query needs from users and the data platform, including required query accuracy, timeliness, and query privacy constraints. A novel privacy metric, (ε, δ)-approximation, is presented to uniformly measure accuracy, efficiency and privacy breach risk. Based on this, we employ bootstrapping to efficiently produce approximate results that meet the preset query requirements. Moreover, we propose a quick response mechanism to deal with homogeneous queries, and discuss the reusage of results when appending data. Theoretical analyses and experimental results demonstrate that the framework is capable of effectively fulfilling multi-target query requirements with high efficiency and accuracy.},
	author = {Z Zhou and H Zhang and S Li and X Du},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ACCESS.2017.2788013},
	issn = {2169-3536},
	journal = {IEEE Access},
	keywords = {sampling; aqp; bootstrap; database},
	pages = {20009-20020},
	title = {Hermes: A Privacy-Preserving Approximate Search Framework for Big Data},
	volume = {6},
	year = {2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxEBBi4uL0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL1pob3UgZXQgYWwuIC0gMjAxOCAtIEhlcm1lcyBBIFByaXZhY3ktUHJlc2VydmluZyBBcHByb3hpbWF0ZSBTZWFyY2ggRnJhbWV3b3JrIGZvciBCaWcgRGF0YS5wZGZPEQO0AAAAAAO0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fWmhvdSBldCBhbC4gLSAyMDE4I0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAkAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgEPLzpVc2VyczpmeXU6TGlicmFyeTpHcm91cCBDb250YWluZXJzOlVCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGU6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleDpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eTpkb2M6bXlfbGlicmFyeTpwZGY6WmhvdSBldCBhbC4gLSAyMDE4IC0gSGVybWVzIEEgUHJpdmFjeS1QcmVzZXJ2aW5nIEFwcHJveGltYXRlIFNlYXJjaCBGcmFtZXdvcmsgZm9yIEJpZyBEYXRhLnBkZgAADgC+AF4AWgBoAG8AdQAgAGUAdAAgAGEAbAAuACAALQAgADIAMAAxADgAIAAtACAASABlAHIAbQBlAHMAIABBACAAUAByAGkAdgBhAGMAeQAtAFAAcgBlAHMAZQByAHYAaQBuAGcAIABBAHAAcAByAG8AeABpAG0AYQB0AGUAIABTAGUAYQByAGMAaAAgAEYAcgBhAG0AZQB3AG8AcgBrACAAZgBvAHIAIABCAGkAZwAgAEQAYQB0AGEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAQ1Vc2Vycy9meXUvTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvWmhvdSBldCBhbC4gLSAyMDE4IC0gSGVybWVzIEEgUHJpdmFjeS1QcmVzZXJ2aW5nIEFwcHJveGltYXRlIFNlYXJjaCBGcmFtZXdvcmsgZm9yIEJpZyBEYXRhLnBkZgAAEwABLwAAFQACAAr//wAAAAgADQAaACQBLgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATm},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2017.2788013}}

@article{wang2018aqp,
	author = {Yuxiang Wang and Yixing Xia and Qiming Fang and Xiaoliang Xu},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Journal of computational science},
	keywords = {aqp; database; sampling},
	pages = {419-431},
	publisher = {Elsevier},
	title = {AQP++: a hybrid approximate query processing framework for generalized aggregation queries},
	volume = {26},
	year = {2018}}

@inproceedings{peng2018aqp,
	author = {Jinglin Peng and Dongxiang Zhang and Jiannan Wang and Jian Pei},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	institution = {ACM},
	journal = {Proceedings of the 2018 International Conference on Management of Data},
	keywords = {aqp; database; sampling},
	pages = {1477-1492},
	title = {AQP++: connecting approximate query processing with aggregate precomputation for interactive analytics},
	url = {https://www.evernote.com/shard/s13/nl/1480559/6b172c17-7c87-4a12-88b4-a1f95eb0435f/},
	year = {2018},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/6b172c17-7c87-4a12-88b4-a1f95eb0435f/}}

@inproceedings{park2018verdictdb,
	author = {Yongjoo Park and Barzan Mozafari and Joseph Sorenson and Junhao Wang},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	institution = {ACM},
	journal = {Proc. SIGMOD'18},
	keywords = {aqp; database; sampling},
	pages = {1461-1476},
	title = {VerdictDB: universalizing approximate query processing},
	url = {https://www.evernote.com/shard/s13/nl/1480559/6b172c17-7c87-4a12-88b4-a1f95eb0435f/},
	year = {2018},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/6b172c17-7c87-4a12-88b4-a1f95eb0435f/}}

@inproceedings{he2018demonstration,
	author = {Wen He and Yongjoo Park and Idris Hanafi and Jacob Yatvitskiy and Barzan Mozafari},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	institution = {ACM},
	journal = {Proceedings of the 2018 International Conference on Management of Data},
	keywords = {aqp; database; sampling},
	pages = {1665-1668},
	title = {Demonstration of VerdictDB, the platform-independent AQP system},
	url = {https://www.evernote.com/shard/s13/nl/1480559/6b172c17-7c87-4a12-88b4-a1f95eb0435f/},
	year = {2018},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/6b172c17-7c87-4a12-88b4-a1f95eb0435f/}}

@article{li2018approximate,
	author = {Kaiyu Li and Guoliang Li},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {4},
	journal = {Data Science and Engineering},
	keywords = {aqp; database; sampling},
	pages = {379-397},
	publisher = {Springer},
	title = {Approximate query processing: what is new and where to go?},
	volume = {3},
	year = {2018}}

@article{zong2018iht,
	abstract = {The Horvitz-Thompson (HT) estimator is widely used in survey sampling. However, the variance of the HT estimator becomes large when the inclusion probabilities are highly heterogeneous. To overcome this shortcoming, in this paper, a hard-threshold method is used for the first-order inclusion probabilities, that is, we carefully choose a threshold value, then replace the inclusion probabilities smaller than the threshold by the threshold. By this shrinkage strategy, we propose a new estimator called improved Horvitz-Thompson (IHT) estimator to estimate the population total. The IHT estimator increases the estimation accuracy although it brings bias which is relatively small. We derive the IHT estimator's MSE and its unbiased estimator, and theoretically compare the IHT estimator with the HT estimator. We also apply our idea to construct the improved ratio estimator. We numerically analyze simulated and real data sets to illustrate that the proposed estimators are more efficient and robust than the classical estimators.},
	author = {Xianpeng Zong and Rong Zhu and Guohua Zou},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	issue = {2016},
	journal = {arXiv preprint arXiv:1804.04255},
	keywords = {bility sampling,horvitz-thompson estimator,ratio estimator,robustness,sampling without replacement,unequal proba-; aqp; database; sampling},
	pages = {1-22},
	title = {Improved Horvitz-Thompson Estimator in Survey Sampling},
	url = {http://arxiv.org/abs/1804.04255},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1804.04255}}

@article{li2018-baq,
	abstract = {Recently, approximate query processing (AQP) has been proposed to enable online approximate OLAP. However, existing AQP methods have some limitations. First, they may involve unacceptable errors on skewed data (e.g., long-tail distribution). Second, they require to store large amount of data and have no significant performance improvement. Third, they only support a small subset of SQL aggregation queries. To overcome these limitations, we propose a bounded approximate query processing framework BAQ. Given a predefined error bound and a set of queries, BAQ judiciously selects high-quality samples from the data to generate a unified synopsis offline, and then uses the synopsis to answer online queries. Compared with existing methods, BAQ has the following salient features. (1) BAQ does not need to generate a synopsis for each query while it only generates a unified synopsis, and thus BAQ has much smaller synopsis. (2) has 100% confidence that the query results computed based on the synopsis must be within the error bound for most SQL aggregation queries. Experimental results on real datasets show that BAQ significantly outperforms state-of-the-art approaches. For example, on a real Microsoft dataset, BAQ has 10-100&#x00D7; improvement on synopsis size and 10-100&#x00D7; improvement on the error compared with state-of-the-art algorithms.},
	author = {Kaiyu Li and Yong Zhang and Guoliang Li and Wenbo Tao and Ying Yan},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/TKDE.2018.2877362},
	issn = {15582191},
	issue = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Approximate Query Processing,Bounded Error,Data Integration,Sampling,Synopsis; aqp; database},
	title = {Bounded Approximate Query Processing},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2018.2877362}}

@article{Quoc2018-spark,
	author = {D Le Quoc and I Ekin Akkus and Pramod Bhatotia and Spyros Blanas and Ruichuan Chen and Christof Fetzer and Thorsten Strufe and Do Le Quoc and Istemi Ekin Akkus and Pramod Bhatotia and Spyros Blanas and Ruichuan Chen and Christof Fetzer and Thorsten Strufe},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Databases,Computer Science - Distributed,Parallel,and Cluster Computing; aqp; database; sampling},
	month = {5},
	title = {Approximate Distributed Joins in Apache Spark},
	url = {http://arxiv.org/abs/1805.05874},
	volume = {abs/1805.0},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1805.05874}}

@inproceedings{Cai2019,
	author = {Walter Cai and Magdalena Balazinska and Dan Suciu},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	isbn = {9781450356435},
	journal = {Proc. SIGMOD},
	keywords = {aqp; database; sampling},
	title = {Pessimistic Cardinality Estimation : Tighter Upper Bounds for Intermediate Join Cardinalities},
	year = {2019}}

@techreport{wang2019-improved,
	author = {Taining Wang and Chee-yong Chan},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	keywords = {aqp; database; sampling},
	pages = {1-15},
	title = {Improved Correlated Sampling for Join Size Estimation},
	year = {2019}}

@inproceedings{zhao2020efficient,
	author = {Zhuoyue Zhao and Feifei Li and Yuxi Liu},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	journal = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
	keywords = {aqp; database; sampling},
	pages = {2027-2042},
	title = {Efficient Join Synopsis Maintenance for Data Warehouse},
	year = {2020}}

@inproceedings{zhang2020selectivity,
	author = {Jiaheng Lu and Chao Zhang and Jiaheng Lu},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	isbn = {9781450388146},
	journal = {32nd International Conference on Scientific and Statistical Database Management},
	keywords = {aqp; database; sampling},
	pages = {1-12},
	title = {Selectivity Estimation for Relation-Tree Joins},
	year = {2020}}

@article{dutt13efficiently,
	abstract = {Today's query optimizers use fast selectivity estimation techniques but are known to be susceptible to large estimation errors. Recent work on supervised learned models for selectivity estimation significantly improves accuracy while ensuring relatively low estimation overhead. However, these models impose significant model construction cost as they need large numbers of training examples and computing se-lectivity labels is costly for large datasets. We propose a novel model construction method that incrementally generates training data and uses approximate selectivity labels, that reduces total construction cost by an order of magnitude while preserving most of the accuracy gains. The proposed method is particularly attractive for model designs that are faster-to-train for a given number of training examples , but such models are known to support a limited class of query expressions. We broaden the applicability of such supervised models to the class of select-project-join query expressions with range predicates and IN clauses. Our extensive evaluation on synthetic benchmark and real-world queries shows that the 95th-percentile error of our proposed models is 10-100× better than traditional selectivity estima-tors. We also demonstrate significant gains in plan quality as a result of improved selectivity estimates.},
	author = {Anshuman Dutt and Chi Wang and Vivek Narasayya and Surajit Chaudhuri},
	date-added = {2022-01-10 07:23:06 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.14778/3407790.3407820},
	issue = {11},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {aqp; database; sampling},
	pages = {2215-2228},
	title = {Efficiently Approximating Selectivity Functions using Low Overhead Regression Models},
	url = {https://doi.org/10.14778/3407790.3407820},
	volume = {13},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.14778/3407790.3407820}}

@misc{fdblp,
	author = {J"org Diederich},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {Faceted \{\{\}DBLP\{\}\}}}

@misc{tpch-skew,
	author = {S Chaudhuri and V Narasayya},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {Program for TPC-D Data Generation with Skew.}}

@misc{tpch,
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {TPC-H Benchmark}}

@article{Conway,
	author = {Neil Conway and Software Technology},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {Inside the PostgreSQL Query Optimizer}}

@article{Peng,
	author = {Peng Peng and Lei Zou and M Tamer {\"O}zsu and Lei Chen and Dongyan Zhao},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {Processing SPARQL Queries Over Linked Data --- A Distributed Graph-based Approach}}

@misc{dblp,
	author = {Michael Ley},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {The \{\{\}DBLP\{\}\} Computer Science Bibliography}}

@article{Bre,
	author = {Sebastian Bre{\ss} and Eike Schallehn and Ingolf Geist},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	pages = {27-35},
	title = {Towards Optimization of Hybrid CPU / GPU Query Plans in Database Systems}}

@misc{jbirch,
	author = {Roberto Perdisci},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {\{\{\}JBIRCH\{\}\}}}

@article{Kaczmarski,
	author = {Krzysztof Kaczmarski and Pawe{\l} Rz{\k a}{\.z}ewski},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	pages = {37-46},
	title = {Thrust and CUDA in Data Intensive Algorithms}}

@article{Bhargava,
	author = {Bharat Bhargava},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	pages = {1-22},
	title = {Concurrency Control in Database Systems 1 Introduction 2 Concurrency Control Approaches and Algorithms Generic Approaches to Synchronization}}

@article{hoe63,
	author = {Wassily Hoeffding},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issn = {01621459},
	issue = {301},
	journal = {Journal of the American Statistical Association},
	keywords = {database},
	pages = {pp. 13--30},
	publisher = {American Statistical Association},
	title = {Probability Inequalities for Sums of Bounded Random Variables},
	url = {http://www.jstor.org/stable/2282952},
	volume = {58},
	year = {1963},
	bdsk-url-1 = {http://www.jstor.org/stable/2282952}}

@article{smit75,
	author = {John Miles Smith and Philip Yen-Tang Chang},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/361020.361025},
	issn = {0001-0782},
	issue = {10},
	journal = {Commun. ACM},
	keywords = {automatic programming,data manipulation language,database optimization,inverted file,query language,relational database,very high level language; database},
	month = {10},
	pages = {568-579},
	publisher = {ACM},
	title = {Optimizing the performance of a relational algebra database interface},
	url = {http://doi.acm.org/10.1145/361020.361025},
	volume = {18},
	year = {1975},
	bdsk-url-1 = {http://doi.acm.org/10.1145/361020.361025}}

@article{wong76,
	author = {Eugene Wong and Karel Youssefi},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {ACM Transactions on Database Systems},
	keywords = {database},
	pages = {223-241},
	title = {Decomposition - a strategy for query processing},
	volume = {1},
	year = {1976}}

@book{coch77,
	author = {William G Cochran},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0-471-16240-X},
	keywords = {database},
	publisher = {John Wiley},
	title = {Sampling Techniques, 3rd Edition},
	year = {1977}}

@article{Comer1979,
	abstract = {B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and implemented using B-trees. This paper reviews B-trees and shows why they have been so successful. It discusses the major variations of the B-tree, especially the B+-tree, contrasting the relative merits and costs of each implementation. It illustrates a general purpose access method which uses a B-tree.},
	author = {Douglas Comer},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/356770.356776},
	issn = {03600300},
	issue = {2},
	journal = {ACM Computing Surveys},
	keywords = {b-tree; database},
	pages = {121-137},
	title = {Ubiquitous B-Tree},
	volume = {11},
	year = {1979},
	bdsk-url-1 = {https://doi.org/10.1145/356770.356776}}

@phdthesis{kooi80,
	author = {Robert Philip Kooi},
	city = {Cleveland, OH, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	note = {AAI8109596},
	publisher = {Case Western Reserve University},
	title = {The optimization of queries in relational databases},
	year = {1980}}

@book{ross80,
	author = {Sheldon M Ross},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	publisher = {Academic Press, Inc.},
	title = {Introduction to Probability Models, 2nd Edition},
	year = {1980}}

@article{Bernstein1981,
	abstract = {In this paper we survey, consolidate, and present the state of the art in distributed database concurrency control. The heart of our analysts is a decomposition of the concurrency control problem into two major subproblems: read-write and write-write synchronization. We describe a series of synchromzation techniques for solving each},
	author = {Philip a. Bernstein and Nathan Goodman},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/356842.356846},
	issn = {03600300},
	issue = {2},
	journal = {ACM Computing Surveys},
	keywords = {concurrency; database},
	pages = {185-221},
	title = {Concurrency Control in Distributed Database Systems},
	volume = {13},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1145/356842.356846}}

@article{Yu1984,
	abstract = {In this paper the problem of finding an optimum strategy of semi joins for solving tree queries is studied under the objective of total time minimization. Tree queries that are conjunctions of equi-join clauses such that any two relations in the query have at most one attribute in common are considered. This class of tree queries is a superset of classes of tree queries, such as chain queries and simple queries, that have been studied for semi-join optimization in the literature. An algorithm based on dynamic programming to find the optimum semi-join strategy for a given query is presented. The search space for finding the optimum is reduced by eliminating strategies that can never be the optimum. This is accomplished by utilizing a set of properties that a potentially optimum strategy should satisfy. \{{\copyright}\} 1984.},
	author = {Clement T Yu and Z Meral Ozsoyoglu and K Lam},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1016/0022-0000(84)90007-2},
	issn = {10902724},
	issue = {3},
	journal = {Journal of Computer and System Sciences},
	keywords = {distributed B-tree,rnal of computer and,system sciences; database},
	pages = {409-445},
	title = {Optimization of distributed tree queries},
	volume = {29},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1016/0022-0000(84)90007-2}}

@article{piat84,
	author = {Gregory Piatetsky-Shapiro and Charles Connell},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/971697.602294},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {6},
	pages = {256-276},
	publisher = {ACM},
	title = {Accurate estimation of the number of tuples satisfying a condition},
	url = {http://doi.acm.org/10.1145/971697.602294},
	volume = {14},
	year = {1984},
	bdsk-url-1 = {http://doi.acm.org/10.1145/971697.602294}}

@article{chri84,
	author = {S Christodoulakis},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/329.318578},
	issn = {0362-5915},
	issue = {2},
	journal = {ACM Trans. Database Syst.},
	keywords = {database},
	month = {6},
	pages = {163-186},
	publisher = {ACM},
	title = {Implications of certain assumptions in database performance evauation},
	url = {http://doi.acm.org/10.1145/329.318578},
	volume = {9},
	year = {1984},
	bdsk-url-1 = {http://doi.acm.org/10.1145/329.318578}}

@article{Jarke1984,
	abstract = {... 11 Queries A query is a language expression that de- scribes data to be retrieved from a database . ... However, an automatic query optimization system be- comes necessary if ad hoc queries are to be asked by use of a general-purpose query language. ... \n},
	author = {Matthias Jarke and Jurgen Koch},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/356924.356928},
	isbn = {1581133324},
	issn = {03600300},
	issue = {2},
	journal = {ACM Computing Surveys (CSUR)},
	keywords = {database},
	pages = {111-152},
	title = {Query Optimization in Database Systems},
	url = {http://portal.acm.org/citation.cfm?doid=356924.356928%5Cnpapers3://publication/doi/10.1145/356924.356928},
	volume = {16},
	year = {1984},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=356924.356928%5Cnpapers3://publication/doi/10.1145/356924.356928},
	bdsk-url-2 = {https://doi.org/10.1145/356924.356928}}

@article{vitt85,
	author = {Jeffrey S Vitter},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/3147.3165},
	issn = {0098-3500},
	issue = {1},
	journal = {ACM Trans. Math. Softw.},
	keywords = {database},
	month = {3},
	pages = {37-57},
	publisher = {ACM},
	title = {Random sampling with a reservoir},
	url = {http://doi.acm.org/10.1145/3147.3165},
	volume = {11},
	year = {1985},
	bdsk-url-1 = {http://doi.acm.org/10.1145/3147.3165}}

@inproceedings{Lehman:1986,
	author = {Tobin J Lehman and Michael J Carey},
	city = {San Francisco, CA, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0-934613-18-4},
	journal = {Proceedings of the 12th International Conference on Very Large Data Bases},
	keywords = {database},
	pages = {294-303},
	publisher = {Morgan Kaufmann Publishers Inc.},
	title = {A Study of Index Structures for Main Memory Database Management Systems},
	url = {http://dl.acm.org/citation.cfm?id=645913.671312},
	year = {1986},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=645913.671312}}

@article{Ioannidis1987a,
	abstract = {Query optimizers of future database management systems are likely to face large access plan spaces in their task. Exhaustively searching such access plan spaces is unacceptable. We propose a query optimization algorithm based on simulated annealing, which is a probabilistic hill climbing algorithm. We show the specific formulation of the algorithm for the case of optimizing complex non-recursive queries that arise in the study of linear recursion. The query answer is explicitly represented and manipulated within the closed semiring of linear relational operators. The optimization algorithm is applied to a state space that is constructed from the equivalent algebraic forms of the query answer. A prototype of the simulated annealing algorithm has been built and few experiments have been performed for a limited class of relational operators. Our initial experience is that, in general, the algorithm converges to processing strategies that are very close to the optimal. Moreover, the traditional processing strategies (e.g., the semi-naive evaluation) have been found to be, in general, suboptimal.},
	author = {Yannis E. Ioannidis and Eugene Wong},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/38714.38722},
	isbn = {0-89791-236-5},
	issn = {0163-5808},
	issue = {3},
	journal = {SIGMOD Record},
	keywords = {database},
	pages = {9--22},
	title = {Query Optimization by Simulated Annealing},
	url = {https://www.evernote.com/shard/s13/nl/1480559/9335d23a-0760-4980-aabb-9efa911008df/},
	volume = {16},
	year = {1987},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/9335d23a-0760-4980-aabb-9efa911008df/},
	bdsk-url-2 = {https://doi.org/10.1145/38714.38722}}

@article{valduriez1987,
	author = {Patrick Valduriez},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {2},
	journal = {ACM Transactions on Database Systems (TODS)},
	keywords = {database},
	pages = {218-246},
	publisher = {ACM},
	title = {Join indices},
	volume = {12},
	year = {1987}}

@inproceedings{mura88,
	author = {M Muralikrishna and David J DeWitt},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. SIGMOD'88},
	keywords = {database},
	pages = {28-36},
	title = {Equi-Depth Histograms For Estimating Selectivity Factors For Multi-Dimensional Queries},
	year = {1988}}

@article{Swama1988,
	abstract = {We investigate the problem of optimizing Select---Project---Join queries with large numbers of joins. Taking advantage of commonly used heuristics, the problem is reduced to that of determining the optimal join order. This is a hard combinatorial optimization problem. Some general techniques, such as iterative improvement and simulated annealing, have often proved effective in attacking a wide variety of combinatorial optimization problems. In this paper, we apply these general algorithms to the large join query optimization problem. We use the statistical techniques of factorial experiments and analysis of variance (ANOVA) to obtain reliable values for the parameters of these algorithms and to compare these algorithms. One interesting result of our experiments is that the relatively simple iterative improvement proves to be better than all the other algorithms (included the more complex simulated annealing). We also find that the general algorithms do quite well at the maximum time limit.},
	author = {Arun Swami and Anoop Gupta and Arun Swama and Stanford Umverslty and Arun Swami and Anoop Gupta and Arun Swama and Stanford Umverslty},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/971701.50203},
	isbn = {0897912683},
	issn = {01635808},
	issue = {3},
	journal = {ACM SIGMOD Record},
	keywords = {database},
	pages = {8-17},
	title = {Optimization of Large Join Queries},
	volume = {17},
	year = {1988},
	bdsk-url-1 = {https://doi.org/10.1145/971701.50203}}

@inproceedings{kersten89,
	author = {M L Kersten},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0-387-51295-0},
	journal = {Proc. FODO '89},
	keywords = {database},
	pages = {228-232},
	publisher = {Springer-Verlag New York, Inc.},
	title = {Using Logarithmic Code-expansion to Speedup Index Access and Maintenance},
	year = {1989}}

@article{swam89,
	author = {A Swami},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/66926.66961},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {6},
	pages = {367-376},
	publisher = {ACM},
	title = {Optimization of large join queries: combining heuristics and combinatorial techniques},
	url = {http://doi.acm.org/10.1145/66926.66961},
	volume = {18},
	year = {1989},
	bdsk-url-1 = {http://doi.acm.org/10.1145/66926.66961}}

@book{ullman1988principles,
	author = {J D Ullman},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9780881751888},
	issue = {v. 1},
	keywords = {database},
	publisher = {Computer Science Press},
	title = {Principles of Database and Knowledge-base Systems},
	url = {https://books.google.com/books?id=vUw1AQAAMAAJ https://books.google.com/books?id=YmpqAAAAMAAJ},
	year = {1989},
	bdsk-url-1 = {https://books.google.com/books?id=vUw1AQAAMAAJ%20https://books.google.com/books?id=YmpqAAAAMAAJ}}

@article{57061,
	author = {S Pramanik and M Kim},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/12.57061},
	issn = {0018-9340},
	issue = {9},
	journal = {IEEE Transactions on Computers},
	keywords = {B-trees,Computer science,Concurrent computing,Costs,Database systems,Degradation,Delay,Indexing,Parallel algorithms,Parallel processing,Proposals,concurrency,large node B-trees,parallel algorithms,parallel processing,response time,tree height,trees (mathematics); database},
	month = {9},
	pages = {1208-1212},
	title = {Parallel processing of large node B-trees},
	volume = {39},
	year = {1990},
	bdsk-url-1 = {https://doi.org/10.1109/12.57061}}

@article{ioan90,
	author = {Y E Ioannidis and Younkyung Kang},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/93605.98740},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {5},
	pages = {312-321},
	publisher = {ACM},
	title = {Randomized algorithms for optimizing large join queries},
	url = {http://doi.acm.org/10.1145/93605.98740},
	volume = {19},
	year = {1990},
	bdsk-url-1 = {http://doi.acm.org/10.1145/93605.98740}}

@article{ioan91,
	author = {Yannis E Ioannidis and Younkyung Cha Kang},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/119995.115813},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {4},
	pages = {168-177},
	publisher = {ACM},
	title = {Left-deep vs. bushy trees: an analysis of strategy spaces and its implications for query optimization},
	url = {http://doi.acm.org/10.1145/119995.115813},
	volume = {20},
	year = {1991},
	bdsk-url-1 = {http://doi.acm.org/10.1145/119995.115813}}

@article{haas92,
	author = {Peter J Haas and Arun N Swami},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/141484.130335},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {6},
	pages = {341-350},
	title = {Sequential sampling procedures for query size estimation},
	url = {http://doi.acm.org/10.1145/141484.130335},
	volume = {21},
	year = {1992},
	bdsk-url-1 = {http://doi.acm.org/10.1145/141484.130335}}

@article{Mishra1992,
	abstract = {To design a relational database integrating clinical and basic science data needed for multidisciplinary treatment and research in the field of vascular anomalies. Based on data points agreed on by the American Society of Pediatric Otolaryngology (ASPO) Vascular Anomalies Task Force. The database design enables sharing of data subsets in a Health Insurance Portability and Accountability Act (HIPAA)-compliant manner for multisite collaborative trials. Vascular anomalies pose diagnostic and therapeutic challenges. Our understanding of these lesions and treatment improvement is limited by nonstandard terminology, severity assessment, and measures of treatment efficacy. The rarity of these lesions places a premium on coordinated studies among multiple participant sites.},
	author = {Priti Mishra and Margaret H. Eich},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/128762.128764},
	issn = {03600300},
	issue = {1},
	journal = {ACM Computing Surveys},
	keywords = {database},
	pages = {63-113},
	pmid = {18209139},
	title = {Join processing in relational databases},
	url = {http://portal.acm.org/citation.cfm?doid=128762.128764},
	volume = {24},
	year = {1992},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=128762.128764},
	bdsk-url-2 = {https://doi.org/10.1145/128762.128764}}

@inproceedings{haas93,
	author = {Peter J Haas and Jeffrey F Naughton and S Seshadri and Arun N Swami},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/153850.153875},
	isbn = {0-89791-593-3},
	keywords = {database},
	pages = {190-201},
	title = {Fixed-precision estimation of join selectivity},
	url = {http://doi.acm.org/10.1145/153850.153875},
	year = {1993},
	bdsk-url-1 = {http://doi.acm.org/10.1145/153850.153875}}

@article{Graefe1993a,
	abstract = {Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate the problem: In order to manipulate large sets of complex objects as efficiently as today's database systems manipulate simple records, query-processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and postrelational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set-matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.},
	author = {Goetz Graefe},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/152610.152611},
	isbn = {3527299785},
	issn = {03600300},
	issue = {2},
	journal = {ACM Computing Surveys},
	keywords = {Complex query evaluation plans,dynamic query evaluation plans,extensible databa,iterators,object-oriented database systems,operator model of parallelization,parallel algorithms,relational database systems,set-matching algorithms,sort-hash duality; database},
	pages = {73-169},
	title = {Query evaluation techniques for large databases},
	volume = {25},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1145/152610.152611}}

@article{Steinbrunn1993a,
	abstract = {Recent developments in database technology, such as deductive database systems, have given rise to the demand for new, cost-effective optimization techniques for join expressions. In this paper many different algorithms that compute approximative solutions for optimizing join orders are studied since traditional dynamic programming techniques are not appropriate for complex problems. First, two possible solution spaces, the space of left-deep and bushy processing trees, respectively, are evaluated from a statistical point of view. The result is that the common limitation to left-deep processing trees is, from a purely statistical point of view, only advisable for certain cost models. Basically, optimizers from three classes are analysed: heuristic, randomized and genetic algorithms. Each one is extensively scrutinized with respect to its working principle and its fitness for the desired application. It turns out that randomized and genetic algorithms are well suited for optimizing join expressions. They generate solutions of high quality within a reasonable running time. The benefits of heuristic optimizers, namely the short running time, are often outweighed by merely moderate optimization performance.},
	author = {Michael Steinbrunn and Guido Moerkotte and Alfons Kemper},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	pages = {3},
	title = {Optimizing Join Orders},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.8155%7B&%7Drep=rep1%7B&%7Dtype=pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.8155&rep=rep1&type=pdf},
	year = {1993},
	bdsk-url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.8155%7B&%7Drep=rep1%7B&%7Dtype=pdf%20http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.8155&rep=rep1&type=pdf}}

@article{250116,
	author = {J W Stamos and H C Young},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/71.250116},
	issn = {2161-9883},
	issue = {12},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Broadcasting,Computational efficiency,Costs,Database systems,Distributed algorithms,Multicast algorithms,Partitioning algorithms,Prototypes,Relational databases,SFR,Unicast,computational complexity,database theory,distributed algorithms,distributed join,distributed joins,fragment and replicate algorithm,intra transaction parallelism,load balancing,multicast communication,parallel database,performance evaluation,relational data model,symmetric fragment and replicate,symmetry,tuple balancing,worst case cost; database},
	month = {12},
	pages = {1345-1354},
	title = {A symmetric fragment and replicate algorithm for distributed joins},
	url = {https://www.evernote.com/shard/s13/nl/1480559/1896e14f-794e-47ae-acfd-00dc971cab42/},
	volume = {4},
	year = {1993},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/1896e14f-794e-47ae-acfd-00dc971cab42/},
	bdsk-url-2 = {https://doi.org/10.1109/71.250116}}

@article{Graefe1995,
	abstract = {This paper describes a new extensible query optimization framework that resolves many of the short- comings of the EXODUS and Volcano optimizer generators. In addition to extensibility, dynamic pro- gramming, and memorization based on and extended from the EXODUS and Volcano prototypes, this new optimizer provides (i) manipulation of operator arguments using rules or functions, (ii) operators that are both logical and physical for predicates etc., (iii) schema-specific rules for materialized views, (iv) rules to insert ''enforcers'' or ''glue operators,'' (v) rule-specific guidance, permitting grouping of rules, (vi) basic facilities that will later permit parallel search, partially ordered cost measures, and dy- namic plans, (vii) extensive tracing support, and (viii) a clean interface and implementation making full use of the abstraction mechanisms of C++. We describe and justify our design choices for each of these issues. The optimizer system described here is operational and will serve as the foundation for new query optimizers in Tandem's NonStop SQL product and in Microsoft's SQL Server product.},
	author = {Goetz Graefe},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {3},
	journal = {Data Engineering Bulletin},
	keywords = {database},
	pages = {19-29},
	title = {The Cascades framework for query optimization},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9460%7B&%7Drep=rep1%7B&%7Dtype=pdf},
	volume = {18},
	year = {1995},
	bdsk-url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9460%7B&%7Drep=rep1%7B&%7Dtype=pdf}}

@article{zhan96,
	author = {Tian Zhang and Raghu Ramakrishnan and Miron Livny},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/235968.233324},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {6},
	pages = {103-114},
	publisher = {ACM},
	title = {BIRCH: an efficient data clustering method for very large databases},
	url = {http://doi.acm.org/10.1145/235968.233324},
	volume = {25},
	year = {1996},
	bdsk-url-1 = {http://doi.acm.org/10.1145/235968.233324}}

@article{Ioannidis1996,
	abstract = {A brief introduction to query optimization.},
	author = {Yannis E Ioannidis},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/234313.234367},
	isbn = {3131267410},
	issn = {03600300},
	issue = {1},
	journal = {ACM Computing Surveys},
	keywords = {database},
	pages = {121-123},
	title = {Query optimization},
	volume = {28},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1145/234313.234367}}

@inproceedings{alon96,
	author = {Noga Alon and Yossi Matias and Mario Szegedy},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {JOURNAL OF COMPUTER AND SYSTEM SCIENCES},
	keywords = {database},
	pages = {20-29},
	title = {The space complexity of approximating the frequency moments},
	year = {1996}}

@article{o1996log,
	author = {Patrick O'Neil and Edward Cheng and Dieter Gawlick and Elizabeth O'Neil and Patrick O Neil and Edward Cheng and Dieter Gawlick and Elizabeth O Neil and Patrick O'Neil and Edward Cheng and Dieter Gawlick and Elizabeth O'Neil},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {4},
	journal = {Acta Informatica},
	keywords = {database},
	pages = {351-385},
	publisher = {Springer},
	title = {The log-structured merge-tree (LSM-tree)},
	volume = {33},
	year = {1996}}

@inproceedings{poos97,
	author = {Viswanath Poosala and Yannis E Ioannidis},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1-55860-470-7},
	keywords = {database},
	pages = {486-495},
	title = {Selectivity Estimation Without the Attribute Value Independence Assumption},
	url = {http://dl.acm.org/citation.cfm?id=645923.673638},
	year = {1997},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=645923.673638}}

@article{Steinbrunn1997,
	abstract = {Recent developments in database technology, such as deductive database systems, have given rise to the demand for new, cost-effective optimization techniques for join expressions. In this paper many different algorithms that compute approximate solutions for optimizing join orders are studied since traditional dynamic programming techniques are not appropriate for complex problems. Two possible solution spaces, the space of left-deep and bushy processing trees, are evaluated from a statistical point of view. The result is that the common limitation to left-deep processing trees is only advisable for certain join graph types. Basically, optimizers from three classes are analysed: heuristic, randomized and genetic algorithms. Each one is extensively scrutinized with respect to its working principle and its fitness for the desired application. It turns out that randomized and genetic algorithms are well suited for optimizing join expressions. They generate solutions of high quality within a reasonable running time. The benefits of heuristic optimizers, namely the short running time, are often outweighed by merely moderate optimization performance.},
	author = {Michael Steinbrunn and Guido Moerkotte and Alfons Kemper},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s007780050040},
	issn = {1066-8888},
	issue = {3},
	journal = {The VLDB Journal The International Journal on Very Large Data Bases},
	keywords = {Genetic algorithms,Heuristic algorithms,Join ordering,Query optimization,Randomized algorithms; database},
	pages = {191-208},
	title = {Heuristic and randomized optimization for the join ordering problem},
	url = {http://dl.acm.org/citation.cfm?id=765554.765556},
	volume = {6},
	year = {1997},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=765554.765556},
	bdsk-url-2 = {https://doi.org/10.1007/s007780050040}}

@inproceedings{gibb97,
	author = {Phillip B Gibbons and Yossi Matias and Viswanath Poosala},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. VLDB'97},
	keywords = {database},
	pages = {466-475},
	title = {Fast Incremental Maintenance of Approximate Histograms},
	year = {1997}}

@article{Karger1997,
	abstract = {We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing net- work protocols such as TCP/fF', and require very little overhead. The protocols work with local control, make efficient use of exist- ing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes nr.inimafly as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.},
	author = {David Karger and Eric Lehman and Tom Leighton and Rina Panigrahy and Matthew Levine and Daniel Lewin},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/258533.258660},
	isbn = {0897918886},
	issn = {0012821X},
	journal = {Proceedings of the twenty-ninth annual ACM symposium on Theory of computing - STOC '97},
	keywords = {consistent-hashing; database},
	pages = {654-663},
	title = {Consistent Hashing and Random Trees},
	url = {http://dl.acm.org/citation.cfm?id=258660$%5C$nhttp://portal.acm.org/citation.cfm?doid=258533.258660 http://dl.acm.org/citation.cfm?id=258660%5Cnhttp://portal.acm.org/citation.cfm?doid=258533.258660},
	year = {1997},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=258660$%5C$nhttp://portal.acm.org/citation.cfm?doid=258533.258660%20http://dl.acm.org/citation.cfm?id=258660%5Cnhttp://portal.acm.org/citation.cfm?doid=258533.258660},
	bdsk-url-2 = {https://doi.org/10.1145/258533.258660}}

@article{Evrendilek1997,
	abstract = {A multidatabase system (MDBS) allows the users to simultaneously access heterogeneous, and autonomous databases using an integrated schema and a single global query language. The query optimization problem in MDBSs is quite different from the query optimization problem in distributed homogeneous databases due to schema heterogeneity and autonomy of local database systems. In this work, we consider the optimization of query distribution in case of data replication and the optimization of intersite joins, that is, the join of the results returned by the local sites in response to the global subqueries. The algorithms presented for the optimization of intersite joins try to maximize the parallelism in execution and take the federated nature of the problem into account. It has also been shown through a comparative performance study that the proposed intersite join optimization algorithms are efficient. The approach presented can easily be generalized to any operation required for intersite query processing. The query optimization scheme presented in this paper is being implemented within the scope of a multidatabase system which is based on OMG`s object management architecture.},
	author = {Cem Evrendilek and Asuman Dogac and Sena Nural and Fatma Ozcan},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issn = {09268782},
	journal = {Distributed and Parallel Databases},
	keywords = {heterogeneity,multidatabases,query optimization; database},
	pages = {77-114},
	title = {Multidatabase Query Optimization},
	volume = {5},
	year = {1997}}

@article{Torp1998,
	author = {K Torp and L Mark and C S Jensen},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/69.706059},
	issn = {1041-4347},
	issue = {4},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Banking,Computational efficiency,Data models,Database languages,Database systems,Hardware,History,Query processing,Transaction databases,Tree data structures,cached timeslices,candidate outsets,current database state,database changes,differential computation,differential timeslice computation,log based storage structure,pointer-less insertion tree,previous database states,query processing,specialized tree structure,temporal databases,temporal query processing techniques,transaction processing,transaction-time databases,tree data structures,tree structure,trees (mathematics); database},
	pages = {599-611},
	title = {Efficient differential timeslice computation},
	volume = {10},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/69.706059}}

@article{mati98,
	author = {Yossi Matias and Jeffrey Scott Vitter and Min Wang},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/276305.276344},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {6},
	pages = {448-459},
	publisher = {ACM},
	title = {Wavelet-based histograms for selectivity estimation},
	url = {http://doi.acm.org/10.1145/276305.276344},
	volume = {27},
	year = {1998},
	bdsk-url-1 = {http://doi.acm.org/10.1145/276305.276344}}

@article{Chaudhuri1998,
	abstract = {There has been extensive work in query optimization since the early `70s. It is hard to capture the breadth and depth of this large body of work in a short article. Therefore, I have decided to focus primarily on the optimization of SQL queries in relational database systems and present my biased and incomplete view of this field. The goal of this article is not to be comprehensive, but rather to explain the foundations and present samplings of significant work in this area. I would like to apologize to the many contributors in this area whose work I have failed to explicitly acknowledge due to oversight or lack of space. I take the liberty of trading technical precision for ease of presentation.},
	author = {Surajit Chaudhuri},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/275487.275492},
	isbn = {0897919963},
	issn = {15294188},
	journal = {Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems},
	keywords = {database},
	pages = {34--43},
	title = {An overview of query optimization in relational systems},
	url = {http://portal.acm.org/citation.cfm?id=275492},
	year = {1998},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?id=275492},
	bdsk-url-2 = {https://doi.org/10.1145/275487.275492}}

@article{simkovics1998enhancement,
	abstract = {It was the author's task to add the support for the two missing features to the existing source code. Before the implementation could be started an intensive study of the relevant parts of the SQL92 standard and the implementation of the existing features of PostgreSQL had been necessary. This document will not present only the results of the implementation but also the knowledge collected while studying the SQL language and the source code of the already existing features.},
	author = {Georg Gottlob and Katrin Seyr and Stefan Simkovics and Paul Petersgasse and Georg Gottlob and Katrin Seyr and Stefan Simkovics and Paul Petersgasse},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Department of Infor},
	keywords = {database},
	publisher = {na},
	title = {Enhancement of the ANSI SQL Implementation of PostgreSQL},
	url = {http://www.dcc.unicamp.br/%7B~%7Dcelio/livrobd/postgres/ansi%7B_%7Dsql%7B_%7Dimplementation%7B_%7Dpostgresql.pdf http://www.dcc.unicamp.br/~celio/livrobd/postgres/ansi_sql_implementation_postgresql.pdf},
	year = {1998},
	bdsk-url-1 = {http://www.dcc.unicamp.br/%7B~%7Dcelio/livrobd/postgres/ansi%7B_%7Dsql%7B_%7Dimplementation%7B_%7Dpostgresql.pdf%20http://www.dcc.unicamp.br/~celio/livrobd/postgres/ansi_sql_implementation_postgresql.pdf}}

@inproceedings{Rao1999,
	author = {Jun Rao and Kenneth A Ross},
	city = {San Francisco, CA, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1-55860-615-7},
	journal = {Proceedings of the 25th International Conference on Very Large Data Bases},
	keywords = {database},
	pages = {78-89},
	publisher = {Morgan Kaufmann Publishers Inc.},
	title = {Cache Conscious Indexing for Decision-Support in Main Memory},
	url = {http://dl.acm.org/citation.cfm?id=645925.671362},
	year = {1999},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=645925.671362}}

@article{lava95,
	author = {Pierre Lavall{\'e}e},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issn = {0163-5808},
	journal = {Survey methodology},
	keywords = {database},
	pages = {25-32},
	title = {Cross-sectional Weighting of Longitudinal Surveys of Individuals and House Holds Using the Weight Share Method},
	volume = {21},
	year = {1999}}

@article{lee99,
	author = {Ju-Hong Lee and Deok-Hwan Kim and Chin-Wan Chung},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	pages = {205-214},
	title = {Multi-dimensional selectivity estimation using compressed histogram information},
	volume = {28},
	year = {1999}}

@article{li1999fast,
	abstract = {Two new algorithms, "Jive-join" and "Slam-join," are proposed for computing the join of two relations using a join index. The algorithms are duals: Jive-join range-partitions input relation tuple-ids then processes each partition, while Slam-join forms ordered runs of input relation tuple-ids and then merges the results. Both algorithms make a single sequential pass through each input relation, in addition to one pass through the join index and two passes through a temporary file whose size is half that of the join index. Both algorithms require only that the number of blocks in main memory is of the order of the square root of the number of blocks in the smaller relation. By storing intermediate and final join results in a vertically partitioned fashion, our algorithms need to manipulate less data in memory at a given time than other algorithms. The algorithms are resistant to data skew and adaptive to memory fluctuations. Selection conditions can be incorporated into the algorithms. ...},
	author = {Zhe Li and Kenneth A Ross},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s007780050071},
	issn = {1066-8888},
	issue = {1},
	journal = {The VLDB Journal---The International Journal on Very Large Data Bases},
	keywords = {decision support systems,query processing; database},
	pages = {1-24},
	publisher = {Springer-Verlag New York, Inc.},
	title = {Fast joins using join indices},
	volume = {8},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1007/s007780050071}}

@article{mati00,
	author = {Yossi Matias and Jeffrey Scott J S Vitter and Min Wang},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. VLDB'00},
	keywords = {database},
	pages = {101--110},
	title = {Dynamic maintenance of wavelet-based histograms},
	year = {2000}}

@article{rao00,
	author = {Jun Rao and Kenneth A Ross},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/335191.335449},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {B+Tree; database},
	month = {5},
	pages = {475-486},
	publisher = {ACM},
	title = {Making B+- Trees Cache Conscious in Main Memory},
	url = {http://doi.acm.org/10.1145/335191.335449},
	volume = {29},
	year = {2000},
	bdsk-url-1 = {http://doi.acm.org/10.1145/335191.335449},
	bdsk-url-2 = {https://doi.org/10.1145/335191.335449}}

@inproceedings{LiJZ01-codas,
	author = {Jianzhong Li and Wenjun Sun and Yingshu Li},
	city = {Washington, DC, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {CODAS '01},
	keywords = {parallel b -tree,parallel database,parallel join algorithm; database},
	pages = {178-185},
	title = {Parallel Join Algorithms based on Parallel B- trees},
	year = {2001}}

@article{wu01,
	author = {Yi-Leh Wu and Divyakant Agrawal and Amr El Abbadi},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/376284.375724},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {cumulative frequency distribution,query estimation,random sampling,range query; database},
	month = {5},
	pages = {449-460},
	publisher = {ACM},
	title = {Applying the golden rule of sampling for query estimation},
	url = {http://doi.acm.org/10.1145/376284.375724},
	volume = {30},
	year = {2001},
	bdsk-url-1 = {http://doi.acm.org/10.1145/376284.375724},
	bdsk-url-2 = {https://doi.org/10.1145/376284.375724}}

@article{desh01,
	author = {Amol Deshpande and Minos Garofalakis and Rajeev Rastogi},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/376284.375685},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {database},
	month = {5},
	pages = {199-210},
	publisher = {ACM},
	title = {Independence is good: dependency-based histogram synopses for high-dimensional data},
	url = {http://doi.acm.org/10.1145/376284.375685},
	volume = {30},
	year = {2001},
	bdsk-url-1 = {http://doi.acm.org/10.1145/376284.375685}}

@article{Lane2003,
	author = {Tom Lane},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {July},
	journal = {Source},
	keywords = {database},
	pages = {1-34},
	title = {Recent PostgreSQL Optimizer Improvements},
	year = {2003}}

@book{rama03,
	author = {Raghu Ramakrishnan and Johannes Gehrke},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	edition = {3},
	isbn = {0072465638, 9780072465631},
	keywords = {database},
	publisher = {McGraw-Hill, Inc.},
	title = {Database Management Systems},
	year = {2003}}

@inproceedings{mark04,
	author = {Volker Markl and Vijayshankar Raman and David Simmen and Guy Lohman and Hamid Pirahesh and Miso Cilimdzic},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/1007568.1007642},
	isbn = {1-58113-859-8},
	keywords = {database},
	pages = {659-670},
	title = {Robust query processing through progressive optimization},
	url = {http://doi.acm.org/10.1145/1007568.1007642},
	year = {2004},
	bdsk-url-1 = {http://doi.acm.org/10.1145/1007568.1007642}}

@article{Bender-siam2005,
	author = {Michael A Bender and Erik D Demaine and Martin Farach-colton},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1581139861},
	issue = {2},
	journal = {SIAM Journal on Computing},
	keywords = {database},
	pages = {341-358},
	title = {Cache-oblivious b-trees ∗},
	volume = {35},
	year = {2005}}

@article{Bender2005,
	author = {Michael A Bender and Jeremy T Fineman and Seth Gilbert and Bradley C Kuszmaul},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1581139861},
	keywords = {cache-oblivious b-tree,concurrent b-tree; database},
	pages = {228-237},
	title = {Concurrent Cache-Oblivious B-Trees},
	year = {2005}}

@article{corm05,
	author = {Graham Cormode and S Muthukrishnan},
	city = {Duluth, MN, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1},
	journal = {J. Algorithms},
	keywords = {database},
	month = {4},
	pages = {58-75},
	publisher = {Academic Press, Inc.},
	title = {An improved Data Stream Summary: the Count-Min Sketch and Its Applications},
	volume = {55},
	year = {2005}}

@article{Bender2006,
	abstract = {B-trees are the data structure of choice for maintaining searchable data on disk. However, B-trees perform suboptimally when keys are long or of variable length, when keys are compressed, even when using front compression , the standard B-tree compression scheme, for range queries, and with respect to memory effects such as disk prefetching. This paper presents a cache-oblivious string B-tree (COSB-tree) data structure that is efficient in all these ways: The COSB-tree searches asymptotically optimally and inserts and deletes nearly optimally. It maintains an index whose size is proportional to the front-compressed size of the dictionary. Furthermore, unlike standard front-compressed strings, keys can be decompressed in a memory-efficient manner. It performs range queries with no extra disk seeks; in contrast, B-trees incur disk seeks when skipping from leaf block to leaf block. It utilizes all levels of a memory hierarchy efficiently and makes good use of disk locality by using cache-oblivious layout strategies.},
	author = {Michael a. Bender and Martin Farach-Colton and Bradley C Kuszmaul},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1142351.1142385},
	isbn = {1595933182},
	journal = {Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems - PODS '06},
	keywords = {15,26,32,5,and are thus typically,cache oblivious string b-tree,compression,front,keys often share large,locality preserving front,packed-memory array,prefixes,range query,rebalance,stored,using front compression,within blocks; database},
	pages = {233},
	title = {Cache-oblivious string B-trees},
	url = {http://portal.acm.org/citation.cfm?doid=1142351.1142385},
	year = {2006},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1142351.1142385},
	bdsk-url-2 = {https://doi.org/10.1145/1142351.1142385}}

@inproceedings{spie06,
	author = {Joshua Spiegel and Neoklis Polyzotis},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/1142473.1142497},
	isbn = {1-59593-434-0},
	keywords = {approximation,relational,selectivity,synopses; database},
	pages = {205-216},
	title = {Graph-based synopses for relational selectivity estimation},
	url = {http://doi.acm.org/10.1145/1142473.1142497},
	year = {2006},
	bdsk-url-1 = {http://doi.acm.org/10.1145/1142473.1142497}}

@article{Jermaine2006,
	abstract = {One of the most common operations in analytic query processing is the application of an aggregate function to the result of a relational join. We describe an algorithm called the Sort-Merge-Shrink (SMS) Join for computing the answer to such a query over large, disk-based input tables. The key innovation of the SMS join is that if the input data are clustered in a statistically random fashion on disk, then at all times, the join provides an online, statistical estimator for the eventual answer to the query as well as probabilistic confidence bounds. Thus, a user can monitor the progress of the join throughout its execution and stop the join when satisfied with the estimate's accuracy or run the algorithm to completion with a total time requirement that is not much longer than that of other common join algorithms. This contrasts with other online join algorithms, which either do not offer such statistical guarantees or can only offer guarantees so long as the input data can fit into main memory. {\copyright} 2006 ACM.},
	author = {C.a b Christopher C.a b Christopher C.a b Jermaine and A.a b Alin A.a b Alin A.a b Dobra and S.a b Subramanian S.a b Subramanian S.a b Arumugam and Shantanu b Joshi and Abhijit A.a b Pol},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1189769.1189775},
	issn = {03625915},
	issue = {4},
	journal = {ACM Transactions on Database Systems},
	keywords = {database},
	pages = {1382-1416},
	title = {The sort-Merge-Shrink join},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846208392&partnerID=40&md5=0d18e202bc0b84e0df6f5c3722e3d87c http://portal.acm.org/citation.cfm?doid=1189769.1189775},
	volume = {31},
	year = {2006},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846208392&partnerID=40&md5=0d18e202bc0b84e0df6f5c3722e3d87c%20http://portal.acm.org/citation.cfm?doid=1189769.1189775},
	bdsk-url-2 = {https://doi.org/10.1145/1189769.1189775}}

@article{Araujo2006a,
	author = {Filipe Ara{\'u}jo and Lu{\'\i}s Rodrigues},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	institution = {University of Lisbon},
	keywords = {database},
	pages = {23},
	title = {Survey on Distributed Hash Tables},
	year = {2006}}

@article{chau07,
	author = {Surajit Chaudhuri and Gautam Das and Vivek Narasayya},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1242524.1242526},
	issn = {0362-5915},
	issue = {2},
	journal = {ACM Trans. Database Syst.},
	keywords = {Random sampling,approximation,query processing; database},
	month = {6},
	publisher = {ACM},
	title = {Optimized stratified sampling for approximate query processing},
	url = {http://doi.acm.org/10.1145/1242524.1242526},
	volume = {32},
	year = {2007},
	bdsk-url-1 = {http://doi.acm.org/10.1145/1242524.1242526},
	bdsk-url-2 = {https://doi.org/10.1145/1242524.1242526}}

@article{Stonebraker2007,
	abstract = {In previous papers SC05, SBC+07, some of us predicted the end of "one size fits all" as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1-2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T. to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that the current RDBMS code lines, while attempting to be a "one size fits all" solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of "from scratch" specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow's requirements, not continue to push code lines and architectures designed for yesterday's needs.},
	author = {Michael Stonebraker and Samuel Madden and Daniel J Abadi and Stavros Harizopoulos and Nabil Hachem and Pat Helland},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1080/13264820701730900},
	isbn = {9781595936493},
	issn = {1595936491},
	issue = {2},
	journal = {Vldb},
	keywords = {database},
	pages = {1150-1160},
	title = {The End of an Architectural Era (It's Time for a Complete Rewrite)},
	url = {http://portal.acm.org/citation.cfm?id=1325851.1325981},
	volume = {12},
	year = {2007},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?id=1325851.1325981},
	bdsk-url-2 = {https://doi.org/10.1080/13264820701730900}}

@article{Bender2007,
	abstract = {A streaming B-tree is a dictionary that efficiently implements insertions and range queries. We present two cache-oblivious streaming B-trees, the shuttle tree, and the cache-oblivious lookahead array (COLA). For block-transfer size B and on N elements, the shuttle tree implements searches in optimal O ` logB+1N ´ transfers, range queries of L successive elements in optimal O ` logB+1N +L/B ´ transfers, and insertions in O `` (logB+1N)/B $Θ$(1/(loglogB) 2 ) +(log2N)/B '' transfers, which is an asymptotic speedup over traditional B-trees if B ≥ (logN) 1+c/logloglog2 N for any constant c \{>\} 1. A COLA implements searches in O(logN)transfers, range queries in O(logN +L/B)transfers, and insertions in amortized O((logN)/B) transfers, matching the bounds for a (cache-aware) buffered repository tree. A partially deamortized COLA matches these bounds but reduces the worst-case insertion cost to O(logN) if memory size M = Ω(logN). We also present a cache-aware version of the COLA, the lookahead array , which achieves the same bounds as Brodal and Fagerberg's (cache-aware) B$ε$ -tree. We compare our COLA implementation to a traditional B-tree. Our COLA implementation runs 790 times faster for random insertions, 3.1 times slower for insertions of sorted data, and 3.5 times slower for searches.},
	author = {Michael A Bender and Martin Farach-Colton and Jeremy T Fineman and Yonatan R Fogel and Bradley C Kuszmaul and Jelani Nelson},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1248377.1248393},
	isbn = {9781595936677},
	journal = {Symposium on Parallel Algorithms and Architectures (SPAA)},
	keywords = {buffered repository tree,cache-oblivious b-tree,cascading ar-,deamortized,lookahead array,ray,shuttle tree; database},
	pages = {81-92},
	title = {Cache-Oblivious Streaming B-trees Categories and Subject Descriptors},
	url = {http://supertech.csail.mit.edu/papers/sbtree.pdf},
	year = {2007},
	bdsk-url-1 = {http://supertech.csail.mit.edu/papers/sbtree.pdf},
	bdsk-url-2 = {https://doi.org/10.1145/1248377.1248393}}

@article{rashid07,
	abstract = {Modern architectures have made considerable increases in processor speed and performance. However, Database Management Systems (DBMSs) fall far short from achieving their ideal performance. DBMSs are widely used in almost every large organization. Therefore, it is important to achieve fast data retrieval and processing. Recent studies have shown that more than 50\{%\} of the execution time in database operations is spent waiting for data. CSB+-trees were introduced to speedup index structure operations, mainly the search and update. In this paper we propose a multithreading technique to utilize the two threads available in an Intel Pentium 4 Hyperthreaded (HT) platform. Our technique gains speedup ranging from 29\{%\} to 70\{%\} for dual-threaded CSB+-tree on an HT enabled platform compared to a single-thread version running on HT disabled architecture.},
	author = {L K Rashid and W M Hassanein},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/CCECE.2007.379},
	isbn = {0840-7789},
	issn = {0840-7789},
	journal = {2007 Canadian Conference on Electrical and Computer Engineering, Vols 1-3},
	keywords = {database},
	pages = {1523-1526},
	title = {Evaluating the performance of CSB+-trees on multithreaded architectures},
	url = {%7B%3C%7DGo to},
	year = {2007},
	bdsk-url-1 = {%7B%3C%7DGo%20to},
	bdsk-url-2 = {https://doi.org/10.1109/CCECE.2007.379}}

@book{lava07,
	author = {Pierre Lavall{\'e}e},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {7397},
	keywords = {database},
	publisher = {Springerverlag New York},
	title = {Indirect sampling},
	year = {2007}}

@inproceedings{fang2007,
	author = {Rui Fang and Bingsheng He and Mian Lu and Ke Yang and Naga K Govindaraju and Qiong Luo and Pedro V Sander},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1247480.1247606},
	isbn = {978-1-59593-686-8},
	journal = {Proc. of SIGMOD '07},
	keywords = {graphics processing units,query processing; database},
	pages = {1061-1063},
	publisher = {ACM},
	title = {GPUQP: Query Co-processing Using Graphics Processors},
	url = {http://doi.acm.org/10.1145/1247480.1247606},
	year = {2007},
	bdsk-url-1 = {http://doi.acm.org/10.1145/1247480.1247606},
	bdsk-url-2 = {https://doi.org/10.1145/1247480.1247606}}

@article{Aguilera2008,
	author = {Marcos K Aguilera},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0000000000000},
	keywords = {database},
	pages = {598-609},
	title = {A Practical Scalable Distributed B-Tree},
	year = {2008}}

@book{garc08,
	author = {Hector Garcia-Molina and Jeffrey D Ullman and Jennifer Widom and Hector Garcia-Molina and Jennifer Widom},
	city = {Upper Saddle River, NJ, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	edition = {2},
	isbn = {9780131873254},
	keywords = {database},
	publisher = {Prentice Hall PTR},
	title = {Database Systems: The Complete Book},
	year = {2008}}

@article{Nickolls2008,
	author = {John Nickolls and Jim Hardwick and Scott Morton and Everett Phillips},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {gpu-j; database},
	pages = {13-27},
	title = {Parallel Computing Experiences With Cuda the Cuda Programming Model Provides a Straightforward Means of},
	year = {2008}}

@article{He2008,
	abstract = {We present a novel design and implementation of relational join algorithms for new-generation graphics processing units (GPUs). The most recent GPU features include support for writing to random memory locations, efficient inter-processor communication, and a programming model for general-purpose computing. Taking advantage of these new features, we design a set of data-parallel primitives such as split and sort, and use these primitives to implement indexed or non-indexed nested-loop, sort-merge and hash joins. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. We have implemented our algorithms on a PC with an NVIDIA G80 GPU and an Intel quad-core CPU. Our GPU-based join algorithms are able to achieve a performance improvement of 2-7X over their optimized CPU-based counterparts.},
	author = {Bingsheng He and Ke Yang and Rui Fang and Mian Lu},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1376616.1376670},
	isbn = {9781605581026},
	issn = {07308078},
	journal = {Proceedings of the \{{\ldots}\}},
	keywords = {gpu-j,graphics processors,join,parallel,primitive,processing,relational database,sort; database},
	pages = {511},
	title = {Relational joins on graphics processors},
	url = {http://portal.acm.org/citation.cfm?doid=1376616.1376670$%5C$nhttp://dl.acm.org/citation.cfm?id=1376670},
	year = {2008},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1376616.1376670$%5C$nhttp://dl.acm.org/citation.cfm?id=1376670},
	bdsk-url-2 = {https://doi.org/10.1145/1376616.1376670}}

@article{Ilyas2008,
	abstract = {Efficient processing of top-k queries is a crucial requirement in many interactive environments that involve massive amounts of data. In particular, efficient top-k processing in domains such as the Web, multimedia search, and distributed systems has shown a great impact on performance. In this survey, we describe and classify top-k processing techniques in relational databases. We discuss different design dimensions in the current techniques including query models, data access methods, implementation levels, data and query certainty, and supported scoring functions. We show the implications of each dimension on the design of the underlying techniques. We also discuss top-k queries in XML domain, and show their connections to relational approaches.},
	author = {Ihab F. Ilyas and George Beskales and Mohamed A. Soliman},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1391729.1391730},
	issn = {03600300},
	issue = {4},
	journal = {ACM Computing Surveys},
	keywords = {database},
	pages = {1-58},
	title = {A survey of top- <i>k</i> query processing techniques in relational database systems},
	url = {http://portal.acm.org/citation.cfm?doid=1391729.1391730},
	volume = {40},
	year = {2008},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1391729.1391730},
	bdsk-url-2 = {https://doi.org/10.1145/1391729.1391730}}

@article{Agrawal:2009:CRD:1516046.1516062,
	author = {Rakesh Agrawal and Anastasia Ailamaki and Philip A Bernstein and Eric A Brewer and Michael J Carey and Surajit Chaudhuri and Anhai Doan and Daniela Florescu and Michael J Franklin and Hector Garcia-Molina and Johannes Gehrke and Le Gruenwald and Laura M Haas and Alon Y Halevy and Joseph M Hellerstein and Yannis E Ioannidis and Hank F Korth and Donald Kossmann and Samuel Madden and Roger Magoulas and Beng Chin Ooi and Tim O'Reilly and Raghu Ramakrishnan and Sunita Sarawagi and Michael Stonebraker and Alexander S Szalay and Gerhard Weikum},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1516046.1516062},
	issn = {0001-0782},
	issue = {6},
	journal = {Commun. ACM},
	keywords = {database},
	month = {6},
	pages = {56-65},
	publisher = {ACM},
	title = {The Claremont Report on Database Research},
	url = {http://doi.acm.org/10.1145/1516046.1516062},
	volume = {52},
	year = {2009},
	bdsk-url-1 = {http://doi.acm.org/10.1145/1516046.1516062},
	bdsk-url-2 = {https://doi.org/10.1145/1516046.1516062}}

@article{He2009,
	abstract = {Graphics processors (GPUs) have recently emerged as powerful coprocessors for general purpose computation. Compared with commodity CPUs, GPUs have an order of magnitude higher computation power as well as memory bandwidth. Moreover, new-generation GPUs allow writes to random memory locations, provide efficient interprocessor communication through on-chip local memory, and support a general purpose parallel programming model. Nevertheless, many of the GPU features are specialized for graphics processing, including the massively multithreaded architecture, the Single-Instruction-Multiple-Data processing style, and the execution model of a single application at a time. Additionally, GPUs rely on a bus of limited bandwidth to transfer data to and from the CPU, do not allow dynamic memory allocation from GPU kernels, and have little hardware support for write conflicts. Therefore, a careful design and implementation is required to utilize the GPU for coprocessing database queries. In this article, we present our design, implementation, and evaluation of an in-memory relational query coprocessing system, GDB, on the GPU. Taking advantage of the GPU hardware features, we design a set of highly optimized data-parallel primitives such as split and sort, and use these primitives to implement common relational query processing algorithms. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. Furthermore, we propose coprocessing techniques that take into account both the computation resources and the GPU-CPU data transfer cost so that each operator in a query can utilize suitable processorsthe CPU, the GPU, or bothfor an optimized overall performance. We have evaluated our GDB system on a machine with an Intel quad-core CPU and an NVIDIA GeForce 8800 GTX GPU. Our workloads include microbenchmark queries on memory-resident data as well as TPC-H queries that involve complex data types and multiple query operators on data sets larger than the GPU memory. Our results show that our GPU-based algorithms are 2-27x faster than their optimized CPU-based counterparts on in-memory data. Moreover, the performance of our coprocessing scheme is similar to, or better than, both the GPU-only and the CPU-only schemes.},
	author = {Bingsheng He and Mian Lu and Ke Yang and Rui Fang and Naga K. Govindaraju and Qiong Luo and Pedro V. Sander},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1620585.1620588},
	isbn = {0362-5915},
	issn = {03625915},
	issue = {4},
	journal = {ACM Transactions on Database Systems},
	keywords = {gpu-j; database},
	pages = {1-39},
	title = {Relational query coprocessing on graphics processors},
	url = {http://portal.acm.org/citation.cfm?doid=1620585.1620588},
	volume = {34},
	year = {2009},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1620585.1620588},
	bdsk-url-2 = {https://doi.org/10.1145/1620585.1620588}}

@article{kim2009,
	abstract = {Join is an important database operation. As computer architectures evolve, the best join algorithm may change hand. This paper re-examines two popular join algorithms - hash join and sort-merge join - to determine if the latest computer architecture trends shift the tide that has favored hash join for many years. For a fair comparison, we implemented the most optimized parallel version of both algorithms on the latest Intel Core i7 platform. Both implementations scale well with the number of cores in the system and take advantages of latest processor features for performance. Our hash-based implementation achieves more than 100M tuples per second which is 17X faster than the best reported performance on CPUs and 8X faster than that reported for GPUs. Moreover, the performance of our hash join implementation is consistent over a wide range of input data sizes from 64K to 128M tuples and is not affected by data skew. We compare this implementation to our highly optimized sort-based implementation that achieves 47M to 80M tuples per second. We developed analytical models to study how both algorithms would scale with upcoming processor architecture trends. Our analysis projects that current architectural trends of wider SIMD, more cores, and smaller memory bandwidth per core imply better scalability potential for sort-merge join. Consequently, sort-merge join is likely to outperform hash join on upcoming chip multiprocessors. In summary, we offer multicore implementations of hash join and sort-merge join which consistently outperform all previously reported results. We further conclude that the tide that favors the hash join algorithm has not changed yet, but the change is just around the corner.},
	author = {Changkyu Kim and Tim Kaldewey and Victor W Lee and Eric Sedlar and Anthony D Nguyen and Nadathur Satish and Jatin Chhugani and Andrea Di Blas and Pradeep Dubey and Andrea Di Blas},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/69.334883},
	isbn = {0000000000000},
	issn = {21508097},
	issue = {2},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {gpu-j; database},
	pages = {1378-1389},
	publisher = {VLDB Endowment},
	title = {Sort vs. Hash revisited: fast join implementation on modern multi-core CPUs},
	url = {http://portal.acm.org/citation.cfm?id=1687564},
	volume = {2},
	year = {2009},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?id=1687564},
	bdsk-url-2 = {https://doi.org/10.1109/69.334883}}

@article{MendezMediavilla2010,
	abstract = {This study presents statistical techniques to obtain local approximate query answers for aggregate multivariate materialized views thus eliminating the need for repetitive scanning of the source data. In widely distributed management information systems, detailed data do not necessarily reside in the same physical location as the decision-maker; thus, requiring scanning of the source data as needed by the query demand. Decision-making, business intelligence and data analysis could involve multiple data sources, data diversity, aggregates and large amounts of data. Management often confronts delays in information acquisition from remote sites. Management decisions usually involve analyses that require the most precise summary data available. These summaries are readily available from data warehouses and can be used to estimate or approximate data in exchange for a quicker response. An approach to supporting aggregate materialized view management is proposed that reconstructs data sets locally using posterior parameter estimates based on sufficient statistics in a log-linear model with a multinomial likelihood.},
	author = {Francis a. M{\'e}ndez Mediavilla and Douglas H Jones},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1080/02664760903148791},
	isbn = {0266476090314},
	issn = {02664763},
	issue = {10},
	journal = {Journal of Applied Statistics},
	keywords = {bipf,data reduction,materialized view management,query approximation; aqp; database; sampling},
	pages = {1703-1715},
	title = {A Bayesian method for query approximation},
	volume = {37},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1080/02664760903148791}}

@article{Taylor2010,
	author = {Ronald C Taylor},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1186/1471-2105-11-S12-S1},
	issn = {1471-2105},
	issue = {Suppl 12},
	journal = {BMC Bioinformatics},
	keywords = {database},
	pages = {S1},
	title = {An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics},
	url = {http://www.biomedcentral.com/1471-2105/11/S12/S1},
	volume = {11},
	year = {2010},
	bdsk-url-1 = {http://www.biomedcentral.com/1471-2105/11/S12/S1},
	bdsk-url-2 = {https://doi.org/10.1186/1471-2105-11-S12-S1}}

@inproceedings{beav10,
	author = {Doug Beaver and Sanjeev Kumar and Harry C Li and Jason Sobel and Peter Vajgel},
	city = {Berkeley, CA, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation},
	keywords = {database},
	pages = {1-8},
	publisher = {USENIX Association},
	title = {Finding a Needle in Haystack: Facebook's Photo Storage},
	url = {http://dl.acm.org/citation.cfm?id=1924943.1924947},
	year = {2010},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1924943.1924947}}

@article{Sun2010,
	abstract = {The growing size of Resource Description Framework (RDF) dataset requires RDF repository to be excellent scalable and highly efficient. Distributed and parallel processing model meets the urgent needs naturally. In this paper, we propose a scalable RDF store based on HBase, which is a distributed, column-oriented database modeled after Google's Bigtable. Our approach adopts the idea of Hexastore and considers both RDF data model and HBase capability. We store RDF triples into six HBase tables (S\{_\}PO, P\{_\}SO, O\{_\}SP, PS\{_\}O, SO\{_\}P and PO\{_\}S) which covers all combinations of RDF triple patterns. And we index them with HBase provided index structure on row key. Besides presenting the storage schema, we also propose a MapReduce strategy for SPARQL Basic Graph Pattern (BGP) processing, which is suitable for our storage schema. It uses multiple MapReduce jobs to process a typical BGP. In each job, it uses a greedy method to select join key and eliminates multiple triple patterns. The evaluation result indicates that our approach works well against large RDF dataset.},
	author = {Jianling Sun and Qiang Jin},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ICACTE.2010.5578937},
	isbn = {9781424465408},
	issn = {2154-7491},
	journal = {Advanced Computer Theory and Engineering (ICACTE), 2010 3rd International Conference on},
	keywords = {Analytical models,HBase,Hexastore,Indexes,Irrigation,Lead,MapReduce,MapReduce strategy,Parallel Processing,RDF,RDF triple patterns,SPARQL,SPARQL basic graph pattern processing,Semantics,Web pages,column-oriented database,distributed database,distributed databases,distributed processing model,greedy method,parallel processing,parallel processing model,resource description framework,scalable RDF store; database},
	pages = {V1--633--V1--636},
	title = {Scalable RDF store based on HBase and MapReduce},
	volume = {1},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/ICACTE.2010.5578937}}

@article{Bakkum2010a,
	abstract = {Prior work has shown dramatic acceleration for various data- base operations on GPUs, but only using primitives that are not part of conventional database languages such as SQL. This paper implements a subset of the SQLite command processor directly on the GPU. This dramatically reduces the effort required to achieve GPU acceleration by avoiding the need for database programmers to use new programming languages such as CUDA or modify their programs to use non-SQL libraries. This paper focuses on accelerating SELECT queries and describes the considerations in an efficient GPU implemen- tation of the SQLite command processor. Results on an NVIDIA Tesla C1060 achieve speedups of 20-70X depend- ing on the size of the result set.},
	author = {Peter Bakkum and Kevin Skadron},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:35:27 -0500},
	doi = {10.1145/1735688.1735706},
	isbn = {9781605589350},
	journal = {Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units - GPGPU '10},
	keywords = {cuda,database},
	pages = {94},
	title = {Accelerating SQL database operations on a GPU with CUDA},
	url = {http://portal.acm.org/citation.cfm?doid=1735688.1735706 http://portal.acm.org/citation.cfm?id=1735688.1735706%5Cnhttp://dl.acm.org/citation.cfm?id=1735706 http://portal.acm.org/citation.cfm?id=1735688.1735706$%5C$nhttp://dl.acm.org/citation.cfm?id=1735706},
	year = {2010},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1735688.1735706%20http://portal.acm.org/citation.cfm?id=1735688.1735706%5Cnhttp://dl.acm.org/citation.cfm?id=1735706%20http://portal.acm.org/citation.cfm?id=1735688.1735706$%5C$nhttp://dl.acm.org/citation.cfm?id=1735706},
	bdsk-url-2 = {https://doi.org/10.1145/1735688.1735706}}

@article{Sewall2011,
	abstract = {Concurrency control onB+ trees is primarily achieved with latches, but serialization and contention can hinder scalability. As core counts on current processors increase, it is imperative to develop scalable latch-free techniques for concurrency control. We present PALM, a novel technique for performing multiple concurrent queries on in-memory B+ trees. PALM is based on the Bulk Synchronous Parallel model, which guarantees freedom from deadlocks and race conditions. Input queries are grouped and processed in atomic batches, and work proceeds in stages that preclude contention. Transitions between stages are accomplished with scalable point-to-point communication. PALM exploits data- and thread-level parallelism on modern many-core architectures, and performs 40M1 updates/second on trees with 128M keys, and 128M updates/second on trees with 512K keys on the latest CPU architectures. Our throughput is 2.3X--19X that of state-of-the- art concurrent update algorithms on in-memory B+ trees. PALM obtains close to peak throughput at very low response times of less than 350µs, even for large trees. We also evaluate PALM on the Intel R demonstrate a speedup of 1.5--2.1X for out-of-cache tree sizes on an Intel R  Many Integrated Core (Intel R  Knights Ferry over a pair of Intel R  MIC) architecture, and  Xeon R DP X5680 (Westmere-EP) in a dual-socket configuration.},
	author = {Jason Sewall and Jatin Chhugani and Changkyu Kim and Nadathur Satish and Pradeep Dubey},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issn = {21508097},
	issue = {11},
	journal = {Vldb},
	keywords = {database},
	pages = {795-806},
	title = {PALM: Parallel Architecture-Friendly Latch-Free Modifications to B+ Trees on Many-Core Processors},
	volume = {4},
	year = {2011}}

@article{Tudorica2011,
	abstract = {This paper is trying to comment on the various NoSQL (Not only Structured Query Language) systems and to make a comparison (using multiple criteria) between them. The NoSQL databases were created as a mean to offer high performance (both in terms of speed and size) and high availability at the price of loosing the ACID (Atomic, Consistent, Isolated, Durable) trait of the traditional databases in exchange with keeping a weaker BASE (Basic Availability, Soft state, Eventual consistency) feature. Remains to be seen which of the multiple solutions created since the official appearance of the NoSQL concept (which was defined in 1998 and reintroduced in 2009, around which moment several NoSQL solutions emerged; at the present moment there are known over 120 such solutions) are really delivering on these promises of higher performance (although several of them are already used with very good results).},
	author = {Bogdan George Tudorica and Cristian Bucur},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 07:14:26 -0500},
	doi = {10.1109/RoEduNet.2011.5993686},
	isbn = {978-1-4577-1233-3},
	issn = {2068-1038},
	journal = {2011 RoEduNet International Conference 10th Edition: Networking in Education and Research},
	keywords = {ACID,Availability,BASE,Benchmark testing,Distributed databases,Generators,Google,NoSQL,NoSQL databases,SQL,Taxonomy,atomic consistent isolated durable database trait,basic availability soft state eventual consistency,comparison,database,not only structured query language systems,performance},
	pages = {1-5},
	title = {A comparison between several NoSQL databases with comments and notes},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5993686},
	year = {2011},
	bdsk-url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5993686},
	bdsk-url-2 = {https://doi.org/10.1109/RoEduNet.2011.5993686}}

@article{Pokorny2011,
	abstract = {The paper is focused on so called NoSQL databases. In context of cloud computing, architectures and basic features of these databases are studied, particularly their horizontal scalability and concurrency model, that is mostly weaker than ACID transactions in relational SQL-like database systems. Some characteristics like a data model and querying capabilities are discussed in more detail. The paper also contains an overview of some representatives of NoSQL databases.},
	author = {Jaroslav Pokorny},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2095536.2095583},
	isbn = {9781450307840},
	issn = {1744-0084},
	journal = {Proceedings of the 13th International Conference on Information Integration and Web-based Applications and Services - iiWAS '11},
	keywords = {cap theorem,cloud computing,consistency,horizontal data,horizontal scaling,nosql database,vertical scaling,weak; database},
	pages = {278},
	title = {NoSQL databases: a step to database scalability in web environment},
	url = {http://dl.acm.org/citation.cfm?id=2095536.2095583},
	year = {2011},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=2095536.2095583},
	bdsk-url-2 = {https://doi.org/10.1145/2095536.2095583}}

@article{Bakkum2011,
	author = {Peter Bakkum and Srimat Chakradhar},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:35:36 -0500},
	journal = {Pbbakkum.Com},
	keywords = {cuda, database},
	title = {Efficient Data Management for GPU Databases},
	url = {http://pbbakkum.com/virginian/paper.pdf$%5C$nhttp://wiki.postgresql.org/wiki/PGStrom$%5C$nhttps://github.com/bakks/virginian},
	year = {2011},
	bdsk-url-1 = {http://pbbakkum.com/virginian/paper.pdf$%5C$nhttp://wiki.postgresql.org/wiki/PGStrom$%5C$nhttps://github.com/bakks/virginian}}

@article{Blanas2011,
	abstract = {1 A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A. Wood. DBMSs on a modern processor: Where does time go? In VLDB, pages 266--277, 1999. 2 P. A. Boncz, S. Manegold, and M. L. Kersten. Database architecture optimized for the new bottleneck: Memory access. In VLDB, pages 54--65, 1999.},
	author = {Spyros Blanas and Yinan Li and Jignesh M Patel},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1989323.1989328},
	isbn = {9781450306614},
	issn = {07308078},
	journal = {Proc. ACM SIGMOD International Conference on Management of Data},
	keywords = {DB New HW,gpu-j; database},
	pages = {37-48},
	title = {Design and evaluation of main memory hash join algorithms for multi-core CPUs},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1145/1989323.1989328}}

@article{Stonebraker2011,
	abstract = {Michael Stonebraker discusses several reasons why NoSQL has not caught on with enterprise users.},
	author = {Michael Stonebraker},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1978542.1978546},
	issn = {00010782},
	issue = {8},
	journal = {Communications of the ACM},
	keywords = {database},
	pages = {10},
	title = {Stonebraker on NoSQL and enterprises},
	volume = {54},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1145/1978542.1978546}}

@article{Catania2011,
	author = {Barbara Catania and Giovanna Guerrini},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	pages = {171-212},
	title = {Towards Adaptively Approximated Search in Distributed Architectures},
	year = {2011}}

@book{Nevarez2011,
	abstract = {The SQL Server Query Optimizer is a cost-based optimizer. It analyzes a number of candidate execution plans for a given query, estimates the cost of each of these plans, and selects the plan with the lowest cost of the choices considered. Indeed, given that the Query Optimizer cannot consider every possible plan for every query, it actually has to find a balance between the optimization time and the quality of the selected plan. Therefore, it is the SQL Server component that has the biggest impact on the perform- ance of your databases. After all, selecting the right (or wrong) execution plan could mean the difference between a query execution time of milliseconds, and one of minutes, or even hours. Naturally, a better understanding of how the Query Optimizer works can help both database administrators and developers to write better queries and to provide the Query Optimizer with the information it needs to produce efficient execution plans. This book will demonstrate how you can use your newfound knowledge of the Query Optimizer's inner workings and, in addition, it will give you the knowledge and tools to troubleshoot the cases when the Query Optimizer is not giving you a good plan.},
	author = {Benjamin Nevarez},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9781906434571},
	keywords = {database},
	pages = {265},
	title = {Inside the SQL Server Query Optimizer},
	year = {2011}}

@article{Han2011,
	abstract = {With the development of the Internet and cloud computing, there need databases to be able to store and process big data effectively, demand for high-performance when reading and writing, so the traditional relational database is facing many new challenges. Especially in large scale and high-concurrency applications, such as search engines and SNS, using the relational database to store and query dynamic user data has appeared to be inadequate. In this case, NoSQL database created. This paper describes the background, basic characteristics, data model of NoSQL. In addition, this paper classifies NoSQL databases according to the CAP theorem. Finally, the mainstream NoSQL databases are separately described in detail, and extract some properties to help enterprises to choose NoSQL.},
	author = {Super Instruments Corporation and Jing Han and E. Haihong and Guan Le and Jian Du and Super Instruments Corporation},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 07:14:26 -0500},
	doi = {10.1109/ICPCA.2011.6106531},
	isbn = {9781457702082},
	issn = {978-1-4577-0207-5},
	journal = {Proceedings - 2011 6th International Conference on Pervasive Computing and Applications, ICPCA 2011},
	keywords = {Big Data,NoSQL,column-oriented,database,document,key-value,nosql},
	pages = {363-366},
	title = {Survey on NoSQL database},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/ICPCA.2011.6106531}}

@article{Wu2011b,
	abstract = {MapReduce has been widely recognized as an efficient tool for large-scale data analysis. It achieves high performance by exploit- ing parallelism among processing nodes while providing a sim- ple interface for upper-layer applications. Some vendors have en- hanced their data warehouse systems by integrating MapReduce into the systems. However, existing MapReduce-based query pro- cessing systems, such as Hive, fall short of the query optimization and competency of conventional database systems. Given an SQL query, Hive translates the query into a set of MapReduce jobs sen- tence by sentence. This design assumes that the user can optimize his query before submitting it to the system. Unfortunately,manual query optimization is time consuming and difficult, even to anex- perienced database user or administrator. In this paper, we propose aquery optimization scheme forMapReduce-based processingsys- tems. Specifically, we embed into Hive a query optimizer whichis designed to generate an efficient query plan based on our proposed cost model. Experiments carried out on our in-house cluster con- firm the effectiveness of our query optimizer.},
	author = {Sai Wu and Feng Li and Sharad Mehrotra and Beng Chin Ooi and Chin Ooi and Beng Chin Ooi},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2038916.2038928},
	isbn = {9781450309769},
	issn = {1450309763},
	journal = {Proceedings of the 2nd ACM Symposium on Cloud Computing - SOCC '11},
	keywords = {hive,mapreduce,multi-way join,query optimization; database},
	pages = {1-13},
	title = {Query optimization for massively parallel data processing},
	url = {http://dl.acm.org/citation.cfm?doid=2038916.2038928},
	year = {2011},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2038916.2038928},
	bdsk-url-2 = {https://doi.org/10.1145/2038916.2038928}}

@article{Cattell2011a,
	abstract = {In this paper, we examine a number of SQL and so called ``NoSQL'' data stores designed to scale simple OLTP-style application loads over many servers. Originally motivated by Web 2.0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses. We contrast the new systems on their data model, consistency mechanisms, storage mechanisms, durability guarantees, availability, query support, and other dimensions. These systems typically sacrifice some of these dimensions, e.g. database-wide transaction consistency, in order to achieve others, e.g. higher availability and scalability.},
	author = {Rick Cattell},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1978915.1978919},
	isbn = {9781450307178},
	issn = {01635808},
	issue = {4},
	journal = {ACM SIGMOD Record},
	keywords = {database},
	month = {5},
	pages = {12},
	publisher = {ACM},
	title = {Scalable SQL and NoSQL data stores},
	url = {http://portal.acm.org/citation.cfm?doid=1978915.1978919},
	volume = {39},
	year = {2011},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1978915.1978919},
	bdsk-url-2 = {https://doi.org/10.1145/1978915.1978919}}

@article{Unknown2011,
	author = {Unknown},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	title = {Sql 语言解析器的实现},
	year = {2011}}

@article{Jin2011,
	author = {Ruoming Jin and Ning Ruan and Yang Xiang and Haixun Wang},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1929934.1929941},
	issn = {03625915},
	issue = {1},
	journal = {ACM Transactions on Database Systems},
	keywords = {database},
	pages = {1-44},
	title = {Path-tree},
	url = {http://portal.acm.org/citation.cfm?doid=1929934.1929941},
	volume = {36},
	year = {2011},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1929934.1929941},
	bdsk-url-2 = {https://doi.org/10.1145/1929934.1929941}}

@article{Braginsky2012,
	author = {Anastasia Braginsky},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9781450312134},
	keywords = {b-tree, concurrency; database},
	title = {A Lock-Free B tree},
	year = {2012}}

@article{Liu2012,
	abstract = {With the rapid development of the Internet Web 2.0 technology, the demands of large-scale distributed service and storage in cloud computing have brought great challenges to traditional relational database. NoSQL database which breaks the shackles of RDBMS is becoming the focus of attention. In this paper, the principles and implementation mechanisms of Auto-Sharding in MongoDB database are firstly presented, then an improved algorithm based on the frequency of data operation is proposed in order to solve the problem of uneven distribution of data in auto-sharding. The improved balancing strategy can effectively balance the data among shards, and improve the cluster's concurrent reading and writing performance.},
	author = {Yimeng Liu and Yizhi Wang and Yi Jin},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ICCSE.2012.6295203},
	isbn = {9781467302425},
	issue = {Iccse},
	journal = {ICCSE 2012 - Proceedings of 2012 7th International Conference on Computer Science and Education},
	keywords = {Auto-Sharding,MongoDB,NoSQL,balance strategy; database},
	pages = {851-854},
	title = {Research on the improvement of MongoDB Auto-Sharding in cloud environment},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/ICCSE.2012.6295203}}

@article{Kaldewey2012,
	author = {Tim Kaldewey and Guy M Lohman and Ren{\'e} M{\"u}ller and Peter Benjamin Volk},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2236584.2236592},
	isbn = {9781450314459},
	journal = {DaMoN},
	keywords = {gpu-j; database},
	pages = {55-62},
	title = {GPU Join Processing Revisited},
	url = {http://dl.acm.org/citation.cfm?doid=2236584.2236592},
	year = {2012},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2236584.2236592},
	bdsk-url-2 = {https://doi.org/10.1145/2236584.2236592}}

@article{Jin2012,
	author = {Ruoming Jin and Ning Ruan and Yang Xiang and Victor Lee},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2213836.2213887},
	isbn = {9781450312479},
	issn = {07308078},
	journal = {Proceedings of the 2012 international conference on Management of Data - SIGMOD '12},
	keywords = {bipartite set cover,distance query,highway-centric labeling,jin,ntwork graph; database},
	pages = {445},
	title = {A highway-centric labeling approach for answering distance queries on large sparse graphs},
	url = {http://dl.acm.org/citation.cfm?doid=2213836.2213887},
	year = {2012},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2213836.2213887},
	bdsk-url-2 = {https://doi.org/10.1145/2213836.2213887}}

@book{Han2012,
	author = {Xixian Han and Jianzhong Li and Donghua Yang},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s10115-011-0429-x},
	isbn = {1011501104},
	issn = {02191377},
	issue = {3},
	journal = {Knowledge and Information Systems},
	keywords = {JPIPT construction stage,Massive data,PI-join,Result output stage; database},
	pages = {527-557},
	title = {PI-Join: Efficiently processing join queries on massive data},
	volume = {32},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/s10115-011-0429-x}}

@article{Aiyer2012,
	abstract = {Facebook Messages, which combines messages, chat and email into a real-time conversation, is the first application in Facebook to use HBase in production. In this article, we will discuss why we chose HBase for this use case, the early improvements we did to make HBase production ready, engineering and operational challenges encountered along the way, and the continued work we have had to do once in production, to improve HBase's efficiency and reliability. We will also describe some of the other use cases of HBase at Facebook. 1},
	author = {Amitanand S Aiyer and Mikhail Bautin and Guoqiang Jerry Chen and Pritam Damania and Prakash Khemani and Kannan Muthukkaruppan and Karthik Ranganathan and Nicolas Spiegelberg and Liyin Tang and Madhuwanti Vaidya},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {2},
	journal = {IEEE Data Eng. Bull.},
	keywords = {database},
	pages = {4-13},
	title = {Storage Infrastructure Behind Facebook Messages: Using HBase at Scale.},
	volume = {35},
	year = {2012}}

@article{Jin2012a,
	abstract = {Most of the existing reachability indices perform well on small- to medium- size graphs, but reach a scalability bottleneck around one million vertices/edges. As graphs become increasingly large, scalability is quickly becoming the major research challenge for the reachability computation today. Can we construct indices which scale to graphs with tens of millions of vertices and edges? Can the existing reachability indices which perform well on moderate-size graphs be scaled to very large graphs? In this paper, we propose SCARAB (standing for SCAlable ReachABility), a unified reachability computation framework: it not only can scale the existing state-of-the-art reachability indices, which otherwise could only be constructed and work on moderate size graphs, but also can help speed up the online query answering approaches. Our experimental results demonstrate that SCARAB can perform on graphs with millions of vertices/edges and is also much faster then GRAIL, the state-of-the-art scalability index approach.},
	author = {Ruoming Jin and Ning Ruan and Saikat Dey and Jeffrey Xu Yu},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2213836.2213856},
	isbn = {9781450312479},
	issn = {07308078},
	journal = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
	keywords = {jin,ntwork graph,reachability backbone,reachability join test,scalable reachability; database},
	pages = {169-180},
	title = {SCARAB: Scaling Reachability Computation on Large Graphs},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1145/2213836.2213856}}

@article{Babu2012b,
	abstract = {Timely and cost-effective analytics over "big data" has emerged as a key ingredient for success in many businesses, scientific and engineering disciplines, and government endeavors. Web clicks, social media, scientific experiments, and datacenter monitoring are among data sources that generate vast amounts of raw data every day. The need to convert this raw data into useful information has spawned considerable innovation in systems for large-scale data analytics, especially over the last decade. This monograph covers the design principles and core features of systems for analyzing very large datasets using massively-parallel computation and storage techniques on large clusters of nodes. We first discuss how the requirements of data analytics have evolved since the early work on parallel database systems. We then describe some of the major technological innovations that have each spawned a distinct category of systems for data analytics. Each unique system category is described along a number of dimensions including data model and query interface, storage layer, execution engine, query optimization, scheduling, resource management, and fault tolerance. We conclude with a summary of present trends in large-scale data analytics. {\copyright} 2013 S. Babu and H. Herodotou.},
	author = {Shivnath Babu},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1561/1900000036},
	issn = {1931-7883},
	issue = {1},
	journal = {Foundations and Trends{\textregistered} in Databases},
	keywords = {database},
	note = {<b>From Duplicate 1 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/></b><br/><b>From Duplicate 1 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/>And Duplicate 4 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/></b><br/>% Tyler 6/18/2016<br/><br/>published: 2013<br/>cited by: 20<br/><br/><b>Main Idea: &quot;</b>The need to convert this raw data into useful information has spawned considerable inno- vation in systems for large-scale data analytics, especially over the last decade. This monograph covers the design principles and core features of systems for analyzing very large datasets using massively-parallel computation and storage techniques on large clusters of nodes&quot;<br/><br/>Columnar Database Systems pg 36<br/><br/>Section 4.2 (pg52) Data layouts speaks on implementation of columnar data layouts in HDFS.<br/><br/><b>From Duplicate 2 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/>And Duplicate 4 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/>And Duplicate 5 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/>And Duplicate 6 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/></b><br/>% Tyler 6/18/2016<br/><br/>published: 2013<br/>cited by: 20<br/><br/><b>Main Idea: &quot;</b>The need to convert this raw data into useful information has spawned considerable inno- vation in systems for large-scale data analytics, especially over the last decade. This monograph covers the design principles and core features of systems for analyzing very large datasets using massively-parallel computation and storage techniques on large clusters of nodes&quot;<br/><br/>Columnar Database Systems pg 36<br/><br/>Section 4.2 (pg52) Data layouts speaks on implementation of columnar data layouts in HDFS.<br/><br/><b>From Duplicate 3 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/></b><br/><b>From Duplicate 2 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/></b><br/><b>From Duplicate 1 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/>And Duplicate 4 (<i>Massively Parallel Databases and MapReduce Systems</i> - Babu, Shivnath)<br/></b><br/>% Tyler 6/18/2016<br/><br/>published: 2013<br/>cited by: 20<br/><br/><b>Main Idea: &quot;</b>The need to convert this raw data into useful information has spawned considerable inno- vation in systems for large-scale data analytics, especially over the last decade. This monograph covers the design principles and core features of systems for analyzing very large datasets using massively-parallel computation and storage techniques on large clusters of nodes&quot;<br/><br/>Columnar Database Systems pg 36<br/><br/>Section 4.2 (pg52) Data layouts speaks on implementation of columnar data layouts in HDFS.},
	pages = {1-104},
	title = {Massively Parallel Databases and MapReduce Systems},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-databases/DBS-036},
	volume = {5},
	year = {2012},
	bdsk-url-1 = {http://www.nowpublishers.com/articles/foundations-and-trends-in-databases/DBS-036},
	bdsk-url-2 = {https://doi.org/10.1561/1900000036}}

@article{Ngo2012,
	abstract = {Efficient join processing is one of the most fundamental and well-studied tasks in database research. In this work, we examine algorithms for natural join queries over many relations and describe a novel algorithm to process these queries optimally in terms of worst-case data complexity. Our result builds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size of a full conjunctive query in terms of the sizes of the individual relations in the body of the query. These bounds, however, are not constructive: they rely on Shearer's entropy inequality which is information-theoretic. Thus, the previous results leave open the question of whether there exist algorithms whose running time achieve these optimal bounds. An answer to this question may be interesting to database practice, as we show in this paper that any project-join plan is polynomially slower than the optimal bound for some queries. We construct an algorithm whose running time is worst-case optimal for all natural join queries. Our result may be of independent interest, as our algorithm also yields a constructive proof of the general fractional cover bound by Atserias, Grohe, and Marx without using Shearer's inequality. In addition, we show that this bound is equivalent to a geometric inequality by Bollob\{{\'a}\}s and Thomason, one of whose special cases is the famous Loomis-Whitney inequality. Hence, our results algorithmically prove these inequalities as well. Finally, we discuss how our algorithm can be used to compute a relaxed notion of joins.},
	author = {Hung Q. Ngo and Ely Porat and Christopher R{\'e} and D B Mar and R Christopher and Christopher R{\'e} and Atri Rudra and D B Mar and R Christopher and Christopher R{\'e} and Atri Rudra},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 07:14:26 -0500},
	doi = {10.1145/2213556.2213565},
	isbn = {9781450312486},
	journal = {Proceedings of the 31st Symposium on Principles of Database Systems},
	keywords = {all or part of,bollob\{{\'a}\}s-thomason inequality,bollob{\'a}s-thomason inequality,compressed,database,ecc,fractional cover bound,ity,join,join algorithms,loomis-whitney inequal-,or hard copies of,permission to make digital,sensing,this work for},
	pages = {37-48},
	title = {Worst-case Optimal Join Algorithms},
	url = {http://dl.acm.org/citation.cfm?doid=2213556.2213565$%5C$nhttp://dx.doi.org/10.1145/2213556.2213565 http://arxiv.org/abs/1203.1952},
	year = {2012},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2213556.2213565$%5C$nhttp://dx.doi.org/10.1145/2213556.2213565%20http://arxiv.org/abs/1203.1952},
	bdsk-url-2 = {https://doi.org/10.1145/2213556.2213565}}

@article{Augustyn2012,
	author = {Dariusz Rafal Augustyn and Sebastian Zederowski},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:35:45 -0500},
	journal = {Gid},
	keywords = {cuda;database},
	pages = {3-12},
	title = {Applying CUDA Technology in DCT-Based Method of Query Selectivity Estimation},
	year = {2012}}

@article{Heimel2012,
	abstract = {Modern graphics cards bundle high-bandwidthmemory with a massively parallel processor, making them an interest- ing platform for running data-intensive operations. Conse- quently, several authors have discussed accelerating database operators using graphics cards, often demonstrating promis- ing speed-ups. However, due to limitations stemming from limited device memory and expensive data transfer, GPU- accelerated databases remain a niche technology. We suggest a novel approach: Using the graphics card as a co-processor during query optimization. Query optimiza- tion is a compute-heavy operation that requires only min- imal data transfer, making it a well-suited target for GPU offloading. Since existing optimizers are typically very effi- cient, we do not suggest to simply accelerate them. Instead, we propose to use the additional resources to leverage more computationally involved optimization methods. This ap- proach indirectly accelerates a database by generating bet- ter plan quality. As a first step towards GPU-assisted query optimization, we present a proof-of-concept that uses the graphics card as a statistical co-processor during selectivity estimation. We integrated this GPU-accelerated estimator into the op- timizer of PostgreSQL. Based on this proof-of-concept, we demonstrate that a GPU can be efficiently used to improve the quality of selectivity estimates in a relational database system.},
	author = {Technische Universit{\"a}t Berlin and Max Heimel and Volker Markl and Technische Universit{\"a}t Berlin},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {The Third International Workshop on Accelerating Data Management Sys- tems using Modern Processor and Storage Architectures (ADMS'12)},
	keywords = {database},
	pages = {1-12},
	title = {A First Step Towards GPU-assisted Query Optimization},
	url = {http://www.adms-conf.org/heimel%7B_%7Dadms12.pdf http://www.adms-conf.org/heimel_adms12.pdf},
	year = {2012},
	bdsk-url-1 = {http://www.adms-conf.org/heimel%7B_%7Dadms12.pdf%20http://www.adms-conf.org/heimel_adms12.pdf}}

@article{Cheng2013,
	author = {James Cheng and Zechao Shang and Hong Cheng and Haixun Wang and Jeffrey Xu Yu},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-013-0346-6},
	issn = {1066-8888},
	issue = {2},
	journal = {The VLDB Journal},
	keywords = {bility index,distance queries,graph indexing,k-hop reachability,reacha-,shortest path index; database},
	pages = {227-252},
	title = {Efficient processing of \{$\}\{$\}k\{$\}\{$\} k -hop reachability queries},
	url = {http://link.springer.com/10.1007/s00778-013-0346-6},
	volume = {23},
	year = {2013},
	bdsk-url-1 = {http://link.springer.com/10.1007/s00778-013-0346-6},
	bdsk-url-2 = {https://doi.org/10.1007/s00778-013-0346-6}}

@book{RomanD.;Villaverde2014,
	abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s13398-014-0173-7.2},
	editor = {Barbara Catania and Lakhmi C Jain},
	isbn = {9780874216561},
	keywords = {high resolution images,research,risks management,sustainable reconstruction; database},
	pages = {350},
	publisher = {Springer-Verlag Berlin Heidelberg},
	title = {Advanced Query Processing},
	volume = {1: Issues},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s13398-014-0173-7.2}}

@inproceedings{Barahmand,
	author = {Sumita Barahmand and Shahram Ghandeharizadeh},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9781450321518},
	journal = {DBTest'13},
	keywords = {benchmarking,dis-,social networks,tributed architectures,zipfian distribution; database},
	title = {D-Zipfian : A Decentralized Implementation of Zipfian},
	year = {2013}}

@article{Moniruzzaman2013,
	abstract = {Digital world is growing very fast and become more complex in the volume (terabyte to petabyte), variety (structured and un-structured and hybrid), velocity (high speed in growth) in nature. This refers to as `Big Data' that is a global phenomenon. This is typically considered to be a data collection that has grown so large it can't be effectively managed or exploited using conventional data management tools: e.g., classic relational database management systems (RDBMS) or conventional search engines. To handle this problem, traditional RDBMS are complemented by specifically designed a rich set of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This paper motivation is to provide - classification, characteristics and evaluation of NoSQL databases in Big Data Analytics. This report is intended to help users, especially to the organizations to obtain an independent understanding of the strengths and weaknesses of various NoSQL database approaches to supporting applications that process huge volumes of data.},
	author = {a B M Moniruzzaman and Syed Akhter Hossain},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {2005-4270},
	issn = {2005-4270},
	issue = {4},
	journal = {International Journal of Database Theory and Application},
	keywords = {big data,big data analytics,newsql database,nosql database; database},
	pages = {1-13},
	title = {NoSQL Database : New Era of Databases for Big data Analytics- Classification , Characteristics and Comparison},
	volume = {6},
	year = {2013}}

@article{Sellam2013,
	author = {Thibault Sellam and Martin Kersten},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Cidrdb.Org},
	keywords = {database},
	title = {Meet Charles, big data query advisor},
	url = {http://www.cidrdb.org/cidr2013/Papers/CIDR13%7B_%7DPaper94.pdf},
	year = {2013},
	bdsk-url-1 = {http://www.cidrdb.org/cidr2013/Papers/CIDR13%7B_%7DPaper94.pdf}}

@article{Abramova2013,
	abstract = {In the past, relational databases were used in a large scope of applications due to their rich set of features, query capabilities and transaction management. However, they are not able to store and process big data effectively and are not very efficient to make transactions and join operations. Recently, emerge a new paradigm, NoSQL databases, to overcome some of these problems, which are more suitable for the usage in web environments. In this paper, we describe NoSQL databases, their characteristics and operational principles. The main focus of this paper is to compare and evaluate two of the most popular NoSQL databases: MongoDB and Cassandra.},
	author = {Veronika Abramova and Jorge Bernardino},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2494444.2494447},
	isbn = {9781450319768},
	issn = {1744-0084},
	journal = {Proceedings of the International C* Conference on Computer Science and Software Engineering - C3S2E '13},
	keywords = {NoSQL databases,database management systems (DBMS); database},
	pages = {14-22},
	title = {NoSQL databases},
	url = {http://dl.acm.org/citation.cfm?id=2494444.2494447},
	year = {2013},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=2494444.2494447},
	bdsk-url-2 = {https://doi.org/10.1145/2494444.2494447}}

@article{Grolinger2013,
	abstract = {Advances in Web technology and the proliferation of mobile devices and sensors connected to the Internet have resulted in immense processing and storage requirements. Cloud computing has emerged as a paradigm that promises to meet these requirements. This work focuses on the storage aspect of cloud computing, specifically on data management in cloud environments. Traditional relational databases were designed in a different hardware and software era and are facing challenges in meeting the performance and scale requirements of Big Data. NoSQL and NewSQL data stores present themselves as alternatives that can handle huge volume of data. Because of the large number and diversity of existing NoSQL and NewSQL solutions, it is difficult to comprehend the domain and even more challenging to choose an appropriate solution for a specific task. Therefore, this paper reviews NoSQL and NewSQL solutions with the objective of: (1) providing a perspective in the field, (2) providing guidance to practitioners and researchers to choose the appropriate data store, and (3) identifying challenges and opportunities in the field. Specifically, the most prominent solutions are compared focusing on data models, querying, scaling, and security related capabilities. Features driving the ability to scale read requests and write requests, or scaling data storage are investigated, in particular partitioning, replication, consistency, and concurrency control. Furthermore, use cases and scenarios in which NoSQL and NewSQL data stores have been used are discussed and the suitability of various solutions for different sets of applications is examined. Consequently, this study has identified challenges in the field, including the immense diversity and inconsistency of terminologies, limited documentation, sparse comparison and benchmarking criteria, and nonexistence of standardized query languages.},
	author = {Katarina Grolinger and Wilson a Higashino and Abhinav Tiwari and Miriam Am Capretz},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1186/2192-113X-2-22},
	isbn = {2192-113X},
	issn = {2192-113X},
	journal = {Journal of Cloud Computing: Advances, Systems and Applications},
	keywords = {big data,cloud computing,data management,distributed storage,newsql,nosql; database},
	pages = {22},
	title = {Data management in cloud environments: NoSQL and NewSQL data stores},
	url = {http://www.journalofcloudcomputing.com/content/2/1/22},
	volume = {2},
	year = {2013},
	bdsk-url-1 = {http://www.journalofcloudcomputing.com/content/2/1/22},
	bdsk-url-2 = {https://doi.org/10.1186/2192-113X-2-22}}

@article{Sitaridi2013,
	author = {Evangelia a. Sitaridi and Kenneth a. Ross},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2485278.2485282},
	isbn = {9781450321969},
	journal = {Proceedings of the Ninth International Workshop on Data Management on New Hardware - DaMoN '13},
	keywords = {database},
	pages = {1},
	title = {Optimizing select conditions on GPUs},
	url = {http://dl.acm.org/citation.cfm?doid=2485278.2485282},
	year = {2013},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2485278.2485282},
	bdsk-url-2 = {https://doi.org/10.1145/2485278.2485282}}

@article{Balkesen2013,
	abstract = {The architectural changes introduced with multi-core CPUs have triggered a redesign of main-memory join algorithms. In the last few years, two diverging views have appeared. One approach advocates careful tailoring of the algorithm to the architectural parameters (cache sizes, TLB, and memory bandwidth). The other approach argues that modern hardware is good enough at hiding cache and TLB miss latencies and, consequently, the careful tailoring can be omitted without sacrificing performance. In this paper we demonstrate through experimental analysis of different algorithms and architectures that hardware still matters. Join algorithms that are hardware conscious perform better than hardware-oblivious approaches. The analysis and comparisons in the paper show that many of the claims regarding the behavior of join algorithms that have appeared in literature are due to selection effects (relative table sizes, tuple sizes, the underlying architecture, using sorted data, etc.) and are not supported by experiments run under different parameters settings. Through the analysis, we shed light on how modern hardware affects the implementation of data operators and provide the fastest implementation of radix join to date, reaching close to 200 million tuples per second.},
	author = {Cagri Balkesen and Jens Teubner and Gustavo Alonso and M Tamer {\"O}zsu},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ICDE.2013.6544839},
	isbn = {9781467349086},
	issn = {10844627},
	journal = {Proceedings - International Conference on Data Engineering},
	keywords = {gpu-j; database},
	pages = {362-373},
	title = {Main-memory hash joins on multi-core CPUs: Tuning to the underlying hardware},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2013.6544839}}

@inproceedings{rauh13,
	author = {Hannes Rauhe and Jonathan Dees and Kai-Uwe Sattler and Franz Faerber},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	editor = {Barbara Catania and Giovanna Guerrini and Jaroslav Pokorn{\'y}},
	isbn = {978-3-642-40682-9},
	journal = {ADBIS},
	keywords = {dblp; database},
	pages = {330-343},
	publisher = {Springer},
	title = {Multi-level Parallel Query Execution Framework for CPU and GPU.},
	url = {http://dblp.uni-trier.de/db/conf/adbis/adbis2013.html#RauheDSF13},
	volume = {8133},
	year = {2013},
	bdsk-url-1 = {http://dblp.uni-trier.de/db/conf/adbis/adbis2013.html#RauheDSF13}}

@article{Bednar2013a,
	abstract = {Onedimensional or multidimensional range query is one of the most important query of physical implementation of DBMS. The number of compared items (of a data structure) can be enormous especially for lower selectivity of the range query. The number of compare operations increases for more complex items (or tuples) with the longer length, e.g. words stored in a B-tree. Due to the possibly high number of compare operations executed during the range query processing, we can take into account hardware devices providing a parallel task computation like CPU's SIMD or GPU. In this paper, we show the performance and scalability of sequential, index, CPU's SIMD, and GPU variants of the range query algorithm. These results make possible a future integration of these computation devices into a DBMS kernel.},
	author = {Pavel Bedn{\'a}{\v r} and Petr Gajdo{\v s} and Michal Kr{\'a}tk{\'y} and Peter Chovanec},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/978-3-642-32518-2},
	isbn = {978-3-642-32517-5},
	journal = {Advances in Intelligent Systems and Computing},
	keywords = {database},
	note = {<b>From Duplicate 1 (<i>Processing of Range Query Using SIMD and GPU</i> - Bedn{\'a}{\v r}, Pavel; Gajdo{\v s}, Petr; Kr{\'a}tk{\'y}, Michal; Chovanec, Peter)<br/></b><br/><b>From Duplicate 1 (<i>Processing of Range Query Using SIMD and GPU</i> - Bedn{\'a}{\v r}, Pavel; Gajdo{\v s}, Petr; Kr{\'a}tk{\'y}, Michal; Chovanec, Peter)<br/></b><br/>NULL<br/><br/><b>From Duplicate 2 (<i>Processing of Range Query Using SIMD and GPU</i> - Bedn{\'a}{\v r}, Pavel; Gajdo{\v s}, Petr; Kr{\'a}tk{\'y}, Michal; Chovanec, Peter)<br/></b><br/>NULL},
	pages = {13-25},
	title = {Processing of Range Query Using SIMD and GPU},
	url = {http://www.springerlink.com/index/10.1007/978-3-642-32518-2},
	volume = {185},
	year = {2013},
	bdsk-url-1 = {http://www.springerlink.com/index/10.1007/978-3-642-32518-2},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-642-32518-2}}

@article{Ngo2013a,
	abstract = {Evaluating the relational join is one of the central algorithmic and most well-studied problems in database systems. A staggering number of variants have been considered including Block-Nested loop join, Hash-Join, Grace, Sort-merge for discussions of more modern issues). Commercial database engines use finely tuned join heuristics that take into account a wide variety of factors including the selectivity of various predicates, memory, IO, etc. In spite of this study of join queries, the textbook description of join processing is suboptimal. This survey describes recent results on join algorithms that have provable worst-case optimality runtime guarantees. We survey recent work and provide a simpler and unified description of these algorithms that we hope is useful for theory-minded readers, algorithm designers, and systems implementors.},
	author = {R Christopher and Hq Ngo and Christopher Re and Atri Rudra},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2590989.2590991},
	issn = {01635808},
	issue = {4},
	journal = {arXiv preprint arXiv:1310.3314},
	keywords = {database},
	title = {Skew Strikes Back: New Developments in the Theory of Join Algorithms},
	url = {http://arxiv.org/abs/1310.3314},
	volume = {42},
	year = {2013},
	bdsk-url-1 = {http://arxiv.org/abs/1310.3314},
	bdsk-url-2 = {https://doi.org/10.1145/2590989.2590991}}

@article{Guo2013b,
	abstract = {Privacy of data owners and query users is vital in modern clouding data management. Many researches have been done on cloud security, but most of them are focused on the privacy of data owners or of query users separately. How to protect the privacy of the data owners and users simultaneously is a great challenge. In this paper, a solution of data storage and query protocol based on classical homomorphic encryption scheme is given to preserve privacy of both data owners and query users. Our main efforts are put on NoSQL database which is less structural than relational database. Storage and indexing structure on NoSQL database, query protocol are proposed, and algorithms for updating and querying are also given. To implement our solution, Berkley DB, an excellent storage solution for NoSQL database is chosen and data are encrypted/decrypted using Elgamal and Paillier encryption system, using basic Java package. Experiments are done under different parameters in order to achieve better efficiency.},
	author = {Yubin Guo and Liankuan Zhang and Fengren Lin and Ximing Li and Guo Yubin and Zhang Liankuan and Lin Fengren and Li Ximing and Yubin Guo and Liankuan Zhang and Fengren Lin and Ximing Li},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.4304/jcp.8.6.1427-1432},
	isbn = {1015106420},
	issn = {1796203X},
	issue = {6},
	journal = {Journal of Computers},
	keywords = {Cloud data management,NoSQL,Privacy preserving,cloud data management,oSQL,preserving,privacy; database},
	pages = {1427-1432},
	title = {A solution for privacy-preserving data manipulation and query on NoSQL database},
	url = {http://ojs.academypublisher.com/index.php/jcp/article/view/9265},
	volume = {8},
	year = {2013},
	bdsk-url-1 = {http://ojs.academypublisher.com/index.php/jcp/article/view/9265},
	bdsk-url-2 = {https://doi.org/10.4304/jcp.8.6.1427-1432}}

@article{Hsu2013,
	abstract = {Today's CPUs are capable of supporting realtime audio for many popular applications, but some compute-intensive audio applications require hardware acceleration. This article looks at some realtime sound-synthesis applications and shares the authors' experiences implementing them on GPUs (graphics processing units).},
	author = {Bill Hsu and Marc Sosnick-P{\'e}rez and Approaches To and Software Synthesis and Bill Hsu and Marc Sosnick-P{\'e}rez},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2461256.2461272},
	issn = {00010782},
	journal = {ACM queue},
	keywords = {database},
	pages = {1-16},
	title = {Realtime GPU Audio},
	volume = {April 1},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1145/2461256.2461272}}

@article{Teodoro2014,
	abstract = {Similarity search in high-dimensional spaces is a pivotal operation for several database applications, including online content-based multimedia services. With the increasing popularity of multimedia applications, these services are facing new challenges regarding (1) the very large and growing volumes of data to be indexed/searched and (2) the necessity of reducing the response times as observed by end-users. In addition, the nature of the interactions between users and online services creates fluctuating query request rates throughout execution, which requires a similarity search engine to adapt to better use the computation platform and minimize response times. In this work, we address these challenges with Hypercurves, a flexible framework for answering approximate k-nearest neighbor (kNN) queries for very large multimedia databases. Hypercurves executes in hybrid CPU-GPU environments and is able to attain massive query-processing rates through the cooperative use of these devices. Hypercurves also changes its CPU-GPU task partitioning dynamically according to the observed load, aiming for optimal response times. In our empirical evaluation, dynamic task partitioning reduced query response times by approximately 50 \{%\} compared to the best static task partition. Due to a probabilistic proof of equivalence to the sequential kNN algorithm, the CPU-GPU execution of Hypercurves in distributed (multi-node) environments can be aggressively optimized, attaining superlinear scalability while still guaranteeing, with high probability, results at least as good as those from the sequential algorithm. \{{\copyright}\} 2013 Springer-Verlag Berlin Heidelberg.},
	author = {G.a George G.a George Teodoro and E.b Eduardo E.b Valle and Nathan N.c Mariano and R.d Ricardo R.d Ricardo Torres and W.c Meira Jr. and J.H.a Joel H. J.H.a Joel H. Saltz and Wagner Meira and J.H.a Joel H. J.H.a Joel H. Saltz and W.c Meira Jr. and J.H.a Joel H. J.H.a Joel H. Saltz and Wagner Meira and J.H.a Joel H. J.H.a Joel H. Saltz},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-013-0329-7},
	issn = {10668888},
	issue = {3},
	journal = {VLDB Journal},
	keywords = {Descriptor indexing,Filter-stream,GPGPU,Hypercurves,Information retrieval,Multimedia databases,descriptor indexing,filter-stream,gpgpu,hypercurves,information retrieval,multimedia databases; database},
	pages = {427-448},
	title = {Approximate similarity search for online multimedia services on distributed CPU-GPU platforms},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84880429477%7B&%7DpartnerID=40%7B&%7Dmd5=7813d26e011bb6c43c366154e830d68c},
	volume = {23},
	year = {2013},
	bdsk-url-1 = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84880429477%7B&%7DpartnerID=40%7B&%7Dmd5=7813d26e011bb6c43c366154e830d68c},
	bdsk-url-2 = {https://doi.org/10.1007/s00778-013-0329-7}}

@article{Faleiro2014,
	abstract = {ABSTRACT Existing database systems employ an eager transaction processing scheme--- that is, upon receiving a transaction request, the system executes all the operations entailed in running the transaction (which typically includes reading database records, executing ...},
	author = {Jose M Faleiro and Alexander Thomson and Daniel J Abadi},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2588555.2610529},
	isbn = {9781450323765},
	issn = {07308078},
	journal = {Proceedings of the 2014 ACM SIGMOD international conference on Management of data - SIGMOD '14},
	keywords = {acid transactions,deterministic database systems,load balancing; database},
	pages = {15-26},
	title = {Lazy Evaluation of Transactions in Database Systems},
	url = {http://dl.acm.org/citation.cfm?doid=2588555.2610529},
	year = {2014},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2588555.2610529},
	bdsk-url-2 = {https://doi.org/10.1145/2588555.2610529}}

@article{Model2014,
	author = {Mongodb Data Model},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {database},
	pages = {1-4},
	title = {Introduction to MongoDB},
	year = {2014}}

@article{Zhang2014,
	author = {Yu Zhang and Yansong Zhang and Mingchuan Su and Fangzhou Wang and Hong Chen},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9783642543692},
	issn = {16113349},
	journal = {WISE 2013 International Workshops BigWebData, MBC, PCS, STeH, QUAT, SCEH, and STSC 2013, Nanjing, China, October 13-15, 2013, Revised Selected Papers},
	keywords = {bitmap join index,cpu platform,hybrid gpu,olap,star join; database},
	pages = {23-36},
	title = {HG-Bitmap Join Index : A Hybrid GPU / CPU Bitmap Join Index Mechanism for OLAP},
	year = {2014}}

@article{Ntarmos2014,
	author = {Nikos Ntarmos and I Patlakas and Peter Triantafillou},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issn = {21508097},
	journal = {Proceedings of the VLDB \{{\ldots}\}},
	keywords = {database},
	pages = {493-504},
	title = {Rank join queries in NoSQL databases},
	url = {http://eprints.gla.ac.uk/89359},
	year = {2014},
	bdsk-url-1 = {http://eprints.gla.ac.uk/89359}}

@inproceedings{Kllapi2014,
	author = {Herald Kllapi and Boulos Harb and Cong Yu},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ICDE.2014.6816728},
	isbn = {978-1-4799-2555-1},
	issn = {10844627},
	journal = {Proceedings - International Conference on Data Engineering (ICDE)},
	keywords = {database},
	pages = {1120-1131},
	title = {Near neighbor join},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6816728},
	year = {2014},
	bdsk-url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6816728},
	bdsk-url-2 = {https://doi.org/10.1109/ICDE.2014.6816728}}

@article{Hu2014a,
	author = {Weiwei Hu and Guoliang Li and Jiacai Ni and Dalie Sun and Kian-lee Tan},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {10},
	journal = {Knowledge and Data Engineering, IEEE Transactions},
	keywords = {database},
	pages = {2368-2381},
	title = {B p -Tree : A Predictive B + -Tree for Reducing Writes on Phase Change Memory},
	volume = {26},
	year = {2014}}

@article{Ma2014,
	author = {Shuai Ma and Jia Li and Chunming Hu and Xuelian Lin and Jinpeng Huai},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {X},
	keywords = {big data,data techniques,distributed computing,graph search,niques,query tech-; database},
	pages = {1-12},
	title = {Big Graph Search : Challenges and T echniques},
	year = {2014}}

@article{Peng2014,
	abstract = {Although SPARQL has been the predominant query language over RDF graphs, some query intentions cannot be well captured by only using SPARQL syntax. On the other hand, the keyword search enjoys widespread usage because of its intuitive way of specifying information needs but suffers from the problem of low precision. To maximize the advantages of both SPARQL and keyword search, we introduce a novel paradigm that combines both of them and propose a hybrid query (called an SK query) that integrates SPARQL and keyword search. In order to answer SK queries efficiently, a structural index is devised, based on a novel integrated query algorithm is proposed. We evaluate our method in large real RDF graphs and experiments demonstrate both effectiveness and efficiency of our method.},
	author = {Peng Peng and Lei Zou and Dongyan Zhao},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {a struc-,and efficiency of our,based on a novel,experiments demonstrate both effectiveness,graphs and,in large real rdf,in order to answer,integrated query algorithm,is proposed,search,sk queries efficiently,tural index is devised,we evaluate our method; database},
	pages = {14},
	title = {On The Marriage of SPARQL and Keywords},
	url = {http://arxiv.org/abs/1411.6335},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1411.6335}}

@article{Wu2014a,
	abstract = {Given a large graph and a query node, finding its k-nearest-neighbor (kNN) is a fundamental problem. Various random walk based measures have been developed to measure the proximity (similarity) between nodes. Existing algorithms for the random walk based},
	author = {Yubao Wu and Ruoming Jin and Xiang Zhang},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2588555.2610500},
	isbn = {9781450323765},
	issn = {07308078},
	journal = {Sigmod},
	keywords = {local search,nearest neighbors,random walk,top- k search; database},
	pages = {1139-1150},
	title = {Fast and unified local search for random walk based k-nearest-neighbor query in large graphs},
	url = {http://dl.acm.org/citation.cfm?doid=2588555.2610500},
	year = {2014},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2588555.2610500},
	bdsk-url-2 = {https://doi.org/10.1145/2588555.2610500}}

@article{Polychroniou2014,
	abstract = {Network communication is the slowest component of many operators in distributed parallel databases deployed for large-scale analytics. Whereas considerable work has focused on speeding up databases on modern hardware, communica-tion reduction has received less attention. Existing parallel DBMSs rely on algorithms designed for disks with minor modifications for networks. A more complicated algorithm may burden the CPUs, but could avoid redundant trans-fers of tuples across the network. We introduce track join, a novel distributed join algorithm that minimizes network traffic by generating an optimal transfer schedule for each distinct join key. Track join extends the trade-off options be-tween CPU and network. Our evaluation based on real and synthetic data shows that track join adapts to diverse cases and degrees of locality. Considering both network traffic and execution time, even with no locality, track join outperforms hash join on the most expensive queries of real workloads.},
	author = {Orestis Polychroniou and Rajkumar Sen and Kenneth a. Ross and Rajkumar Sen and Kenneth a. Ross},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 15:12:33 -0500},
	doi = {10.1145/2588555.2610521},
	isbn = {9781450323765},
	issn = {07308078},
	journal = {ACM SIGMOD International Conference on Management of Data},
	keywords = {ck join,distributed joins with minimal,network traffic,orestis polychroniou; aqp; database; sampling},
	pages = {1483-1494},
	title = {Track join: distributed joins with minimal network traffic},
	url = {http://dl.acm.org/citation.cfm?doid=2588555.2610521},
	year = {2014},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2588555.2610521},
	bdsk-url-2 = {https://doi.org/10.1145/2588555.2610521}}

@article{Walters2014a,
	abstract = {As more scientific workloads are moved into the cloud, the need for high performance accelerators increases. Accelerators such as GPUs offer improvements in both performance and power efficiency over traditional multi-core processors, however, their use in the cloud has been limited. Today, several common hypervisors support GPU passthrough, but their performance has not been systematically characterized. In this paper we show that low overhead GPU passthrough is achievable across 4 major hypervisors and two processor microarchitectures. We compare the performance of two generations of NVIDIA GPUs within the Xen, VMWare ESXi, and KVM hypervisors, and we also compare the performance to that of Linux Containers (LXC). We show that GPU passthrough to KVM achieves 98 -- 100% of the base system's performance across two architectures, while Xen and VMWare achieve 96 -- 99% of the base systems performance, respectively. In addition, we describe several valuable lessons learned through our analysis and share the advantages and disadvantages of each hypervisor/GPU passthrough solution.},
	author = {John Paul Walters and Andrew J. Younge and Dong-in In Kang and Ke-thia Thia Yao and Mikyung Kang and Stephen P. Crago and Geoffrey C. Fox and Kang Dong In and Yao Ke Thia and Kang Mikyung and Stephen P. Crago and Geoffrey C. Fox and Dong-in In Kang and Ke-thia Thia Yao and Mikyung Kang and Stephen P. Crago and Geoffrey C. Fox and Kang Dong In and Yao Ke Thia and Kang Mikyung and Stephen P. Crago and Geoffrey C. Fox and Dong-in In Kang and Ke-thia Thia Yao and Mikyung Kang and Stephen P. Crago and Geoffrey C. Fox},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:35:52 -0500},
	doi = {10.1109/CLOUD.2014.90},
	isbn = {9781479950638},
	issn = {21596190},
	journal = {IEEE International Conference on Cloud Computing, CLOUD},
	keywords = {gpu,cuda;database},
	pages = {636-643},
	title = {GPU passthrough performance: A comparison of KVM, Xen, VMWare ESXi, and LXC for CUDA and openCL applications},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/CLOUD.2014.90}}

@article{Mirzoev2014a,
	author = {T Mirzoev and C Brockman},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1107/S0021889808001684},
	issue = {1},
	journal = {arXiv preprint arXiv:1404.2160},
	keywords = {database management systems,dbms,hana,language,rdbms,relational database management system,saphigh performance analytics appliance,sql,structured query; database},
	pages = {13-21},
	title = {SAP HANA and its performance benefits},
	url = {http://arxiv.org/abs/1404.2160},
	volume = {2},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1404.2160},
	bdsk-url-2 = {https://doi.org/10.1107/S0021889808001684}}

@inproceedings{Meister2015,
	author = {Andreas Meister},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {VLDB PhD Workshop},
	keywords = {database},
	title = {GPU-accelerated join-order optimization},
	year = {2015}}

@article{faleiro2015fit,
	author = {Jose M Faleiro and Daniel J Abadi},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1},
	journal = {Data Engineering},
	keywords = {database},
	pages = {10-17},
	title = {FIT: A Distributed Database Performance Tradeoff},
	volume = {38},
	year = {2015}}

@article{Trummer2015,
	abstract = {Evaluating query predicates on data samples is the only way to estimate their selectivity in certain scenarios. Finding a guaranteed optimal query plan is not a reasonable optimization goal in those cases as it might require an infinite number of samples. We therefore introduce probably approximately optimal query optimization (PAO) where the goal is to find a query plan whose cost is near-optimal with a certain probability. We will justify why PAO is a suitable formalism to model scenarios in which predicate sampling and optimization need to be interleaved. We present the first algorithm for PAO. Our algorithm is non-intrusive and uses standard query optimizers and sampling components as sub-functions. It is generic and can be applied to a wide range of scenarios. Our algorithm is iterative and calculates in each iteration a query plan together with a region in the selectivity space where the plan has near-optimal cost. It determines the confidence that the true selectivity values fall within the aforementioned region and chooses the next samples to take based on the current state if the confidence does not reach the threshold specified as problem input. We devise different algorithm variants and analyze their complexity. We experimentally compare them in terms of the number of optimizer invocations, samples, and iterations over many different query classes.},
	author = {Immanuel Trummer and Christoph Koch},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {CoRR},
	keywords = {database},
	title = {Probably Approximately Optimal Query Optimization},
	url = {http://arxiv.org/abs/1511.01782},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1511.01782}}

@article{bernstein2015scaling,
	author = {Philip A Bernstein and Sudipto Das},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1},
	journal = {IEEE Data Eng. Bull},
	keywords = {database},
	title = {Scaling optimistic concurrency control by approximately partitioning the certifier and log},
	volume = {38},
	year = {2015}}

@article{Fegaras2015,
	abstract = {This paper addresses online processing for large-scale, incremental computations on a distributed stream processing engine (DSPE). Our goal is to convert any distributed batch query to an incremental DSPE program automatically. In contrast to other approaches, we derive incremental programs that return accurate results, not approximate answers, by retaining a minimal state during the query evaluation lifetime and by using incremental evaluation techniques to return an accurate snapshot answer at each time interval that depends on the current state and the latest batches of data. Our methods can handle many forms of queries, including iterative and nested queries, group-by with aggregation, and joins on one-to-many relationships. Finally, we report on a prototype implementation of our framework using MRQL running on top of Spark and we experimentally validate the effectiveness of our methods.},
	author = {Leonidas Fegaras},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {11},
	journal = {CoRR},
	keywords = {database},
	pages = {2998-3012},
	title = {Incremental Query Processing on Big Data Streams},
	url = {http://arxiv.org/abs/1511.07846},
	volume = {28},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1511.07846}}

@article{Faleiro2015,
	abstract = {Multi-versioned database systems have the potential to significantly increase the amount of concurrency in transaction processing because they can avoid read-write conflicts. Unfortunately, the increase in concurrency usually comes at the cost of transaction serializability. If a database user requests full serializability, modern multi-versioned systems significantly constrain read-write concurrency among conflicting transactions and employ expensive synchronization patterns in their design. In main-memory multi-core settings, these additional constraints are so burdensome that multi-versioned systems are often significantly outperformed by single-version systems. We propose Bohm, a new concurrency control protocol for main-memory multi-versioned database systems. Bohm guarantees serializable execution while ensuring that reads never block writes. In addition, Bohm does not require reads to perform any book-keeping whatsoever, thereby avoiding the overhead of tracking reads via contended writes to shared memory. This leads to excellent scalability and performance in multi-core settings. Bohm has all the above characteristics without performing validation based concurrency control. Instead, it is pessimistic, and is therefore not prone to excessive aborts in the presence of contention. An experimental evaluation shows that Bohm performs well in both high contention and low contention settings, and is able to dramatically outperform state-of-the-art multi-versioned systems despite maintaining the full set of serializability guarantees.},
	author = {Jose M. Faleiro and Daniel J. Abadi},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/2809974.2809981},
	issn = {21508097},
	issue = {11},
	journal = {Proceedings of the 41st International Conference on Very Large Data Bases},
	keywords = {database},
	pages = {1190-1201},
	title = {Rethinking serializable multiversion concurrency control},
	url = {http://arxiv.org/abs/1412.2324 http://dl.acm.org/citation.cfm?doid=2809974.2809981},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1412.2324%20http://dl.acm.org/citation.cfm?doid=2809974.2809981},
	bdsk-url-2 = {https://doi.org/10.14778/2809974.2809981}}

@article{Commons2015a,
	author = {Creative Commons and Attribution License},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {FOSS4G North America 2015},
	keywords = {database},
	pages = {1-56},
	title = {Explaining the Postgres Query Optimizer Postgres Query Execution},
	url = {https://2015.foss4g-na.org/conference/session-slides},
	year = {2015},
	bdsk-url-1 = {https://2015.foss4g-na.org/conference/session-slides}}

@article{Melorose2015e,
	author = {J. Melorose and R. Perroy and S. Careas},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1017/CBO9781107415324.004},
	isbn = {9788578110796},
	issn = {1098-6596},
	journal = {Statewide Agricultural Land Use Baseline 2015},
	keywords = {distributed B-tree,icle; database},
	pages = {598-609},
	pmid = {25246403},
	title = {A Practical Scalable Distributed B-Tree},
	volume = {1},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1017/CBO9781107415324.004}}

@article{Chestnut2016,
	author = {Stephen R Chestnut and Nikita Ivkin and Jelani Nelson and Zhengyu Wang and David P Woodruff and Vladimir Braverman},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0001415123},
	keywords = {database},
	pages = {1-15},
	title = {BPTree : an heavy hitters algorithm using constant memory},
	year = {2016}}

@article{Abiteboul2017,
	author = {Serge Abiteboul and Leonid Libkin and Wim Martens and Tova Milo and Filip Murlak and Frank Neven and Magdalena Ortiz and Thomas Schwentick and Julia Stoyanovich and Jianwen Su and Dan Suciu and Marcelo Arenas and Victor Vianu and Ke Yi and Pablo Barcel{\'o} and Meghyn Bienvenu and Diego Calvanese and Claire David and Richard Hull and Eyke H{\"u}llermeier and Benny Kimelfeld},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/3092931.3092933},
	issn = {01635808},
	issue = {4},
	journal = {ACM SIGMOD Record},
	keywords = {database},
	pages = {5-17},
	title = {Research Directions for Principles of Data Management (Abridged)},
	url = {http://dl.acm.org/citation.cfm?doid=3092931.3092933},
	volume = {45},
	year = {2017},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=3092931.3092933},
	bdsk-url-2 = {https://doi.org/10.1145/3092931.3092933}}

@inproceedings{YongjooParkAhmadShahabTajikMichaelCafarella2017,
	author = {Yongjoo Park and Ahmad Shahab Tajik and Michael Cafarella and Barzan Mozafari and Barzan Mozafari Yongjoo Park, Ahmad Shahab Tajik, Michael Cafarella},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.475/123},
	isbn = {9781450335423},
	issn = {16130073},
	journal = {SIGMOD},
	keywords = {approximate query processing,query approximation; database},
	title = {Database Learning: Toward a Database that Becomes Smarter Every Time},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.475/123}}

@inproceedings{trummer2017solving,
	author = {Immanuel Trummer and Christoph Koch},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	institution = {ACM},
	journal = {Proceedings of the 2017 ACM International Conference on Management of Data},
	keywords = {database},
	pages = {1025-1040},
	title = {Solving the join ordering problem via mixed integer linear programming},
	year = {2017}}

@article{baca2017xml,
	author = {Radim Ba{\v c}a and Michal Kr{\'a}tk{\'y} and Irena Holubov{\'a} and Martin Ne{\v c}ask{\'y} and Tom{\'a}{\v s} Skopal and Martin Svoboda and Sherif Sakr},
	city = {New York, NY, USA},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/3095798},
	issn = {0360-0300},
	issue = {5},
	journal = {ACM Comput. Surv.},
	keywords = {XML,structural XML query processing; database},
	month = {9},
	pages = {64:1--64:41},
	publisher = {ACM},
	title = {Structural XML Query Processing},
	volume = {50},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3095798}}

@article{Faerber2017,
	abstract = {This article provides an overview of recent developments in main-memory database systems. With growing memory sizes and memory prices dropping by a factor of 10 every 5 years, data having a ``primary home'' in memory is now a reality. Main-memory databases eschew many of the traditional architectural pillars of relational database systems that optimized for disk-resident data. The result of these memory-optimized designs are systems that feature several innovative approaches to fundamental issues (e.g., concurrency control, query processing) that achieve orders of magnitude performance improvements over traditional designs. Our survey covers five main issues and architectural choices that need to be made when building a high performance main-memory optimized database: data organization and storage, indexing, concurrency control, durability and recovery techniques, and query processing and compilation. We focus our survey on four commercial and research systems: H-Store/VoltDB, Hekaton, HyPer, and SAP HANA. These systems are diverse in their design choices and form a representative sample of the state of the art in main-memory database systems. We also cover other commercial and academic systems, along with current and future research trends.},
	author = {Franz Faerber and Alfons Kemper and Per {\AA}ke Larson and Justin Levandoski and Thomas Neumann and Andrew Pavlo},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1561/1900000058},
	issn = {19317891},
	issue = {1-2},
	journal = {Foundations and Trends in Databases},
	keywords = {database},
	pages = {1-130},
	title = {Main memory database systems},
	volume = {8},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1561/1900000058}}

@article{Leis2018,
	author = {Viktor Leis and Bernhard Radke and Andrey Gubichev and Atanas Mirchev and Peter Boncz and Alfons Kemper and Thomas Neumann},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-017-0480-7},
	issn = {0949-877X},
	issue = {5},
	journal = {The VLDB Journal},
	keywords = {database},
	month = {10},
	pages = {643-668},
	title = {Query optimization through the looking glass, and what we found running the Join Order Benchmark},
	url = {https://www.evernote.com/shard/s13/nl/1480559/90385de1-dba1-43ac-aa2f-07fddbf3617b/},
	volume = {27},
	year = {2018},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/90385de1-dba1-43ac-aa2f-07fddbf3617b/},
	bdsk-url-2 = {https://doi.org/10.1007/s00778-017-0480-7}}

@article{Makreshanski2018,
	abstract = {Database architectures typically process queries one at a time, executing concurrent queries in independent execution contexts. Often, such a design leads to unpredictable performance and poor scalability. One approach to circumvent the problem is to take advantage of sharing opportunities across concurrently running queries. In this paper, we propose many-query join (MQJoin), a novel method for sharing the execution of a join that can efficiently deal with hundreds of concurrent queries. This is achieved by minimizing redundant work and making efficient use of main-memory bandwidth and multi-core architectures. Compared to existing proposals, MQJoin is able to efficiently handle larger workloads regardless of the schema by exploiting more sharing opportunities. We also compared MQJoin to two commercial main-memory column-store databases. For a TPC-H-based workload, we show that MQJoin provides 2--5$$\times $$\{\texttimes\}higher throughput with significantly more stable response times.},
	author = {Darko Makreshanski and Georgios Giannikis and Gustavo Alonso and Donald Kossmann},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-017-0475-4},
	issn = {0949-877X},
	issue = {5},
	journal = {The VLDB Journal},
	keywords = {database},
	month = {10},
	pages = {669-692},
	title = {Many-query join: efficient shared execution of relational joins on modern hardware},
	url = {https://doi.org/10.1007/s00778-017-0475-4},
	volume = {27},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/s00778-017-0475-4}}

@article{Neumann2020,
	abstract = {The increases in main-memory sizes over the last decade have made pure in-memory database systems feasible, and in-memory systems offer unprecedented performance. However, DRAM is still relatively expensive, and the growth of main-memory sizes has slowed down. In contrast, the prices for SSDs have fallen substantially in the last years, and their read bandwidth has increased to gigabytes per second. This makes it attractive to combine a large in-memory buffer with fast SSDs as storage devices, combining the excellent performance for the in-memory working set with the scalability of a disk-based system. In this paper we present the Umbra system, an evolution of the pure in-memory HyPer system towards a disk-based, or rather SSD-based, system. We show that by introducing a novel low-overhead buffer manager with variable-size pages we can achieve comparable performance to an in-memory database system for the cached working set, while handling accesses to uncached data gracefully. We discuss the changes and techniques that were necessary to handle the out-of-memory case gracefully and with low overhead, offering insights into the design of a memory optimized disk-based system.},
	author = {Thomas Neumann and Michael Freitag},
	date-added = {2022-01-10 07:14:26 -0500},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Cidr},
	keywords = {database},
	title = {Umbra: A Disk-Based System with In-Memory Performance},
	url = {http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf},
	year = {2020},
	bdsk-url-1 = {http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf}}

@article{hansen1943hhest,
	author = {Morris H Hansen and William N Hurwitz},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {4},
	journal = {The Annals of Mathematical Statistics},
	keywords = {sampling; database},
	pages = {333-362},
	publisher = {JSTOR},
	title = {On the theory of sampling from finite populations},
	volume = {14},
	year = {1943}}

@article{goodman1949estimation,
	author = {Leo A Goodman and others},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {4},
	journal = {The Annals of Mathematical Statistics},
	keywords = {sampling; database},
	pages = {572-579},
	publisher = {Institute of Mathematical Statistics},
	title = {On the estimation of the number of classes in a population},
	volume = {20},
	year = {1949}}

@book{Cochran1977,
	author = {William G. Cochran},
	date-modified = {2022-01-10 13:26:52 -0500},
	edition = {3rd},
	keywords = {sampling; database},
	publisher = {New York, NY (USA) Wiley},
	title = {Sampling Techniques},
	year = {1977}}

@article{burnham1978estimation,
	author = {Kenneth P Burnham and Walter Scott Overton},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {3},
	journal = {Biometrika},
	keywords = {sampling; database},
	pages = {625-633},
	publisher = {Oxford University Press},
	title = {Estimation of the size of a closed population when capture probabilities vary among animals},
	volume = {65},
	year = {1978}}

@inproceedings{seli79,
	author = {P Griffiths Selinger and M M Astrahan and D D Chamberlin and R A Lorie and T G Price},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. SIGMOD'79},
	keywords = {sampling; database},
	pages = {23-34},
	title = {Access path selection in a relational database management system},
	year = {1979}}

@article{efron1979,
	author = {B Efron},
	date-modified = {2022-01-10 15:02:02 -0500},
	doi = {10.1214/aos/1176344552},
	issn = {0090-5364},
	issue = {1},
	journal = {The Annals of Statistics},
	keywords = {sampling; bootstrap; database},
	pages = {1-26},
	title = {Bootstrap methods: another look at the jackknife},
	volume = {7},
	year = {1979},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxDqLi4vTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvRWZyb24gLSAxOTgyIC0gYm9vdHN0cmFwIG1ldGhvZHMgYW5vdGhlciBsb29rIGF0IHRoZSBqYWNra25pZmUucGRmTxEDRAAAAAADRAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0Vmcm9uIC0gMTk4MiAtIGJvbyNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAJAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIA8y86VXNlcnM6Znl1OkxpYnJhcnk6R3JvdXAgQ29udGFpbmVyczpVQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlOk9uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXg6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHk6ZG9jOm15X2xpYnJhcnk6cGRmOkVmcm9uIC0gMTk4MiAtIGJvb3RzdHJhcCBtZXRob2RzIGFub3RoZXIgbG9vayBhdCB0aGUgamFja2tuaWZlLnBkZgAADgCGAEIARQBmAHIAbwBuACAALQAgADEAOQA4ADIAIAAtACAAYgBvAG8AdABzAHQAcgBhAHAAIABtAGUAdABoAG8AZABzACAAYQBuAG8AdABoAGUAcgAgAGwAbwBvAGsAIABhAHQAIAB0AGgAZQAgAGoAYQBjAGsAawBuAGkAZgBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgDxVXNlcnMvZnl1L0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL0Vmcm9uIC0gMTk4MiAtIGJvb3RzdHJhcCBtZXRob2RzIGFub3RoZXIgbG9vayBhdCB0aGUgamFja2tuaWZlLnBkZgAAEwABLwAAFQACAAr//wAAAAgADQAaACQBEQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARZ},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176344552}}

@article{burnham1979robust,
	author = {Kenneth P Burnham and W Scott Overton},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {5},
	journal = {Ecology},
	keywords = {sampling; database},
	pages = {927-936},
	publisher = {Wiley Online Library},
	title = {Robust estimation of population size when capture probabilities vary among animals},
	volume = {60},
	year = {1979}}

@article{heltshe1983estimating,
	author = {James F Heltshe and Nancy E Forrester},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Biometrics},
	keywords = {sampling; database},
	pages = {1-11},
	publisher = {JSTOR},
	title = {Estimating species richness using the jackknife procedure},
	year = {1983}}

@article{chao1984nonparametric,
	author = {Anne Chao},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Scandinavian Journal of statistics},
	keywords = {sampling; database},
	pages = {265-270},
	publisher = {JSTOR},
	title = {Nonparametric estimation of the number of classes in a population},
	year = {1984}}

@article{gardy1984sizes,
	author = {Daniele Gardy and Claude Puech},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {3-4},
	journal = {Information Systems},
	keywords = {sampling; database},
	pages = {231-235},
	publisher = {Elsevier},
	title = {On the sizes of projections: a generating function approach},
	volume = {9},
	year = {1984}}

@article{Olken1986,
	abstract = {Sampling is a fundamental operation for the auditing and statistical analysis of large databases. It is not well supported in existing relational database management systems. We discuss how to obtain samples from the results of relational queries without first performing the query. Specifically, we examine simple random sampling from selections, projections, joins, unions, and intersects. We discuss data structures and algorithms for sampling, and their performance. We show that samples of relational queries can often be computed for a small fraction of the effort of computing the entire relational query, i.e., in time proportional to sample size, rather than time propotional to the size of the full result of the relational query.},
	author = {Frank Olken and Doron Rotem},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Procedings of the Twelfth International Conference on Very Large Databases},
	keywords = {sampling; database},
	pages = {160-169},
	title = {Simple random from relational sampling databases},
	year = {1986}}

@inproceedings{olke86,
	abstract = {Sampling is a fundamental operation for the auditing and statistical analysis of large databases. It is not well supported in existing relational database management systems. We discuss how to obtain samples from the results of relational queries without first performing the query. Specifically, we examine simple random sampling from selections, projections, joins, unions, and intersects. We discuss data structures and algorithms for sampling, and their performance. We show that samples of relational queries can often be computed for a small fraction of the effort of computing the entire relational query, i.e., in time proportional to sample size, rather than time propotional to the size of the full result of the relational query.},
	author = {Frank Olken and Doron Rotem},
	city = {San Francisco, CA, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0-934613-18-4},
	journal = {Procedings of the Twelfth International Conference on Very Large Databases},
	keywords = {sampling; database},
	pages = {160-169},
	publisher = {Morgan Kaufmann Publishers Inc.},
	title = {Simple random sampling from relational databases},
	url = {http://dl.acm.org/citation.cfm?id=645913.671474},
	year = {1986},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=645913.671474}}

@article{astrahan1987,
	author = {Morton M Astrahan and Mario Schkolnick and Kyu-Young Whang},
	city = {Oxford, UK, UK},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1},
	journal = {Inf. Syst.},
	keywords = {sampling; database},
	month = {1},
	pages = {11-15},
	publisher = {Elsevier Science Ltd.},
	title = {Approximating the Number of Unique Values of an Attribute Without Sorting},
	volume = {12},
	year = {1987}}

@inproceedings{hou88,
	author = {Wen-Chi Hou and Gultekin Ozsoyoglu and Baldeo K Taneja},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/308386.308455},
	isbn = {0-89791-263-2},
	keywords = {sampling; database},
	pages = {276-287},
	title = {Statistical estimators for relational algebra expressions},
	year = {1988},
	bdsk-url-1 = {http://doi.acm.org/10.1145/308386.308455}}

@article{Zhang1989,
	abstract = {Ordered labeled trees are trees in which the left-to-right order among siblings is. significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically,},
	author = {Kaizhong Zhang and Dennis Shasha},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1137/0218082},
	isbn = {0097-5397},
	issn = {0097-5397},
	issue = {6},
	journal = {SIAM Journal on Computing},
	keywords = {68p05,68q20,68q25,68r10,ams,dynamic programming,editing distance,mos,parallel algorithm,pattern recognition,subject classifications,trees; sampling; database},
	pages = {1245-1262},
	pmid = {9358185},
	title = {Simple Fast Algorithms for the Editing Distance between Trees and Related Problems},
	volume = {18},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1137/0218082}}

@article{Melchers1989,
	abstract = {Importance sampling as a technique to improve the Monte Carlo method for probability integration can be shown to be extremely efficient and versatile. This paper addresses the accuracy and efficiency of the method, and its application to series and parallel systems in structures.},
	author = {R.E. E Melchers},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1016/0167-4730(89)90003-9},
	issn = {01674730},
	issue = {1},
	journal = {Structural Safety},
	keywords = {Monte Carlo,failure probability,importance sampling,integration,parallel systems,reliability,series systems,simulation techniques,systems reliability; sampling; database},
	pages = {3-10},
	title = {Importance sampling in structural systems},
	url = {http://www.sciencedirect.com/science/article/pii/0167473089900039},
	volume = {6},
	year = {1989},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/0167473089900039},
	bdsk-url-2 = {https://doi.org/10.1016/0167-4730(89)90003-9}}

@inproceedings{lipton90-query,
	author = {Richard J Lipton and Jeffrey F Naughton},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1},
	journal = {Journal of Computer and System Sciences},
	keywords = {sampling; database},
	pages = {40-46},
	publisher = {Elsevier},
	title = {Query size estimation by adaptive sampling},
	volume = {51},
	year = {1990}}

@inproceedings{lipton90-practical,
	author = {Richard J Lipton and Jeffrey F Naughton and Donovan A Schneider},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {http://doi.acm.org/10.1145/93605.93611},
	issn = {0163-5808},
	issue = {2},
	journal = {Proceedings of the 1990 ACM SIGMOD international conference on Management of data},
	keywords = {sampling; database},
	month = {5},
	pages = {1-11},
	publisher = {ACM},
	title = {Practical selectivity estimation through adaptive sampling},
	url = {http://doi.acm.org/10.1145/93605.93611},
	volume = {19},
	year = {1990},
	bdsk-url-1 = {http://doi.acm.org/10.1145/93605.93611}}

@article{KentanMu1991,
	author = {Kentan Mulmuley},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0897914260},
	keywords = {sampling; database},
	title = {Randomized Multidimensional Dynamic Search Trees : Sampling},
	year = {1991}}

@article{hou91-error,
	author = {Wen-Chi Hou and Gultekin Ozsoyoglu and Erdogan Dogdu},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/119995.115837},
	issn = {0163-5808},
	issue = {2},
	journal = {SIGMOD Rec.},
	keywords = {sampling; database},
	month = {4},
	pages = {278--287},
	publisher = {Association for Computing Machinery},
	title = {Error-Constrained COUNT Query Evaluation in Relational Databases},
	url = {https://doi.org/10.1145/119995.115837},
	volume = {20},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1145/119995.115837}}

@inproceedings{Haas1992-strat,
	author = {J; Haas and Arun N Swami},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0897915224},
	journal = {Proc. SIGMOD'92},
	keywords = {sampling; database},
	pages = {341-350},
	title = {Sequential Sampling Procedures Query Size},
	year = {1992}}

@article{chao1992estimating,
	author = {Anne Chao and Shen-Ming Lee},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {417},
	journal = {Journal of the American statistical Association},
	keywords = {sampling; database},
	pages = {210-217},
	publisher = {Taylor & Francis Group},
	title = {Estimating the number of classes via sample coverage},
	volume = {87},
	year = {1992}}

@phdthesis{Olken1993,
	author = {Frank Olken},
	date-modified = {2022-01-10 13:26:52 -0500},
	school = {University of California, Berkeley},
	type = {phdthesis},
	keywords = {sampling; aqp; database},
	title = {Random Sampling from Databases},
	year = {1993}}

@article{bunge1993estimating,
	author = {John Bunge and Michael Fitzpatrick},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {421},
	journal = {Journal of the American Statistical Association},
	keywords = {sampling; database},
	pages = {364-373},
	publisher = {Taylor & Francis},
	title = {Estimating the number of species: a review},
	volume = {88},
	year = {1993}}

@inproceedings{hellerstein1993predicate,
	author = {Joseph M Hellerstein and Michael Stonebraker},
	city = {Washington, D.C., USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. SIGMOD'9},
	keywords = {sampling; database},
	pages = {267-276},
	publisher = {ACM},
	title = {Predicate migration: Optimizing queries with expensive predicates},
	year = {1993}}

@inproceedings{chen1994adaptive,
	author = {Chungmin Melvin Chen and Nick Roussopoulos},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. SIGMOD'94},
	keywords = {parametric modeling; sampling; database},
	pages = {161-172},
	publisher = {ACM},
	title = {Adaptive Selectivity Estimation Using Query Feedback},
	year = {1994}}

@book{efron94bootstrap,
	author = {Bradley Efron and Robert J Tibshirani},
	date-modified = {2022-01-10 15:18:55 -0500},
	keywords = {sampling; bootstrap; database},
	publisher = {CRC press},
	title = {An introduction to the bootstrap},
	year = {1994}}

@inproceedings{Haas1995,
	author = {Peter J Haas and Jeffrey F Naughtont and Lynne Stokes},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. VLDB'95},
	keywords = {sampling; database},
	pages = {311-322},
	title = {Sampling-Based Estimation of the Number of Distinct Estimation of Values of an Attribute},
	year = {1995}}

@article{Mykland1995,
	author = {P Mykland and L Tierney and B Yu},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {429},
	journal = {Journal of the American Statistical Association},
	keywords = {sampling; database},
	pages = {233-241},
	title = {Regeneration in \{M\}arkov \{C\}hain \{S\}amplers},
	volume = {90},
	year = {1995}}

@inproceedings{gang96-biofocal,
	author = {Sumit Ganguly and Phillip B Gibbons and Yossi Matias and Avi Silberschatz},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/233269.233340},
	isbn = {0-89791-794-4},
	journal = {Proceedings of the 1996 ACM SIGMOD international conference on Management of data},
	keywords = {skew; sampling; database},
	pages = {271-281},
	publisher = {ACM},
	title = {Bifocal sampling for skew-resistant join size estimation},
	url = {http://doi.acm.org/10.1145/233269.233340},
	year = {1996},
	bdsk-url-1 = {http://doi.acm.org/10.1145/233269.233340},
	bdsk-url-2 = {https://doi.org/10.1145/233269.233340}}

@article{haas1996selectivity,
	author = {Peter J Haas and Jeffrey F Naughton and S Seshadri and Arun N Swami},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {3},
	journal = {Journal of Computer and System Sciences},
	keywords = {sampling; database},
	pages = {550-569},
	publisher = {Elsevier},
	title = {Selectivity and cost estimation for joins based on random sampling},
	volume = {52},
	year = {1996}}

@article{Cohen1997,
	abstract = {Computing the transitive closure in directed graphs is a fundamental graph problem. We consider the more restricted problem of computing the number of nodes reachable from every node and the size of the transitive closure. The fastest known transitive closure algorithms run inO(min\{mn,n2.38\}) time, wherenis the number of nodes andmthe number of edges in the graph. We present anO(m) time randomized (Monte Carlo) algorithm that estimates, with small relative error, the sizes of all reachability sets and the transitive closure. Another ramification of our estimation scheme is a {\~O}(m) time algorithm for estimating sizes of neighborhoods in directed graphs with nonnegative edge lengths. Our size-estimation algorithms are much faster than performing the respective explicit computations.},
	author = {Edith Cohen},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1006/jcss.1997.1534},
	issn = {00220000},
	issue = {3},
	journal = {Journal of Computer and System Sciences},
	keywords = {sampling; database},
	pages = {441-453},
	title = {Size-Estimation Framework with Applications to Transitive Closure and Reachability},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000097915348},
	volume = {55},
	year = {1997},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/S0022000097915348},
	bdsk-url-2 = {https://doi.org/10.1006/jcss.1997.1534}}

@article{Thomas1998,
	abstract = {The paper examines in detail one particular measure of variable importance for linear regression that was theoretically justified by Pratt (1987), but which has since been criticized by Bring (1996) for producing ﾓcounterintuitiveﾔ results in certain situations, and by other authors for failing to guarantee that importance be non-negative. In the article, the ﾓcounterintuitiveﾔ result is explored and shown to be a defensible characteristic of an importance measure. It is also shown that negative importance of large magnitude can only occur in the presence of multicollinearity of the explanatory variables, and methods for applying Pratt's measure in such cases are described. The objective of the article is to explain and to clarify the characteristics of Pratt's measure, and thus to assist practitioners who have to choose from among the many methods available for assessing variable importance in linear regression.},
	author = {D Roland Thomas and Edward Hughes and Bruno D Zumbo},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.2307/27522344},
	issn = {03038300},
	issue = {1},
	journal = {Social Indicators Research},
	keywords = {coefficients,discriminant ratio,least squares geometry,multicollinearity,negative importance,pratt,s importance measures,variable importance; sampling; database},
	pages = {253-275},
	title = {On Variable Importance in Linear Regression},
	url = {http://dx.doi.org/10.1023/A:1006954016433},
	volume = {45},
	year = {1998},
	bdsk-url-1 = {http://dx.doi.org/10.1023/A:1006954016433},
	bdsk-url-2 = {http://dx.doi.org/10.2307/27522344}}

@inproceedings{gibb98,
	author = {Phillip B Gibbons and Yossi Matias},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/276304.276334},
	isbn = {0-89791-995-5},
	journal = {Proceedings of the 1998 ACM SIGMOD international conference on Management of data},
	keywords = {sampling; database},
	pages = {331-342},
	publisher = {ACM},
	title = {New sampling-based summary statistics for improving approximate query answers},
	url = {http://doi.acm.org/10.1145/276304.276334},
	year = {1998},
	bdsk-url-1 = {http://doi.acm.org/10.1145/276304.276334},
	bdsk-url-2 = {https://doi.org/10.1145/276304.276334}}

@article{haas1999ripple,
	abstract = {We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (DBMS). Such queries arise naturally in interactive exploratory decision-support applications. Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of on-line aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing DBMS using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join aspect-ratio parameters. In experiments with an initial implementation of our algorithms in the POSTGRES DBMS, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers.},
	author = {Peter J. Haas and Joseph M. Hellerstein},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {2},
	journal = {ACM SIGMOD Record},
	keywords = {sampling; database},
	pages = {287-298},
	title = {Ripple joins for online aggregation},
	volume = {28},
	year = {1999}}

@inproceedings{Alon1999-pods,
	author = {Noga Alon and Phillip B Gibbons and Yossi Matias and Mario Szegedy},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/303976.303978},
	isbn = {1-58113-062-7},
	journal = {Proceedings of the Eighteenth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
	keywords = {sampling; database},
	pages = {10-20},
	publisher = {ACM},
	title = {Tracking Join and Self-join Sizes in Limited Storage},
	url = {http://doi.acm.org/10.1145/303976.303978},
	year = {1999},
	bdsk-url-1 = {http://doi.acm.org/10.1145/303976.303978},
	bdsk-url-2 = {https://doi.org/10.1145/303976.303978}}

@article{Tipping1999,
	abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss the advantages conveyed by the definition of a probability density function for PCA.},
	author = {Michael E Tipping and Christopher M Bishop},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1111/1467-9868.00196},
	isbn = {9781424444427},
	issn = {1369-7412},
	issue = {3},
	journal = {Journal of the Royal Statistical Society},
	keywords = {Mathematical and Computing Sciences not elsewhere; sampling; database},
	pages = {611-622},
	pmid = {21092268},
	title = {Probabilistic principal component analysis},
	url = {http://eprints.aston.ac.uk/7361/},
	volume = {61},
	year = {1999},
	bdsk-url-1 = {http://eprints.aston.ac.uk/7361/},
	bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00196}}

@article{lohr-2000book,
	author = {Sharon L. Lohr},
	date-modified = {2022-01-10 15:05:12 -0500},
	doi = {10.2307/1271491},
	journal = {Technometrics},
	keywords = {sampling; database},
	title = {Sampling: Design and Analysis},
	volume = {42},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.2307/1271491}}

@article{Neal2001,
	abstract = {Simulated annealing - moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions - has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.},
	author = {Radford M Neal},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1023/A:1008923215028},
	issn = {09603174},
	issue = {2},
	journal = {Statistics and Computing},
	keywords = {Estimation of normalizing constants,Free energy computation,Sequential importance sampling,Tempered transitions; sampling; database},
	pages = {125-139},
	pmid = {17705625},
	title = {Annealed importance sampling},
	url = {http://arxiv.org/pdf/physics/9803008v2.pdf},
	volume = {11},
	year = {2001},
	bdsk-url-1 = {http://arxiv.org/pdf/physics/9803008v2.pdf},
	bdsk-url-2 = {https://doi.org/10.1023/A:1008923215028}}

@article{Stillger2001,
	author = {Michael Stillger and Guy M Lohman and Volker Markl and Mokhtar Kandil},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1-55860-804-4},
	journal = {Vldb},
	keywords = {sampling; database},
	pages = {19-28},
	title = {LEO - DB2's LEarning Optimizer},
	year = {2001}}

@inproceedings{GibbonsP2001,
	abstract = {Estimating the number of distinct values is a well-studied problem, due to its frequent occurrence in queries and its importance in selecting good query plans. Previous work has shown powerful nega-tive results on the quality of distinct-values esti-mates based on sampling (or other techniques that examine only part of the input data). We present an approach, called distinct sampling, that collects a specially tailored sample over the distinct values in the input, in a single scan of the data. In contrast to the previous negative results, our small Distinct Samples are guaranteed to accurately estimate the number of distinct values. The samples can be incrementally maintained up-to-date in the pres-ence of data insertions and deletions, with mini-mal time and memory overheads, so that the full scan may be performed only once. Moreover, a stored Distinct Sample can be used to accurately estimate the number of distinct values within any range specified by the query, or within any other subset of the data satisfying a query predicate. We present an extensive experimental study of distinct sampling. Using synthetic and real-world data sets, we show that distinct sampling gives distinct-values estimates to within 0%--10% rel-ative error, whereas previous methods typically incur 50%--250% relative error. Next, we show how distinct sampling can provide fast, highly-accurate approximate answers for " report " queries in high-volume, session-based event recording en-vironments, such as IP networks, customer service call centers, etc. For a commercial call center en-vironment, we show that a 1% Distinct Sample},
	author = {P. Gibbons and Phillip B Gibbons and Murray Hill Nj},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1-55860-804-4},
	journal = {Proc. VLDB'01},
	keywords = {sampling; aqp; database},
	pages = {541-550},
	title = {Distinct Sampling for Highly-Accurate Answers to Distinct Values Queries and Event Reports},
	year = {2001}}

@article{Au2001a,
	abstract = {An analytical study of the failure region of the first excursion reliability problem for linear dynamical systems subjected to Gaussian white noise excitation is carried out with a view to constructing a suitable importance sampling density for computing the first excursion failure probability. Central to the study are 'elementary failure regions', which are defined as the failure region in the load space corresponding to the failure of a particular output response at a particular instant. Each elementary failure region is completely characterized by its design point, which can be computed readily using impulse response functions of the system. It is noted that the complexity of the first excursion problem stems from the structure of the union of the elementary failure regions. One important consequence of this union structure is that, in addition to the global design point, a large number of neighboring design points are important in accounting for the failure probability. Using information from the analytical study, an importance sampling density is proposed. Numerical examples are presented, which demonstrate that the efficiency of using the proposed importance sampling density to calculate system reliability is remarkable. ?? 2001 Elsevier Science Ltd.},
	author = {S. K. Au and J. L. Beck},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1016/S0266-8920(01)00002-9},
	issn = {02668920},
	issue = {3},
	journal = {Probabilistic Engineering Mechanics},
	keywords = {First excursion problem,First passage problem,Importance sampling,Linear systems,Monte Carlo simulation,Reliability,first excursion problem,first passage problem,importance sampling,linear systems,monte carlo simulation,reliability; sampling; database},
	pages = {193-207},
	title = {First excursion probabilities for linear systems by very efficient importance sampling},
	volume = {16},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1016/S0266-8920(01)00002-9}}

@article{Andrieu2002,
	author = {Christophe Andrieu and Nando De Freitas and Arnaud Doucet and Michael I Jordan},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1-2},
	journal = {Machine learning},
	keywords = {markov chain monte carlo,mcmc,sampling,stochastic algorithms; database},
	pages = {1-39},
	publisher = {Springer},
	title = {An Introduction to MCMC for Machine Learning},
	volume = {50},
	year = {2002}}

@book{casella2002statistical,
	author = {George Casella and Roger L Berger},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {sampling; database},
	publisher = {Duxbury Pacific Grove, CA},
	title = {Statistical inference},
	volume = {2},
	year = {2002}}

@article{efron2003,
	abstract = {This brief review article is appearing in the issue of Statistical Science that marks the 25th anniversary of the bootstrap. It concerns some of the theoretical and methodological aspects of the bootstrap and how they might influence future work in statistics.},
	author = {Bradley Efron},
	date-modified = {2022-01-10 15:00:41 -0500},
	doi = {10.1214/ss/1063994968},
	isbn = {0883-4237},
	issn = {0883-4237},
	issue = {2},
	journal = {Statistical Science},
	keywords = {bootstrap; database; sampling},
	pages = {135-140},
	pmid = {1506186},
	title = {Second Thoughts on the Bootstrap},
	url = {http://projecteuclid.org/euclid.ss/1063994968},
	volume = {18},
	year = {2003},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxDbLi4vTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvRWZyb24gLSAyMDAzIC0gU2Vjb25kIFRob3VnaHRzIG9uIHRoZSBCb290c3RyYXAucGRmTxEDBgAAAAADBgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0Vmcm9uIC0gMjAwMyAtIFNlYyNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAJAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIA5C86VXNlcnM6Znl1OkxpYnJhcnk6R3JvdXAgQ29udGFpbmVyczpVQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlOk9uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXg6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHk6ZG9jOm15X2xpYnJhcnk6cGRmOkVmcm9uIC0gMjAwMyAtIFNlY29uZCBUaG91Z2h0cyBvbiB0aGUgQm9vdHN0cmFwLnBkZgAOAGgAMwBFAGYAcgBvAG4AIAAtACAAMgAwADAAMwAgAC0AIABTAGUAYwBvAG4AZAAgAFQAaABvAHUAZwBoAHQAcwAgAG8AbgAgAHQAaABlACAAQgBvAG8AdABzAHQAcgBhAHAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAOJVc2Vycy9meXUvTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvRWZyb24gLSAyMDAzIC0gU2Vjb25kIFRob3VnaHRzIG9uIHRoZSBCb290c3RyYXAucGRmABMAAS8AABUAAgAK//8AAAAIAA0AGgAkAQIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEDA==},
	bdsk-url-1 = {http://projecteuclid.org/euclid.ss/1063994968},
	bdsk-url-2 = {https://doi.org/10.1214/ss/1063994968}}

@article{Dobra2004,
	abstract = {Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed. Randomized techniques, based on computing small ``sketch'' synopses for each stream, have recently been shown to be a very effective tool for approximating the result of a single SQL query over streaming data tuples. In this paper, we investigate the problems arising when data-stream sketches are used to process multiple such queries concurrently. We demonstrate that, in the presence of multiple query expressions, intelligently sharing sketches among concurrent query evaluations can result in substantial improvements in the utilization of the available sketching space and the quality of the resulting approximation error guarantees. We provide necessary and sufficient conditions for multi- query sketch sharing that guarantee the correctness of the result-estimation process. We also investigate the difficult optimization problem of determining sketch-sharing configurations that are optimal (e.g., under a certain error metric for a given amount of space). We prove that optimal sketch sharing typically gives rise to NP-hard questions, and we propose novel heuristic algorithms for finding good sketch-sharing configurations in practice. Results from our experimental study with queries from the TPC-H benchmark verify the effectiveness of our approach, clearly demonstrating the benefits of our sketch-sharing methodology.},
	author = {Dobra and Garofalakis and Gehrke and Rastogi},
	date-modified = {2022-01-10 13:26:52 -0500},
	issn = {03029743},
	issue = {1},
	journal = {Edbt},
	keywords = {sampling; database},
	pages = {1-24},
	title = {Sketch-Based Multi-Query Processing over Data Streams},
	year = {2004}}

@article{Hamze2004,
	abstract = {We present new MCMC algorithms for com- puting the posterior distributions and expec- tations of the unknown variables in undi- rected graphical models with regular struc- ture. For demonstration purposes, we fo- cus on Markov Random Fields (MRFs). By partitioning the MRFs into non-overlapping trees, it is possible to compute the posterior distribution of a particular tree exactly by conditioning on the remaining tree. These exact solutions allow us to construct effi- cient blocked and Rao-Blackwellised MCMC algorithms. We show empirically that tree sampling is considerably more efficient than other partitioned sampling schemes and the naive Gibbs sampler, even in cases where loopy belief propagation fails to converge. We prove that tree sampling exhibits lower variance than the naive Gibbs sampler and other naive partitioning schemes using the theoretical measure of maximal correlation. We also construct new information theory tools for comparing different MCMC schemes and show that, under these, tree sampling is more efficient.},
	author = {Firas Hamze and N de Freitas},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {0-9749039-0-6},
	journal = {Uai},
	keywords = {sampling; database},
	pages = {243-250},
	title = {From fields to trees},
	url = {http://dl.acm.org/citation.cfm?id=1036873},
	year = {2004},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1036873}}

@article{Dean2004,
	abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
	author = {Jeffrey Dean and Sanjay Ghemawat},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1327452.1327492},
	isbn = {9781595936868},
	issn = {00010782},
	journal = {Proceedings of 6th Symposium on Operating Systems Design and Implementation},
	keywords = {sampling; database},
	pages = {137-149},
	pmid = {11687618},
	title = {MapReduce: Simplied Data Processing on Large Clusters},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1145/1327452.1327492}}

@inproceedings{Cormode2005-vldb,
	author = {Graham Cormode and Minos Garofalakis},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. VLDB'05},
	keywords = {sampling; database},
	pages = {13-24},
	title = {Sketching Streams Through the Net: Distributed Approximate Query Tracking},
	year = {2005}}

@article{pol-sigmod05,
	abstract = {Statistical estimation and approximate query processing have become increasingly prevalent applications for database systems. However, approximation is usually of little use without some sort of guarantee on estimation accuracy, or "confidence bound." ...},
	author = {Abhijit Pol and Christopher Jermaine},
	date-modified = {2022-01-10 15:27:22 -0500},
	doi = {http://doi.acm.org/10.1145/1066157.1066224},
	isbn = {1-59593-060-4},
	issn = {07308078},
	journal = {Proc. SIGMOD'05},
	keywords = {sampling; bootstrap; database},
	pages = {587-598},
	title = {Relational confidence bounds are easy with the bootstrap},
	year = {2005},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxD7Li4vTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvUG9sLCBKZXJtYWluZSAtIDIwMDUgLSBSZWxhdGlvbmFsIGNvbmZpZGVuY2UgYm91bmRzIGFyZSBlYXN5IHdpdGggdGhlIGJvb3RzdHJhcC5wZGZPEQOGAAAAAAOGAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fUG9sLCBKZXJtYWluZSAtIDIwI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAkAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgEELzpVc2VyczpmeXU6TGlicmFyeTpHcm91cCBDb250YWluZXJzOlVCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGU6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleDpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eTpkb2M6bXlfbGlicmFyeTpwZGY6UG9sLCBKZXJtYWluZSAtIDIwMDUgLSBSZWxhdGlvbmFsIGNvbmZpZGVuY2UgYm91bmRzIGFyZSBlYXN5IHdpdGggdGhlIGJvb3RzdHJhcC5wZGYADgCoAFMAUABvAGwALAAgAEoAZQByAG0AYQBpAG4AZQAgAC0AIAAyADAAMAA1ACAALQAgAFIAZQBsAGEAdABpAG8AbgBhAGwAIABjAG8AbgBmAGkAZABlAG4AYwBlACAAYgBvAHUAbgBkAHMAIABhAHIAZQAgAGUAYQBzAHkAIAB3AGkAdABoACAAdABoAGUAIABiAG8AbwB0AHMAdAByAGEAcAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIBAlVzZXJzL2Z5dS9MaWJyYXJ5L0dyb3VwIENvbnRhaW5lcnMvVUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZS9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4L09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5L2RvYy9teV9saWJyYXJ5L3BkZi9Qb2wsIEplcm1haW5lIC0gMjAwNSAtIFJlbGF0aW9uYWwgY29uZmlkZW5jZSBib3VuZHMgYXJlIGVhc3kgd2l0aCB0aGUgYm9vdHN0cmFwLnBkZgATAAEvAAAVAAIACv//AAAACAANABoAJAEiAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABKw=},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1066224},
	bdsk-url-2 = {http://doi.acm.org/10.1145/1066157.1066224}}

@article{Division2005,
	author = {U C Berkeley Division and Working Paper and Mark J Van Der Laan and Mark J van der Laan and U C Berkeley Division and Working Paper and Mark J Van Der Laan and Mark J van der Laan},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {sampling; database},
	title = {University of California , Berkeley Statistical Inference for Variable Importance},
	year = {2005}}

@article{Owen2005,
	abstract = {This work presents a version of the Metropolis-Hastings algorithm using quasi-Monte Carlo inputs. We prove that the method yields consistent estimates in some problems with finite state spaces and completely uniformly distributed inputs. In some numerical examples, the proposed method is much more accurate than ordinary Metropolis-Hastings sampling.},
	author = {A. B. Owen and S. D. Tribble},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1073/pnas.0409596102},
	issn = {0027-8424},
	issue = {25},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {sampling; database},
	pages = {8844-8849},
	pmid = {15956207},
	title = {A quasi-Monte Carlo Metropolis algorithm},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0409596102},
	volume = {102},
	year = {2005},
	bdsk-url-1 = {http://www.pnas.org/cgi/doi/10.1073/pnas.0409596102},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0409596102}}

@article{Chaudhuri2005,
	abstract = {A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. It is not even known whether it is possible to generate a sample of a join tree without first evaluating the join tree completely. We undertake a detailed study of this problem and attempt to analyze it in a variety of settings. We present theoretical results explaining the difficulty of this problem and setting limits on the efficiency that can be achieved. Based on new insights into the interaction between join and sampling, we develop join sampling techniques for the settings where our negative results do not apply. Our new sampling algorithms are significantly more efficient than those known earlier. We present experimental evaluation of our techniques on Microsoft's SQL Server 7.0.},
	author = {Surajit Chaudhuri and Rajeev Motwani and Vivek Narasayya},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/304181.304206},
	issn = {01635808},
	issue = {2},
	journal = {ACM SIGMOD Record},
	keywords = {sampling; database},
	pages = {263-274},
	title = {On random sampling over joins},
	volume = {28},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1145/304181.304206}}

@article{Leskovec2006,
	abstract = {Given a huge real graph, how can we derive a representative sample? There are many known algorithms to compute interesting measures (shortest paths, centrality, betweenness, etc.), but several of them become impractical for large graphs. Thus graph sampling is essential.The natural questions to ask are (a) which sampling method to use, (b) how small can the sample size be, and (c) how to scale up the measurements of the sample (e.g., the diameter), to get estimates for the large graph. The deeper, underlying question is subtle: how do we measure success?.We answer the above questions, and test our answers by thorough experiments on several, diverse datasets, spanning thousands nodes and edges. We consider several sampling methods, propose novel methods to check the goodness of sampling, and develop a set of scaling laws that describe relations between the properties of the original and the sample.In addition to the theoretical contributions, the practical conclusions from our work are: Sampling strategies based on edge selection do not perform well; simple uniform random node selection performs surprisingly well. Overall, best performing methods are the ones based on random-walks and "forest fire"; they match very accurately both static as well as evolutionary graph patterns, with sample sizes down to about 15% of the original graph.},
	author = {Jure Leskovec and Christos Faloutsos},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1150402.1150479},
	isbn = {1595933395},
	issn = {1595933395},
	journal = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
	keywords = {graph mining,graph sampling,scaling laws; sampling; database},
	pages = {631-636},
	title = {Sampling from large graphs},
	url = {http://dl.acm.org/citation.cfm?id=1150479},
	year = {2006},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1150479},
	bdsk-url-2 = {https://doi.org/10.1145/1150402.1150479}}

@article{Efraimidis2006,
	abstract = {In this work, a new algorithm for drawing a weighted random sample of size m from a population of n weighted items, where m???n, is presented. The algorithm can generate a weighted random sample in one-pass over unknown populations. ?? 2005 Elsevier B.V. All rights reserved.},
	author = {Pavlos S Efraimidis and Paul G Spirakis},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1016/j.ipl.2005.11.003},
	issn = {00200190},
	issue = {5},
	journal = {Information Processing Letters},
	keywords = {Data streams,Parallel algorithms,Randomized algorithms,Reservoir sampling,Weighted random sampling; sampling; database},
	pages = {181-185},
	pmid = {1000183068},
	title = {Weighted random sampling with a reservoir},
	volume = {97},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1016/j.ipl.2005.11.003}}

@inproceedings{estan2006end-biased,
	author = {Cristian Estan and Jeffrey F Naughton},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ICDE.2006.61},
	journal = {Proc. ICDE'06},
	keywords = {Frequency,Histograms,Mon,Telecommunication traffic; sampling; aqp; database},
	pages = {20-20},
	title = {End-biased Samples for Join Cardinality Estimation},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/ICDE.2006.61}}

@article{Newberg2007,
	abstract = {MOTIVATION: Identification of functionally conserved regulatory elements in sequence data from closely related organisms is becoming feasible, due to the rapid growth of public sequence databases. Closely related organisms are most likely to have common regulatory motifs; however, the recent speciation of such organisms results in the high degree of correlation in their genome sequences, confounding the detection of functional elements. Additionally, alignment algorithms that use optimization techniques are limited to the detection of a single alignment that may not be representative. Comparative-genomics studies must be able to address the phylogenetic correlation in the data and efficiently explore the alignment space, in order to make specific and biologically relevant predictions. RESULTS: We describe here a Gibbs sampler that employs a full phylogenetic model and reports an ensemble centroid solution. We describe regulatory motif detection using both simulated and real data, and demonstrate that this approach achieves improved specificity, sensitivity, and positive predictive value over non-phylogenetic algorithms, and over phylogenetic algorithms that report a maximum likelihood solution. AVAILABILITY: The software is freely available at http://bayesweb.wadsworth.org/gibbs/gibbs.html. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
	author = {Lee A Newberg and William A Thompson and Sean Conlan and Thomas M Smith and Lee Ann McCue and Charles E Lawrence},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1093/bioinformatics/btm241},
	isbn = {1367-4811 (Electronic)$\$r1367-4803 (Linking)},
	issn = {1367-4811},
	issue = {14},
	journal = {Bioinformatics (Oxford, England)},
	keywords = {sampling; database},
	pages = {1718-1727},
	pmid = {17488758},
	title = {A phylogenetic Gibbs sampler that yields centroid solutions for cis-regulatory site prediction},
	volume = {23},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1093/bioinformatics/btm241}}

@inproceedings{Al-Kateb2007,
	abstract = {Reservoir sampling is a well-known technique for sequential random sampling over data streams. Conventional reservoir sampling assumes a fixed-size reservoir. There are situations, however, in which it is necessary and/or advantageous to adaptively adjust the size of a reservoir in the middle of sampling due to changes in data characteristics and/or application behavior. This paper studies adaptive size reservoir sampling over data streams considering two main factors: reservoir size and sample uniformity. First, the paper conducts a theoretical study on the effects of adjusting the size of a reservoir while sampling is in progress. The theoretical results show that such an adjustment may bring a negative impact on the probability of the sample being uniform (called uniformity confidence herein). Second, the paper presents a novel algorithm for maintaining the reservoir sample after the reservoir size is adjusted such that the resulting uniformity confidence exceeds a given threshold. Third, the paper extends the proposed algorithm to an adaptive multi-reservoir sampling algorithm for a practical application in which samples are collected from memory-limited wireless sensor networks using a mobile sink. Finally, the paper empirically examines the adaptivity of the multi-reservoir sampling algorithm with regard to reservoir size and sample uniformity using real sensor networks data sets.},
	author = {Mohammed Al-Kateb and Byung Suk Lee and X. Sean Wang},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/SSDBM.2007.29},
	isbn = {0769528686},
	issn = {10993371},
	journal = {Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM},
	keywords = {sampling; database},
	title = {Adaptive-size reservoir sampling over data streams},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/SSDBM.2007.29}}

@article{Rusu2008-tods,
	author = {Florin Rusu and Alin Dobra},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1386118.1386121},
	issn = {03625915},
	issue = {3},
	journal = {ACM Transactions on Database Systems},
	keywords = {AGMS sketches,Count-Min sketches,DMAP,Fast range-summation,Fast-AGMS sketches,Fast-Count sketches,Size of join estimation; sampling; database},
	pages = {15:1-15:46},
	title = {Sketches for size of join estimation},
	volume = {33},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1145/1386118.1386121}}

@article{Walker2008,
	abstract = {Model-based algorithms are emerging as a preferred method for document clustering. As computing resources improve, methods such as Gibbs sampling have become more common for parameter estimation in these models. Gibbs sampling is well understood for many applications, but has not been extensively studied for use in document clustering. We explore the convergence rate, the possi- bility of label switching, and chain summarization methodologies for document clustering on a particular model, namely a mixture of multinomials model, and show that fairly simple methods can be employed, while still producing clusterings of superior quality compared to those produced with the EM algorithm.},
	author = {Daniel David Walker and Eric K Ringger},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1401890.1401975},
	isbn = {9781605581934},
	journal = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
	keywords = {EM,Gibbs sampling,MCMC,collapsed sam- plers,document clustering,practical guidelines; sampling; database},
	pages = {704-713},
	title = {Model-based document clustering with a collapsed gibbs sampler},
	url = {http://dl.acm.org/citation.cfm?doid=1401890.1401975},
	year = {2008},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=1401890.1401975},
	bdsk-url-2 = {https://doi.org/10.1145/1401890.1401975}}

@article{archer08,
	abstract = {Microarray studies yield data sets consisting of a large number of candidate predictors (genes) on a small number of observations (samples). When interest lies in predicting phenotypic class using gene expression data, often the goals are both to produce an accurate classifier and to uncover the predictive structure of the problem. Most machine learning methods, such as k-nearest neighbors, support vector machines, and neural networks, are useful for classification. However, these methods provide no insight regarding the covariates that best contribute to the predictive structure. Other methods, such as linear discriminant analysis, require the predictor space be substantially reduced prior to deriving the classifier. A recently developed method, random forests (RF), does not require reduction of the predictor space prior to classification. Additionally, RF yield variable importance measures for each candidate predictor. This study examined the effectiveness of RF variable importance measures in identifying the true predictor among a large number of candidate predictors. An extensive simulation study was conducted using 20 levels of correlation among the predictor variables and 7 levels of association between the true predictor and the dichotomous response. We conclude that the RF methodology is attractive for use in classification problems when the goals of the study are to produce an accurate classifier and to provide insight regarding the discriminative ability of individual predictor variables. Such goals are common among microarray studies, and therefore application of the RF methodology for the purpose of obtaining variable importance measures is demonstrated on a microarray data set. ?? 2007 Elsevier B.V. All rights reserved.},
	author = {Kellie J Archer and Ryan V Kimes},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1016/j.csda.2007.08.015},
	isbn = {0167-9473},
	issn = {01679473},
	issue = {4},
	journal = {Computational Statistics and Data Analysis},
	keywords = {Bootstrap aggregating,Classification tree,Random forest,Variable importance; sampling; bootstrap; database},
	pages = {2249-2260},
	title = {Empirical characterization of random forest variable importance measures},
	volume = {52},
	year = {2008},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxEBCy4uL0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL0FyY2hlciwgS2ltZXMgLSAyMDA4IC0gRW1waXJpY2FsIGNoYXJhY3Rlcml6YXRpb24gb2YgcmFuZG9tIGZvcmVzdCB2YXJpYWJsZSBpbXBvcnRhbmNlIG1lYXN1cmVzLnBkZk8RA8YAAAAAA8YAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BcmNoZXIsIEtpbWVzIC0gMjAjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEACQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACARQvOlVzZXJzOmZ5dTpMaWJyYXJ5Okdyb3VwIENvbnRhaW5lcnM6VUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZTpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4Ok9uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5OmRvYzpteV9saWJyYXJ5OnBkZjpBcmNoZXIsIEtpbWVzIC0gMjAwOCAtIEVtcGlyaWNhbCBjaGFyYWN0ZXJpemF0aW9uIG9mIHJhbmRvbSBmb3Jlc3QgdmFyaWFibGUgaW1wb3J0YW5jZSBtZWFzdXJlcy5wZGYADgDIAGMAQQByAGMAaABlAHIALAAgAEsAaQBtAGUAcwAgAC0AIAAyADAAMAA4ACAALQAgAEUAbQBwAGkAcgBpAGMAYQBsACAAYwBoAGEAcgBhAGMAdABlAHIAaQB6AGEAdABpAG8AbgAgAG8AZgAgAHIAYQBuAGQAbwBtACAAZgBvAHIAZQBzAHQAIAB2AGEAcgBpAGEAYgBsAGUAIABpAG0AcABvAHIAdABhAG4AYwBlACAAbQBlAGEAcwB1AHIAZQBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgESVXNlcnMvZnl1L0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL0FyY2hlciwgS2ltZXMgLSAyMDA4IC0gRW1waXJpY2FsIGNoYXJhY3Rlcml6YXRpb24gb2YgcmFuZG9tIGZvcmVzdCB2YXJpYWJsZSBpbXBvcnRhbmNlIG1lYXN1cmVzLnBkZgATAAEvAAAVAAIACv//AAAACAANABoAJAEzAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABP0=},
	bdsk-url-1 = {https://doi.org/10.1016/j.csda.2007.08.015}}

@article{Strobl2008,
	author = {Carolin Strobl and Anne-Laure Boulesteix and Thomas Kneib and Thomas Augustin and Achim Zeileis},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1186/1471-2105-9-307},
	issn = {1471-2105},
	issue = {1},
	journal = {BMC Bioinformatics},
	keywords = {sampling; database},
	pages = {307},
	title = {Conditional Variable Importance for Random Forests},
	url = {http://www.biomedcentral.com/1471-2105/9/307},
	volume = {9},
	year = {2008},
	bdsk-url-1 = {http://www.biomedcentral.com/1471-2105/9/307},
	bdsk-url-2 = {https://doi.org/10.1186/1471-2105-9-307}}

@article{Ringner2008a,
	abstract = {Principal component analysis is often incorporated into genome-wide expression studies, but what is it and how can it be used to explore high-dimensional data?},
	author = {M Ringner and Markus Ringn{\'e}r and M Ringner},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1038/nbt0308-303},
	isbn = {1546-1696},
	issn = {1087-0156},
	issue = {3},
	journal = {Nat Biotechnol},
	keywords = {sampling; database},
	pages = {303-304},
	pmid = {18327243},
	title = {What is principal component analysis?},
	url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve%7B&%7Ddb=PubMed%7B&%7Ddopt=Citation%7B&%7Dlist%7B_%7Duids=18327243 http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Citation&list_uids=18327243},
	volume = {26},
	year = {2008},
	bdsk-url-1 = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve%7B&%7Ddb=PubMed%7B&%7Ddopt=Citation%7B&%7Dlist%7B_%7Duids=18327243%20http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Citation&list_uids=18327243},
	bdsk-url-2 = {https://doi.org/10.1038/nbt0308-303}}

@book{chernick2008a,
	abstract = {A practical and accessible introduction to the bootstrap method------newly revised and updatedOver the past decade, the application of bootstrap methods to new areas of study has expanded, resulting in theoretical and applied advances across various fields. Bootstrap Methods, Second Edition is a highly approachable guide to the multidisciplinary, real-world uses of bootstrapping and is ideal for readers who have a professional interest in its methods, but are without an advanced background in mathematics.Updated to reflect current techniques and the most up-to-date work on the topic, the Second Edition features:The addition of a second, extended bibliography devoted solely to publications from 1999--2007, which is a valuable collection of references on the latest research in the fieldA discussion of the new areas of applicability for bootstrap methods, including use in the pharmaceutical industry for estimating individual and population bioequivalence in clinical trialsA revised chapter on when and why bootstrap fails and remedies for overcoming these drawbacksAdded coverage on regression, censored data applications, P-value adjustment, ratio estimators, and missing dataNew examples and illustrations as well as extensive historical notes at the end of each chapterWith a strong focus on application, detailed explanations of methodology, and complete coverage of modern developments in the field, Bootstrap Methods, Second Edition is an indispensable reference for applied statisticians, engineers, scientists, clinicians, and other practitioners who regularly use statistical methods in research. It is also suitable as a supplementary text for courses in statistics and resampling methods at the upper-undergraduate and graduate levels.},
	author = {Michael R. Chernick},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {9780471756217},
	isbn = {1118211596},
	issn = {0254-6450},
	journal = {Wiley series in probability and statistics},
	keywords = {sampling; bootstrap; database},
	pages = {400},
	pmid = {21162849},
	title = {Bootstrap methods: a guide for practitioners and researchers},
	url = {https://www.evernote.com/shard/s13/nl/1480559/3eaa5707-59c1-4eef-b091-534589b51d30/},
	year = {2008},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/3eaa5707-59c1-4eef-b091-534589b51d30/},
	bdsk-url-2 = {https://doi.org/9780471756217}}

@article{Hasan2009,
	author = {Mohammad Al Hasan and Mohammed J. Zaki},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/1687627.1687710},
	isbn = {0000000000000},
	issn = {2150-8097},
	issue = {1},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {sampling; database},
	pages = {730-741},
	title = {Output space sampling for graph patterns},
	url = {http://dl.acm.org/citation.cfm?id=1687627.1687710},
	volume = {2},
	year = {2009},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1687627.1687710},
	bdsk-url-2 = {https://doi.org/10.14778/1687627.1687710}}

@article{spie09tug,
	author = {Joshua Spiegel and Neoklis Polyzotis},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1508857.1508860},
	issn = {0362-5915},
	issue = {1},
	journal = {ACM Trans. Database Syst.},
	keywords = {Data synopses,approximate query processing,selectivity estimation; sampling; database},
	month = {4},
	pages = {3:1----3:56},
	publisher = {ACM},
	title = {TuG synopses for approximate query answering},
	url = {http://doi.acm.org/10.1145/1508857.1508860},
	volume = {34},
	year = {2009},
	bdsk-url-1 = {http://doi.acm.org/10.1145/1508857.1508860},
	bdsk-url-2 = {https://doi.org/10.1145/1508857.1508860}}

@article{Jiang2009,
	author = {Zhewei Jiang},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1108/17440080910983565},
	issue = {3},
	keywords = {databases,extensible markup language,programming and algorithm theory,query languages; sampling; database},
	pages = {305-326},
	title = {Efficient XML tree pattern query evaluation using a novel one-phase holistic twig join scheme},
	volume = {5},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1108/17440080910983565}}

@article{Gromping2009,
	abstract = {Relative importance of regressor variables is an old topic that still awaits a satisfactory solution. When interest is in attributing importance in linear regression, averaging over orderings methods for decomposing R(2) are among the state-of-the-art methods, although the mechanism behind their behavior is not (yet) completely understood. Random forests-a machine-learning tool for classification and regression proposed a few years ago-have an inherent procedure of producing variable importances. This article compares the two approaches (linear model on the one hand and two versions of random forests on the other hand) and finds both striking similarities and differences, some of which can be explained whereas others remain a challenge. The investigation improves understanding of the nature of variable importance in random forests. This article has supplementary material online.},
	author = {Ulrike Gr{\"o}mping},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1198/tast.2009.08199},
	isbn = {0003-1305},
	issn = {0003-1305},
	issue = {4},
	journal = {The American Statistician},
	keywords = {Linear model,Random forest,Variable importance; sampling; database},
	pages = {308-319},
	title = {Variable Importance Assessment in Regression: Linear Regression versus Random Forest},
	volume = {63},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1198/tast.2009.08199}}

@article{Haas2009,
	author = {Peter J. Haas and Ihab F. Ilyas and Guy M. Lohman and Volker Markl},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {4},
	journal = {Statistical Analysis and Data Mining},
	keywords = {data mining,query feedback,query optimization,relational database,sampling,statistical structure; aqp; database},
	pages = {223-250},
	publisher = {Wiley Online Library},
	title = {Discovering and Exploiting Statistical Properties for Query Optimization in Relational Databases: A Survey},
	volume = {1},
	year = {2009}}

@incollection{encyc-aqp,
	author = {Qing Liu},
	booktitle = {Encyclopedia of Database Systems},
	city = {Boston, MA},
	date-modified = {2022-01-22 11:10:36 -0500},
	doi = {10.1007/978-0-387-39940-9_534},
	editor = {LIU, LING and {\"O}ZSU, M. TAMER},
	isbn = {978-0-387-39940-9},
	keywords = {sampling; aqp; database},
	pages = {113--119},
	publisher = {Springer US},
	title = {Approximate Query Processing},
	url = {https://doi.org/10.1007/978-0-387-39940-9_534},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1007/978-0-387-39940-9_534}}

@article{Nath2010,
	abstract = {Recent advances in flash media have made it an attractive alternative for data storage in a wide spectrum of computing devices, such as embedded sensors, mobile phones, PDA's, laptops, and even servers. However, flash media has many unique characteristics ...},
	author = {Suman Nath and Phillip B Gibbons},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-009-0164-z},
	isbn = {1-58113-859-8},
	issn = {10668888},
	issue = {1},
	journal = {VLDB Journal},
	keywords = {Flash storage,Random sample,Semi-random writes,Sensor networks; sampling; database},
	pages = {67-90},
	title = {Online maintenance of very large random samples on flash storage},
	volume = {19},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/s00778-009-0164-z}}

@article{Huang2010,
	abstract = {MapReduce programming model is emerging as an efficient tool for data-intensive applications. Hadoop, an open-source implementation of MapReduce, has been widely adopted and experienced by both academia and enterprise. Recently, lots of efforts have been done on improving the performance of MapReduce system and on analyzing the MapReduce process based on the log files generated during the Hadoop execution. Visualizing log files seems to be a very useful tool to understand the behavior of the Hadoop process. In this paper, we present MR-Scope, a real-time MapReduce tracing tool. MR-Scope provides a real-time insight of the MapReduce process, including the ongoing progress of every task hosted in Task Tracker. In addition, it displays the health of the Hadoop cluster data nodes, the distribution of the file system's blocks and their replicas and the content of the different block splits of the file system. We implement MR-Scope in native Hadoop 0.1. Experimental results demonstrat that MR-Scope's overhead is less than 4\{%\} when running wordcount benchmark.},
	author = {Dachuan Huang and Xuanhua Shi and Shadi Ibrahim and Lu Lu and Hongzhang Liu and Song Wu and Hai Jin},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1851476.1851598},
	isbn = {9781605589428},
	journal = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing - HPDC '10},
	keywords = {Hadoop,MapReduce,real-time tracing,visualization; sampling; database},
	pages = {849-855},
	title = {MR-Scope: A Real-Time Tracing Tool for MapReduce},
	url = {http://portal.acm.org/citation.cfm?doid=1851476.1851598},
	year = {2010},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=1851476.1851598},
	bdsk-url-2 = {https://doi.org/10.1145/1851476.1851598}}

@article{Zliobaite2010,
	abstract = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research fields and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift research field, provide a context of the research and position it within broad spectrum of research fields and applications.},
	author = {Indr{\.e} {\v Z}liobait{\.e}},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1002/sam},
	isbn = {9781634393973},
	issn = {09574174},
	keywords = {data,interactive pattern mining,mcmc sampling,pattern mining from hidden; sampling; database},
	pmid = {21824845},
	title = {Learning under Concept Drift: an Overview},
	url = {http://arxiv.org/abs/1010.4784},
	year = {2010},
	bdsk-url-1 = {http://arxiv.org/abs/1010.4784},
	bdsk-url-2 = {https://doi.org/10.1002/sam}}

@article{Wick2010,
	abstract = {Probabilistic databases play a crucial role in the management and understanding of uncertain data. However, incorporating probabilities into the semantics of incomplete databases has posed many challenges, forcing systems to sacrifice modeling power, scalability, or restrict the class of relational algebra formula under which they are closed. We propose an alternative approach where the underlying relational database always represents a single world, and an external factor graph encodes a distribution over possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity. Our approach allows the efficient evaluation of arbitrary queries over probabilistic databases with arbitrary dependencies expressed by graphical models with structure that changes during inference. MCMC sampling provides efficiency by hypothesizing \{\em modifications\} to possible worlds rather than generating entire worlds from scratch. Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world. A significant innovation of this work is the connection between MCMC sampling and materialized view maintenance techniques: we find empirically that using view maintenance techniques is several orders of magnitude faster than naively querying each sampled world. We also demonstrate our system's ability to answer relational queries with aggregation, and demonstrate additional scalability through the use of parallelization.},
	author = {Michael Wick and Andrew McCallum and Gerome Miklau},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/1920841.1920942},
	isbn = {9781450320375},
	issn = {21508097},
	keywords = {sampling; database},
	title = {Scalable Probabilistic Databases with Factor Graphs and MCMC},
	url = {http://arxiv.org/abs/1005.1934},
	year = {2010},
	bdsk-url-1 = {http://arxiv.org/abs/1005.1934},
	bdsk-url-2 = {https://doi.org/10.14778/1920841.1920942}}

@article{Wei2011,
	abstract = {MOTIVATION: RNA secondary structure plays an important role in the function of many RNAs, and structural features are often key to their interaction with other cellular components. Thus, there has been considerable interest in the prediction of secondary structures for RNA families. In this article, we present a new global structural alignment algorithm, RNAG, to predict consensus secondary structures for unaligned sequences. It uses a blocked Gibbs sampling algorithm, which has a theoretical advantage in convergence time. This algorithm iteratively samples from the conditional probability distributions P(Structure | Alignment) and P(Alignment | Structure). Not surprisingly, there is considerable uncertainly in the high-dimensional space of this difficult problem, which has so far received limited attention in this field. We show how the samples drawn from this algorithm can be used to more fully characterize the posterior space and to assess the uncertainty of predictions.$\$n$\$nRESULTS: Our analysis of three publically available datasets showed a substantial improvement in RNA structure prediction by RNAG over extant prediction methods. Additionally, our analysis of 17 RNA families showed that the RNAG sampled structures were generally compact around their ensemble centroids, and at least 11 families had at least two well-separated clusters of predicted structures. In general, the distance between a reference structure and our predicted structure was large relative to the variation among structures within an ensemble.$\$n$\$nAVAILABILITY: The Perl implementation of the RNAG algorithm and the data necessary to reproduce the results described in Sections 3.1 and 3.2 are available at http://ccmbweb.ccv.brown.edu/rnag.html$\$n$\$nCONTACT: charles\{_\}lawrence@brown.edu$\$n$\$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
	author = {Donglai Wei and Lauren V Alpert and Charles E Lawrence},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1093/bioinformatics/btr421},
	isbn = {1367-4811 (Electronic)$\$r1367-4803 (Linking)},
	issn = {1367-4811},
	issue = {18},
	journal = {Bioinformatics (Oxford, England)},
	keywords = {Algorithms,Base Sequence,Cluster Analysis,Molecular Sequence Data,Nucleic Acid Conformation,RNA,RNA: chemistry,RNA: methods,Sequence Alignment,Sequence Analysis; sampling; database},
	pages = {2486-2493},
	pmid = {21788211},
	title = {RNAG: a new Gibbs sampler for predicting RNA secondary structure for unaligned sequences.},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3167047%7B&%7Dtool=pmcentrez%7B&%7Drendertype=abstract},
	volume = {27},
	year = {2011},
	bdsk-url-1 = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3167047%7B&%7Dtool=pmcentrez%7B&%7Drendertype=abstract},
	bdsk-url-2 = {https://doi.org/10.1093/bioinformatics/btr421}}

@article{Buck2011,
	abstract = {Hadoop has become the de facto platform for large-scale data analysis in commercial applications, and increasingly so in scientific applications. However, Hadoop's byte stream data model causes inefficiencies when used to process scientific data that is commonly stored in highly-structured, array-based binary file formats resulting in limited scalability of Hadoop applications in science. We introduce Sci-Hadoop, a Hadoop plugin allowing scientists to specify logical queries over array-based data models. Sci-Hadoop executes queries as map/reduce programs defined over the logical data model. We describe the implementation of a Sci-Hadoop prototype for NetCDF data sets and quantify the performance of five separate optimizations that address the following goals for several representative aggregate queries: reduce total data transfers, reduce remote reads, and reduce unnecessary reads. Two optimizations allow holistic aggregate queries to be evaluated opportunistically during the map phase; two additional optimizations intelligently partition input data to increase read locality, and one optimization avoids block scans by examining the data dependencies of an executing query to prune input partitions. Experiments involving a holistic function show run-time improvements of up to 8x, with drastic reductions of IO, both locally and over the network.},
	author = {Joe B Buck and Noah Watkins and Jeff LeFevre and Kleoni Ioannidou and Carlos Maltzahn and Neoklis Polyzotis and Scott Brandt},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2063384.2063473},
	isbn = {9781450307710},
	journal = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis on - SC '11},
	keywords = {data intensive,map reduce,query op-,scientific file-formats; sampling; database},
	pages = {1},
	title = {SciHadoop},
	url = {http://dl.acm.org/citation.cfm?doid=2063384.2063473},
	year = {2011},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2063384.2063473},
	bdsk-url-2 = {https://doi.org/10.1145/2063384.2063473}}

@inproceedings{Wlodarczyk,
	abstract = {Query processing using mostly various NoSQL languages becomes a significant application area for Hadoop. Despite significant work on performance improvement of these languages the performance dependence on basic configuration parameters seems not to be fully considered. In this paper we present a relatively comprehensive study into influence the basic configuration parameters have on performance of typical types of queries. We choose three queries from Lehigh University Benchmark that can represent the most typical challenges and we analyze their dependence on parameters such as: dataset size, number of nodes, number of reducers and loading overhead. The results indicate strong dependence on the amount of reducers and IO performance of the cluster, which proves the common opinion that MapReduce is IO bound. These results can help to compare performance behavior of different languages and serve as a basis for understanding the influence of configuration parameters on the final performance.},
	author = {Tomasz Wiktor Wlodarczyk and Yi Han and Chunming Rong},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/WAINA.2011.130},
	isbn = {978-1-61284-829-7},
	journal = {2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications},
	keywords = {sampling; database},
	month = {3},
	note = {% Tyler 6/20/2016<br/><br/>published: 2011<br/>cited: 11<br/><br/>this paper is from IEEE, but I feel that it has significance so I wanted to add it in case we get an account and can download it.<br/><br/><b>Main idea</b>: In this paper we present a relatively comprehensive study into influence the basic configuration parameters have on performance of typical types of queries using NoSQL on Hadoop.},
	pages = {507-513},
	publisher = {IEEE},
	title = {Performance Analysis of Hadoop for Query Processing},
	url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5763552&url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5763552 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5763552 http://ieeexplore.ieee.org/xpl/articleDetails.jsp},
	year = {2011},
	bdsk-url-1 = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5763552&url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5763552%20http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5763552%20http://ieeexplore.ieee.org/xpl/articleDetails.jsp},
	bdsk-url-2 = {https://doi.org/10.1109/WAINA.2011.130}}

@article{Mccallum2011,
	abstract = {Traditional approaches to probabilistic inference such as loopy belief propagation and Gibbs sampling typically compute marginals for all the unobserved variables in a graphical model. However, in many real-world applications the user's inter- ests are focused on a subset of the variables, specified by a query. In this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten. In this paper we propose a query-specific approach to MCMC that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efficiency. Surprisingly there has been almost no previous work on query-aware MCMC. We demonstrate the success of our approach with positive experimental results on a wide range of graphical models.},
	author = {Andrew Mccallum},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9781618395993},
	journal = {Advances in Neural Information Processing Systems 24},
	keywords = {sampling; database},
	pages = {2564-2572},
	title = {Query-Aware MCMC},
	url = {https://papers.nips.cc/paper/4237-query-aware-mcmc.pdf},
	year = {2011},
	bdsk-url-1 = {https://papers.nips.cc/paper/4237-query-aware-mcmc.pdf}}

@article{Cohen2011,
	abstract = {A Bottom-sketch is a summary of a set of items with nonnegative weights that supports approximate query processing. A sketch is obtained by associating with each item in a ground set an independent random rank drawn from a probability distribution that depends on the weight of the item and including the k items with smallest rank value. Bottom-k sketches are an alternative to k-mins sketches[9], which consist of the k minimum ranked items in k independent rank assignments,and of min-hash [5] sketches, where hash functions replace random rank assignments. Sketches support approximate aggregations, including weight and selectivity of a subpopulation. Coordinated sketches of multiple subsets over the same ground set support subset-relation queries such as Jaccard similarity or the weight of the union. All-distances sketches are applicable for datasets where items lie in some metric space such as data streams (time) or networks. These sketches compactly encode the respective plain sketches of all neighborhoods of a location. These sketches support queries posed over time windows or neighborhoods and time/spatially decaying aggregates. An important advantage of bottom-k sketches, established in a line of recent work, is much tighter estimators for several basic aggregates. To materialize this benefit, we must adapt traditional k-mins applications to use bottom-k sketches. We propose all-distances bottom-k sketches and develop and analyze data structures that incrementally construct bottom-k sketches and all-distances bottom-k sketches. Another advantage of bottom-k sketches is that when the data is represented explicitly, they can be obtained much more efficiently than k-mins sketches. We show that k-mins sketches can be derived from respective bottom-k sketches, which enables the use of bottom-k sketches with off-the-shelf k-mins estimators. (In fact, we obtain tighter estimators since each bottom-k sketch is adistribution over k-mins sketches).},
	author = {Edith Cohen and Haim Kaplan},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/1281100.1281133},
	isbn = {9781595936165},
	keywords = {all-distances sketches,bottom-k sketches,data streams; sampling; database},
	pages = {225},
	title = {Summarizing data using bottom-k sketches},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1145/1281100.1281133}}

@inproceedings{xu2012sampling,
	author = {Yujie Xu and Peng Zou and Wenyu Qu and Zhiyang Li and Keqiu Li and Xiaoli Cui},
	date-modified = {2022-01-10 13:26:52 -0500},
	institution = {IEEE},
	journal = {ChinaGrid Annual Conference (ChinaGrid), 2012 Seventh},
	keywords = {sampling; database},
	pages = {1-8},
	title = {Sampling-based partitioning in MapReduce for skewed data},
	year = {2012}}

@article{Feng2012,
	abstract = {The increasing use of statistical data analysis in enterprise applications has created an arms race among database vendors to offer ever more sophisticated in-database analytics. One challenge in this race is that each new statistical technique must be implemented from scratch in the RDBMS, which leads to a lengthy and complex development process. We argue that the root cause for this overhead is the lack of a unified architecture for in-database analytics. Our main contribution in this work is to take a step towards such a unified architecture. A key benefit of our unified architecture is that performance optimizations for analytics techniques can be studied generically instead of an ad hoc, per-technique fashion. In particular, our technical contributions are theoretical and empirical studies of two key factors that we found impact performance: the order data is stored, and parallelization of computations on a single-node multicore RDBMS. We demonstrate the feasibility of our architecture by integrating several popular analytics techniques into two commercial and one open-source RDBMS. Our architecture requires changes to only a few dozen lines of code to integrate a new statistical technique. We then compare our approach with the native analytics tools offered by the commercial RDBMSes on various analytics tasks, and validate that our approach achieves competitive or higher performance, while still achieving the same quality.},
	author = {Xixuan Feng and Arun Kumar and Benjamin Recht and Christopher R{\'e}},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2213836.2213874},
	isbn = {978-1-4503-1247-9},
	issn = {07308078},
	journal = {Proceedings of the 2012 international conference on Management of Data},
	keywords = {analytics,convex programming,incremental gradient descent,user-defined aggregate; sampling; database},
	pages = {325-336},
	title = {Towards a unified architecture for in-RDBMS analytics},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1145/2213836.2213874}}

@inproceedings{grover2012extending,
	author = {Raman Grover and Michael J Carey},
	date-modified = {2022-01-10 13:26:52 -0500},
	institution = {IEEE},
	journal = {Data Engineering (ICDE), 2012 IEEE 28th International Conference on},
	keywords = {sampling; database},
	pages = {486-497},
	title = {Extending map-reduce for efficient predicate-based sampling},
	year = {2012}}

@article{laptev2012earl,
	abstract = {Approximate results based on samples often provide the only way in which advanced analytical applications on very massive data sets can satisfy their time and resource constraints. Unfortunately, methods and tools for the computation of accurate early results are currently not supported in \{MapReduce-oriented\} systems although these are intended for 'big data'. Therefore, we proposed and implemented a non-parametric extension of Hadoop which allows the incremental computation of early results for arbitrary work-flows, along with reliable on-line estimates of the degree of accuracy achieved so far in the computation. These estimates are based on a technique called bootstrapping that has been widely employed in statistics and can be applied to arbitrary functions and data distributions. In this paper, we describe our Early Accurate Result Library (\{EARL)\} for Hadoop that was designed to minimize the changes required to the \{MapReduce\} framework. Various tests of \{EARL\} of Hadoop are presented to characterize the frequent situations where \{EARL\} can provide major speed-ups over the current version of Hadoop.},
	author = {Nikolay Laptev and Kai Zeng and Carlo Zaniolo},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/2336664.2336675},
	isbn = {21508097 (ISSN)},
	issn = {2150-8097},
	issue = {10},
	journal = {Proc. VLDB Endow.},
	keywords = {sampling; big data; bootstrap; database},
	month = {6},
	pages = {1028-1039},
	publisher = {VLDB Endowment},
	title = {Early Accurate Results for Advanced Analytics on MapReduce},
	url = {http://dx.doi.org/10.14778/2336664.2336675 http://dl.acm.org/citation.cfm?id=2336664.2336675 http://arxiv.org/abs/1207.0142},
	volume = {5},
	year = {2012},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxEBBS4uL0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL0xhcHRldiwgWmVuZywgWmFuaW9sbyAtIDIwMTIgLSBFYXJseSBBY2N1cmF0ZSBSZXN1bHRzIGZvciBBZHZhbmNlZCBBbmFseXRpY3Mgb24gTWFwUmVkdWNlLnBkZk8RA64AAAAAA64AAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9MYXB0ZXYsIFplbmcsIFphbmkjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEACQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAQ4vOlVzZXJzOmZ5dTpMaWJyYXJ5Okdyb3VwIENvbnRhaW5lcnM6VUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZTpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4Ok9uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5OmRvYzpteV9saWJyYXJ5OnBkZjpMYXB0ZXYsIFplbmcsIFphbmlvbG8gLSAyMDEyIC0gRWFybHkgQWNjdXJhdGUgUmVzdWx0cyBmb3IgQWR2YW5jZWQgQW5hbHl0aWNzIG9uIE1hcFJlZHVjZS5wZGYADgC8AF0ATABhAHAAdABlAHYALAAgAFoAZQBuAGcALAAgAFoAYQBuAGkAbwBsAG8AIAAtACAAMgAwADEAMgAgAC0AIABFAGEAcgBsAHkAIABBAGMAYwB1AHIAYQB0AGUAIABSAGUAcwB1AGwAdABzACAAZgBvAHIAIABBAGQAdgBhAG4AYwBlAGQAIABBAG4AYQBsAHkAdABpAGMAcwAgAG8AbgAgAE0AYQBwAFIAZQBkAHUAYwBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgEMVXNlcnMvZnl1L0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL0xhcHRldiwgWmVuZywgWmFuaW9sbyAtIDIwMTIgLSBFYXJseSBBY2N1cmF0ZSBSZXN1bHRzIGZvciBBZHZhbmNlZCBBbmFseXRpY3Mgb24gTWFwUmVkdWNlLnBkZgATAAEvAAAVAAIACv//AAAACAANABoAJAEtAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABN8=},
	bdsk-url-1 = {http://dx.doi.org/10.14778/2336664.2336675%20http://dl.acm.org/citation.cfm?id=2336664.2336675%20http://arxiv.org/abs/1207.0142},
	bdsk-url-2 = {http://dx.doi.org/10.14778/2336664.2336675}}

@article{Vojnovic2012a,
	abstract = {Big Data Analytics requires partitioning datasets into thousands of partitions according to a specific set of keys so that different machines can process different partitions in parallel. Range partition is one of the ways to partition the data that is needed whenever global ordering is required. It partitions the data according to a pre-defined set of exclusive and continuous ranges that covers the entire domain of the partition key. Providing high-quality (approximately equal-sized) partitions is a key problem for the big data analytics because the job latency is determined by the most loaded node. This problem is especially challenging because typically no statistics about the key distribution over machines for an input dataset is available at the beginning of a range partition. The system needs to find a way to determine the partition boundaries that is both cost-effective and accurate. This paper presents a weighted-sampling based approach, implemented in Cosmos--the cloud infrastructure for big data analytics used by Microsoft Online Service Division. The approach has been used by many jobs daily and was found to be both efficient and providing desired partition quality.},
	author = {M. Vojnovic and F. Xu and J. Zhou},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Microsoft Research},
	keywords = {partition; sampling; big data; database},
	title = {Sampling Based Range Partition Methods for Big Data Analytics},
	url = {http://research.microsoft.com/pubs/159275/MSR-TR-2012-18.pdf},
	year = {2012},
	bdsk-url-1 = {http://research.microsoft.com/pubs/159275/MSR-TR-2012-18.pdf}}

@article{kleiner2012-blb,
	author = {A Kleiner and A Talwalkar and P Sarkar and M Jordan},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Machine Learning,Statistics - Machine Learning; sampling; big data; bootstrap; database},
	month = {6},
	title = {The Big Data Bootstrap},
	year = {2012},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxDaLi4vTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvS2xlaW5lciBldCBhbC4gLSAyMDEyIC0gVGhlIEJpZyBEYXRhIEJvb3RzdHJhcC5wZGZPEQMEAAAAAAMEAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fS2xlaW5lciBldCBhbC4gLSAyI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAkAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgDjLzpVc2VyczpmeXU6TGlicmFyeTpHcm91cCBDb250YWluZXJzOlVCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGU6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleDpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eTpkb2M6bXlfbGlicmFyeTpwZGY6S2xlaW5lciBldCBhbC4gLSAyMDEyIC0gVGhlIEJpZyBEYXRhIEJvb3RzdHJhcC5wZGYAAA4AZgAyAEsAbABlAGkAbgBlAHIAIABlAHQAIABhAGwALgAgAC0AIAAyADAAMQAyACAALQAgAFQAaABlACAAQgBpAGcAIABEAGEAdABhACAAQgBvAG8AdABzAHQAcgBhAHAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAOFVc2Vycy9meXUvTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvS2xlaW5lciBldCBhbC4gLSAyMDEyIC0gVGhlIEJpZyBEYXRhIEJvb3RzdHJhcC5wZGYAABMAAS8AABUAAgAK//8AAAAIAA0AGgAkAQEAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAECQ==}}

@article{Diaconis2012,
	abstract = {We develop algorithms for sampling from a probability distribution on a submanifold embedded in Rn. Applications are given to the evaluation of algorithms in 'Topological Statistics'; to goodness of fit tests in exponential families and to Neyman's smooth test. This article is partially expository, giving an introduction to the tools of geometric measure theory.},
	author = {Persi Diaconis and Susan Holmes and Mehrdad Shahshahani},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {and phrases,conditional distribution,geometric measure theory,manifold,sampling; database},
	pages = {1-21},
	title = {Sampling From A Manifold},
	url = {http://arxiv.org/abs/1206.6913},
	year = {2012},
	bdsk-url-1 = {http://arxiv.org/abs/1206.6913}}

@article{Das2013,
	author = {Barnan Das and Narayanan C Krishnan and Diane J Cook},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/ICDM.2013.18},
	isbn = {978-0-7695-5108-1},
	issn = {15504786},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	keywords = {Gibbs sampling,Imbalanced class distribution,Markov chain Monte Carlo (MCMC),oversampling; sampling; database},
	pages = {111-120},
	title = {WRACOG: A gibbs sampling-based oversampling technique},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/ICDM.2013.18}}

@article{Borgs2013,
	abstract = {A fundamental problem arising in many applications in Web science and social network analysis is, given an arbitrary approximation factor \{$\}c\{>\}1\{$\}, to output a set \{$\}S\{$\} of nodes that with high probability contains all nodes of PageRank at least \{$\}\backslashDelta\{\$\}, and no node of PageRank smaller than \{$\}\backslashDelta/c\{$\}. We call this problem \{\{\}$\$sc SignificantPageRanks\{\}\}. We develop a nearly optimal, local algorithm for the problem with runtime complexity \{$\}\backslashtilde\{\\{\}O\{\\}\}(n/\backslashDelta)\{$\} on networks with \{$\}n\{$\} nodes. We show that any algorithm for solving this problem must have runtime of \{$\}\{\{\}\backslashOmega\{\\}\}(n/\backslashDelta)\{$\}, rendering our algorithm optimal up to logarithmic factors. Our algorithm comes with two main technical contributions. The first is a multi-scale sampling scheme for a basic matrix problem that could be of interest on its own. In the abstract matrix problem it is assumed that one can access an unknown \{\{\}$\$em right-stochastic matrix\{\}\} by querying its rows, where the cost of a query and the accuracy of the answers depend on a precision parameter \{$\}\backslashepsilon\{\$\}. At a cost propositional to \{$\}1/\backslashepsilon\{\$\}, the query will return a list of \{$\}O(1/\backslashepsilon)\{$\} entries and their indices that provide an \{$\}\backslashepsilon\{\$\}-precision approximation of the row. Our task is to find a set that contains all columns whose sum is at least \{$\}\backslashDelta\{\$\}, and omits any column whose sum is less than \{$\}\backslashDelta/c\{$\}. Our multi-scale sampling scheme solves this problem with cost \{$\}\backslashtilde\{\\{\}O\{\\}\}(n/\backslashDelta)\{$\}, while traditional sampling algorithms would take time \{$\}\backslashTheta((n/\backslashDelta)\{^\}2)\{$\}. Our second main technical contribution is a new local algorithm for approximating personalized PageRank, which is more robust than the earlier ones developed in $\$cite\{\{\}JehW03,AndersenCL06\{\}\} and is highly efficient particularly for networks with large in-degrees or out-degrees. Together with our multiscale sampling scheme we are able to optimally solve the \{\{\}$\$sc SignificantPageRanks\{\}\} problem.},
	author = {Christian Borgs and Michael Brautbar and Jennifer Chayes and Shang-hua Teng},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1080/15427951.2013.802752},
	issn = {1542-7951},
	issue = {August 2015},
	journal = {Internet Mathematics},
	keywords = {sampling; database},
	pages = {1-19},
	title = {Multi-Scale Matrix Sampling and Sublinear-Time PageRank Computation},
	url = {http://arxiv.org/abs/1202.2771$%5C$nhttp://www.tandfonline.com/doi/abs/10.1080/15427951.2013.802752 http://arxiv.org/abs/1202.2771%5Cnhttp://www.tandfonline.com/doi/abs/10.1080/15427951.2013.802752},
	year = {2013},
	bdsk-url-1 = {http://arxiv.org/abs/1202.2771$%5C$nhttp://www.tandfonline.com/doi/abs/10.1080/15427951.2013.802752%20http://arxiv.org/abs/1202.2771%5Cnhttp://www.tandfonline.com/doi/abs/10.1080/15427951.2013.802752},
	bdsk-url-2 = {https://doi.org/10.1080/15427951.2013.802752}}

@article{Zhang2013a,
	abstract = {Factor graphs and Gibbs sampling are a popular combination for Bayesian statistical methods that are used to solve diverse problems including insurance risk models, pricing models, and information extraction. Given a fixed sampling method and a fixed amount of time, an implementation of a sampler that achieves a higher throughput of samples will achieve a higher quality than a lower-throughput sampler. We study how (and whether) traditional data processing choices about materialization, page layout, and buffer-replacement policy need to be changed to achieve high-throughput Gibbs sampling for factor graphs that are larger than main memory. We find that both new theoretical and new algorithmic techniques are required to understand the tradeoff space for each choice. On both real and synthetic data, we demonstrate that traditional baseline approaches may achieve two orders of magnitude lower throughput than an optimal approach. For a handful of popular tasks across several storage backends, including HBase and traditional unix files, we show that our simple prototype achieves competitive (and sometimes better) throughput compared to specialized state-of-the-art approaches on factor graphs that are larger than main memory.},
	author = {Ce Zhang and Christopher R{\'e}},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2463676.2463702},
	isbn = {9781450320375},
	issn = {07308078},
	journal = {Proceedings of the 2013 international conference on Management of data - SIGMOD '13},
	keywords = {gibbs sampling,scalability,storage manager; sampling; database},
	pages = {397},
	title = {Towards high-throughput gibbs sampling at scale: a study across storage managers},
	url = {http://dl.acm.org/citation.cfm?id=2463676.2463702},
	year = {2013},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=2463676.2463702},
	bdsk-url-2 = {https://doi.org/10.1145/2463676.2463702}}

@article{Albers2013,
	abstract = {The stochastic block-model and its non-parametric extension, the Infinite Relational Model (IRM), have become key tools for discovering group-structure in complex networks. Identifying these groups is a combinatorial inference problem which is usually solved by Gibbs sampling. However, whether Gibbs sampling suffices and can be scaled to the modeling of large scale real world complex networks has not been examined sufficiently. In this paper we evaluate the performance and mixing ability of Gibbs sampling in the Infinite Relational Model (IRM) by implementing a high performance Gibbs sampler. We find that Gibbs sampling can be computationally scaled to handle millions of nodes and billions of links. Investigating the behavior of the Gibbs sampler for different sizes of networks we find that the mixing ability decreases drastically with the network size, clearly indicating a need for better sampling strategies.},
	author = {Kristoffer Jon Albers and Andreas Leon Aagard Moth and Morten M{\o}rup and Mikkel N. Schmidt and Morten Morup and Mikkel N. Schmidt and Morten M{\o}rup and Mikkel N. Schmidt},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/MLSP.2013.6661904},
	isbn = {9781479911806},
	issn = {21610363},
	journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
	keywords = {Bayesian inference,Gibbs sampling,Infinite Relational Model,Markov Chain Monte Carlo,large scale network modelling; sampling; database},
	pages = {1-6},
	title = {Large scale inference in the Infinite Relational Model: Gibbs sampling is not enough},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/MLSP.2013.6661904}}

@article{kleiner-sigkdd13,
	abstract = {As datasets become larger, more complex, and more avail- able to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the qual- ity of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality as- sessment in data analysis, this task has eluded highly auto- matic solutions. While the bootstrap provides perhaps the most promising step in this direction, its level of automa- tion is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency. Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap's outputs, determining whether or not the bootstrap is per- forming satisfactorily when applied to a given dataset and estimator. We show that our proposed diagnostic is effective via an extensive empirical evaluation on a variety of estima- tors and simulated and real datasets, including a real-world query workload from Conviva, Inc. involving 1.7TB of data (i.e., approximately 0.5 billion data points)},
	author = {Ariel Kleiner and Ameet Talwalkar and Sameer Agarwal and Ion Stoica and Michael I. Jordan},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2487575.2487650},
	isbn = {9781450321747},
	journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
	keywords = {bootstrap,diagnostic,estimator quality as-,performance; sampling; database},
	pages = {419},
	title = {A general bootstrap performance diagnostic},
	url = {http://dl.acm.org/citation.cfm?doid=2487575.2487650},
	year = {2013},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxDuLi4vTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvS2xlaW5lciBldCBhbC4gLSAyMDEzIC0gQSBHZW5lcmFsIEJvb3RzdHJhcCBQZXJmb3JtYW5jZSBEaWFnbm9zdGljLnBkZk8RA1QAAAAAA1QAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9LbGVpbmVyIGV0IGFsLiAtIDIjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEACQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAPcvOlVzZXJzOmZ5dTpMaWJyYXJ5Okdyb3VwIENvbnRhaW5lcnM6VUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZTpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4Ok9uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5OmRvYzpteV9saWJyYXJ5OnBkZjpLbGVpbmVyIGV0IGFsLiAtIDIwMTMgLSBBIEdlbmVyYWwgQm9vdHN0cmFwIFBlcmZvcm1hbmNlIERpYWdub3N0aWMucGRmAAAOAI4ARgBLAGwAZQBpAG4AZQByACAAZQB0ACAAYQBsAC4AIAAtACAAMgAwADEAMwAgAC0AIABBACAARwBlAG4AZQByAGEAbAAgAEIAbwBvAHQAcwB0AHIAYQBwACAAUABlAHIAZgBvAHIAbQBhAG4AYwBlACAARABpAGEAZwBuAG8AcwB0AGkAYwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIA9VVzZXJzL2Z5dS9MaWJyYXJ5L0dyb3VwIENvbnRhaW5lcnMvVUJGOFQzNDZHOS5PbmVEcml2ZVN0YW5kYWxvbmVTdWl0ZS9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS5ub2luZGV4L09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5L2RvYy9teV9saWJyYXJ5L3BkZi9LbGVpbmVyIGV0IGFsLiAtIDIwMTMgLSBBIEdlbmVyYWwgQm9vdHN0cmFwIFBlcmZvcm1hbmNlIERpYWdub3N0aWMucGRmAAATAAEvAAAVAAIACv//AAAACAANABoAJAEVAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABG0=},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=2487575.2487650},
	bdsk-url-2 = {https://doi.org/10.1145/2487575.2487650}}

@inproceedings{Agarwal2013,
	abstract = {In this paper, we presentBlinkDB , a massively parallel, ap-proximatequeryengineforrunninginteractiveSQLqueries on large volumes of data.BlinkDB allows users to trade-o query accuracy for response time, enabling interactive queriesovermassivedatabyrunningqueriesondatasamples andpresentingresultsannotatedwithmeaningfulerrorbars. To achieve this,BlinkDBuses two key ideas:({\~O}) an adaptive optimization framework that builds and maintains a set of multi-dimensionalstratiedsamplesfromoriginaldataover time,and({\'o}) adynamicsampleselectionstrategythatselects anappropriatelysizedsamplebasedonaquery'saccuracyor responsetimerequirements.WeevaluateBlinkDB againstthe well-known TPC-H benchmarks and a real-world analytic workload derived from Conviva Inc. , a company that man-ages video distribution over the Internet . Our experiments ona{\~O}þþnodeclustershowthatBlinkDB cananswerqueries onupto{\~O}{\ss}TBsofdatainlessthan{\'o}seconds(over{\'o}þþ× faster thanHive),withinanerrorof{\'o}-{\~O}þ{\^U}.},
	author = {S Agarwal and B Mozafari and A Panda and H Milner and S Madden and I Stoica and Query Petabytes and Blink Time},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2465351.2465355},
	isbn = {1450319947},
	journal = {Eurosys'13},
	keywords = {sampling,aqp; database},
	pages = {29-42},
	title = {BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large Data},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1145/2465351.2465355}}

@article{nirkhiwale-vldb13,
	author = {Supriya Nirkhiwale and Alin Dobra and Christopher Jermaine},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/2556549.2556563},
	issn = {21508097},
	issue = {14},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {sampling; database},
	pages = {1798-1809},
	title = {A sampling algebra for aggregate estimation},
	volume = {6},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.14778/2556549.2556563}}

@article{Hu2014,
	author = {Xiaocheng Hu and Miao Qiao and Yufei Tao},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9781450323758},
	journal = {Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
	keywords = {all or part of,independent range sampling,lower bound,or,or hard copies of,permission to make digital,range reporting,this work for personal; sampling; database},
	pages = {246-255},
	title = {Independent range sampling},
	year = {2014}}

@article{Richter2014,
	abstract = {Several research works have focused on supporting index access in MapReduce systems. These works have allowed users to significantly speed up selective MapReduce jobs by orders of magnitude. However, all these proposals require users to create indexes upfront, which might be a difficult task in certain applications (such as in scientific and social applications) where workloads are evolving or hard to predict. To overcome this problem, we propose LIAH (Lazy Indexing and Adaptivity in Hadoop), a parallel, adaptive approach for indexing at minimal costs for MapReduce systems. The main idea of LIAH is to automatically and incrementally adapt to users' workloads by creating clustered indexes on HDFS data blocks as a byproduct of executing MapReduce jobs. Besides distributing indexing efforts over multiple computing nodes, LIAH also parallelises indexing with both map tasks computation and disk I/O. All this without any additional data copy in main memory and with minimal synchronisation. The beauty of LIAH is that it piggybacks index creation on map tasks, which read relevant data from disk to main memory anyways. Hence, LIAH does not introduce any additional read I/O-costs and exploit free CPU cycles. As a result and in contrast to existing adaptive indexing works, LIAH has a very low (or invisible) indexing overhead, usually for the very first job. Still, LIAH can quickly converge to a complete index, i.e. all HDFS data blocks are indexed. Especially, LIAH can trade early job runtime improvements with fast complete index convergence. We compare LIAH with HAIL, a state-of-the-art indexing technique, as well as with standard Hadoop with respect to indexing overhead and workload performance.},
	author = {Stefan Richter and Jorge Arnulfo Quian{\'e}-Ruiz and Stefan Schuh and Jens Dittrich},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-013-0332-z},
	issn = {0949877X},
	issue = {3},
	journal = {VLDB Journal},
	keywords = {Adaptive indexing,Big data,HDFS,Hadoop,Indexing,Map reduce,Physical design; sampling; database},
	pages = {469-494},
	title = {Towards zero-overhead static and adaptive indexing in Hadoop},
	volume = {23},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00778-013-0332-z}}

@article{Alexandrov2014,
	abstract = {We present Stratosphere, an open-source soft- ware stack for parallel data analysis. Stratosphere brings together a unique set of features that allow the expressive, easy, and efficient programming of analytical applications at very large scale. Stratosphere's features include ``in situ'' data processing, a declarative query language, treatment of user-defined functions as first-class citizens, automatic program parallelization and optimization, support for iterative programs, and a scalable and efficient execution engine. Stratosphere covers a variety of ``Big Data'' use cases, such as data warehousing, information extraction and integration, data cleansing, graph analysis, and statistical analysis applications. In this paper,we present the overall system architecture design decisions, introduce Stratosphere through example queries, and then dive into the internal workings of the system's components that relate to extensibility, programming model, optimization, and query execution.We experi- mentally compare Stratosphere against popular open-source alternatives, and we conclude with a research outlook for the next years.},
	author = {Alexander Alexandrov and Rico Bergmann and Stephan Ewen and Johann-Christoph Freytag and Fabian Hueske and Arvid Heise and Odej Kao and Marcus Leich and Ulf Leser and Volker Markl and Felix Naumann and Mathias Peters and Astrid Rheinl{\"a}nder and Matthias J Sax and Sebastian Schelter and Mareike H{\"o}ger and Kostas Tzoumas and Daniel Warneke},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1007/s00778-014-0357-y},
	issn = {1066-8888},
	journal = {The VLDB Journal},
	keywords = {Big data,Data cleansing,Distributed systems,Graph processing,Parallel databases,Query Optimization,Query processing,Text mining; sampling; database},
	pages = {939-964},
	title = {The Stratosphere platform for big data analytics},
	url = {http://link.springer.com/10.1007/s00778-014-0357-y$%5C$nhttp://www.dbis.informatik.hu-berlin.de/fileadmin/research/papers/journals/2014-VLDBJ%7B_%7DStratosphere%7B_%7DOverview.pdf},
	year = {2014},
	bdsk-url-1 = {http://link.springer.com/10.1007/s00778-014-0357-y$%5C$nhttp://www.dbis.informatik.hu-berlin.de/fileadmin/research/papers/journals/2014-VLDBJ%7B_%7DStratosphere%7B_%7DOverview.pdf},
	bdsk-url-2 = {https://doi.org/10.1007/s00778-014-0357-y}}

@article{zeng2014-abs,
	author = {Kai Zeng},
	date-modified = {2022-01-28 21:59:48 -0500},
	isbn = {9781450323765},
	journal = {Sigmod},
	keywords = {approximate query processing,bootstrap,error estimation; sampling; database; aqp},
	pages = {1067-1070},
	title = {{ABS}: A System for Scalable Approximate Queries with Accuracy Guarantees},
	year = {2014},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxEBAC4uL0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL1plbmcgLSAyMDE0IC0gQUJTIGEgU3lzdGVtIGZvciBTY2FsYWJsZSBBcHByb3hpbWF0ZSBRdWVyaWVzIHdpdGggQWNjdXJhY3kgR3VhcmFudGVlcy5wZGZPEQOcAAAAAAOcAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fWmVuZyAtIDIwMTQgLSBBQlMgI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAkAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgEJLzpVc2VyczpmeXU6TGlicmFyeTpHcm91cCBDb250YWluZXJzOlVCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGU6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleDpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eTpkb2M6bXlfbGlicmFyeTpwZGY6WmVuZyAtIDIwMTQgLSBBQlMgYSBTeXN0ZW0gZm9yIFNjYWxhYmxlIEFwcHJveGltYXRlIFF1ZXJpZXMgd2l0aCBBY2N1cmFjeSBHdWFyYW50ZWVzLnBkZgAADgCyAFgAWgBlAG4AZwAgAC0AIAAyADAAMQA0ACAALQAgAEEAQgBTACAAYQAgAFMAeQBzAHQAZQBtACAAZgBvAHIAIABTAGMAYQBsAGEAYgBsAGUAIABBAHAAcAByAG8AeABpAG0AYQB0AGUAIABRAHUAZQByAGkAZQBzACAAdwBpAHQAaAAgAEEAYwBjAHUAcgBhAGMAeQAgAEcAdQBhAHIAYQBuAHQAZQBlAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAQdVc2Vycy9meXUvTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvWmVuZyAtIDIwMTQgLSBBQlMgYSBTeXN0ZW0gZm9yIFNjYWxhYmxlIEFwcHJveGltYXRlIFF1ZXJpZXMgd2l0aCBBY2N1cmFjeSBHdWFyYW50ZWVzLnBkZgAAEwABLwAAFQACAAr//wAAAAgADQAaACQBKAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATI}}

@inproceedings{Levin2014,
	author = {Roy Levin and Cornell Tech},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {9781450323765},
	journal = {Sigmod},
	keywords = {mapreduce,social networks,stratified sampling; sampling; database},
	pages = {863-874},
	title = {Stratified-Sampling over Social Networks Using MapReduce},
	year = {2014}}

@article{Doulkeridis2014a,
	abstract = {Enterprises today acquire vast volumes of data from different sources and leverage this information by means of data analysis to support effective decision-making and provide new functionality and services. The key requirement of data analytics is scalability, simply due to the immense volume of data that need to be extracted, processed, and analyzed in a timely fashion. Arguably the most popular framework for contemporary large-scale data analytics is MapReduce, mainly due to its salient features that include scalability, fault-tolerance, ease of programming, and flexibility. However, despite its merits, MapReduce has evident performance limitations in miscellaneous analytical tasks, and this has given rise to a significant body of research that aim at improving its efficiency, while maintaining its desirable properties. This survey aims to review the state of the art in improving the performance of parallel query processing using MapReduce. A set of the most significant weaknesses and limitations of MapReduce is discussed at a high level, along with solving techniques. A taxonomy is presented for categorizing existing research on MapReduce improvements according to the specific problem they target. Based on the proposed taxonomy, a classification of existing research is provided focusing on the optimization objective. Concluding, we outline interesting directions for future parallel data processing systems.},
	author = {Christos Doulkeridis and Kjetil N{\o}rv{\aa}g},
	date-modified = {2022-01-14 09:59:46 -0500},
	doi = {10.1007/s00778-013-0319-9},
	isbn = {1066-8888},
	issn = {1066-8888},
	issue = {3},
	journal = {The VLDB Journal},
	keywords = {database, big data, sampling},
	month = {6},
	note = {<b>From Duplicate 1 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/>\{%\}Tyler 06/14/2016<br/><br/>published: 2014<br/><br/>MapReduce has evident performance limitations in miscellaneous analytical tasks and this has given rise to a significant body of research that aim at improving its efficiency. <br/><br/>*In this paper a set of the most significent weaknesses and limits of MapReduce is discussed at a high level along with solving techniques. <br/><br/>Weaknesses and techniques on page 5<br/><br/>Page 17 introduces in-memory processing using a column-oriented datastore! This might be very useful for us. The program is called &quot; Power Drill &quot;<br/><br/><b>From Duplicate 2 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/><b>From Duplicate 1 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/><b>From Duplicate 1 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/>\{%\}Tyler 06/14/2016<br/><br/>published: 2014<br/><br/>MapReduce has evident performance limitations in miscellaneous analytical tasks and this has given rise to a significant body of research that aim at improving its efficiency. <br/><br/>*In this paper a set of the most significent weaknesses and limits of MapReduce is discussed at a high level along with solving techniques. <br/><br/>Weaknesses and techniques on page 5<br/><br/>Page 17 introduces in-memory processing using a column-oriented datastore! This might be very useful for us. The program is called &quot; Power Drill &quot;<br/><br/><b>From Duplicate 3 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/>And Duplicate 4 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/>And Duplicate 5 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/>%Tyler 06/14/2016<br/><br/>published: 2014<br/><br/>MapReduce has evident performance limitations in miscellaneous analytical tasks and this has given rise to a significant body of research that aim at improving its efficiency. <br/><br/>*In this paper a set of the most significent weaknesses and limits of MapReduce is discussed at a high level along with solving techniques. <br/><br/><b>Weaknesses and techniques on page 5<br/><br/>Page 17 introduces in-memory processing using a column-oriented datastore! This might be very useful for us. The program is called &quot; Power Drill &quot;</b><br/><br/><b>From Duplicate 3 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/><b>From Duplicate 1 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/>\{%\}Tyler 06/14/2016<br/><br/>published: 2014<br/><br/>MapReduce has evident performance limitations in miscellaneous analytical tasks and this has given rise to a significant body of research that aim at improving its efficiency. <br/><br/>*In this paper a set of the most significent weaknesses and limits of MapReduce is discussed at a high level along with solving techniques. <br/><br/>Weaknesses and techniques on page 5<br/><br/>Page 17 introduces in-memory processing using a column-oriented datastore! This might be very useful for us. The program is called &quot; Power Drill &quot;<br/><br/><b>From Duplicate 3 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/>And Duplicate 4 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/>And Duplicate 5 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/>%Tyler 06/14/2016<br/><br/>published: 2014<br/><br/>MapReduce has evident performance limitations in miscellaneous analytical tasks and this has given rise to a significant body of research that aim at improving its efficiency. <br/><br/>*In this paper a set of the most significent weaknesses and limits of MapReduce is discussed at a high level along with solving techniques. <br/><br/><b>Weaknesses and techniques on page 5<br/><br/>Page 17 introduces in-memory processing using a column-oriented datastore! This might be very useful for us. The program is called &quot; Power Drill &quot;</b><br/><br/><b>From Duplicate 4 (<i>A survey of large-scale analytical query processing in MapReduce</i> - Doulkeridis, Christos; N{\o}rv{\aa}g, Kjetil)<br/></b><br/>%Tyler 06/14/2016<br/><br/>published: 2014<br/><br/>MapReduce has evident performance limitations in miscellaneous analytical tasks and this has given rise to a significant body of research that aim at improving its efficiency. <br/><br/>*In this paper a set of the most significent weaknesses and limits of MapReduce is discussed at a high level along with solving techniques. <br/><br/><b>Weaknesses and techniques on page 5<br/><br/>Page 17 introduces in-memory processing using a column-oriented datastore! This might be very useful for us. The program is called &quot; Power Drill &quot;</b>},
	pages = {355-380},
	title = {A survey of large-scale analytical query processing in MapReduce},
	url = {http://link.springer.com/10.1007/s00778-013-0319-9},
	volume = {23},
	year = {2014},
	bdsk-url-1 = {http://link.springer.com/10.1007/s00778-013-0319-9},
	bdsk-url-2 = {https://doi.org/10.1007/s00778-013-0319-9}}

@article{Samplers2014,
	author = {Gibbs Samplers and Colin Fox and Albert Parker},
	date-modified = {2022-01-10 13:29:09 -0500},
	issue = {1},
	journal = {SIAM Journal on Scientific Computing},
	pages = {124-147},
	title = {Convergence in Variance of Chebyshev Accelerated Gibbs Samplers},
	volume = {36},
	year = {2014}}

@phdthesis{zeng2014approximation,
	author = {Kai Zeng},
	date-modified = {2022-01-14 10:07:54 -0500},
	institution = {UCLA},
	keywords = {sampling; bootstrap; database},
	school = {University of California, Los Angeles},
	title = {Approximation and Search Optimization on Massive Data Bases and Data Streams},
	year = {2014},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxEBBi4uL0xpYnJhcnkvR3JvdXAgQ29udGFpbmVycy9VQkY4VDM0Nkc5Lk9uZURyaXZlU3RhbmRhbG9uZVN1aXRlL09uZURyaXZlIC0gWW91bmdzdG93biBTdGF0ZSBVbml2ZXJzaXR5Lm5vaW5kZXgvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkvZG9jL215X2xpYnJhcnkvcGRmL1plbmcgLSAyMDE0IC0gQXBwcm94aW1hdGlvbiBhbmQgU2VhcmNoIE9wdGltaXphdGlvbiBvbiBNYXNzaXZlIERhdGEgQmFzZXMgYW5kIERhdGEgU3RyZWFtcy5wZGZPEQO0AAAAAAO0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fWmVuZyAtIDIwMTQgLSBBcHByI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAkAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgEPLzpVc2VyczpmeXU6TGlicmFyeTpHcm91cCBDb250YWluZXJzOlVCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGU6T25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleDpPbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eTpkb2M6bXlfbGlicmFyeTpwZGY6WmVuZyAtIDIwMTQgLSBBcHByb3hpbWF0aW9uIGFuZCBTZWFyY2ggT3B0aW1pemF0aW9uIG9uIE1hc3NpdmUgRGF0YSBCYXNlcyBhbmQgRGF0YSBTdHJlYW1zLnBkZgAADgC+AF4AWgBlAG4AZwAgAC0AIAAyADAAMQA0ACAALQAgAEEAcABwAHIAbwB4AGkAbQBhAHQAaQBvAG4AIABhAG4AZAAgAFMAZQBhAHIAYwBoACAATwBwAHQAaQBtAGkAegBhAHQAaQBvAG4AIABvAG4AIABNAGEAcwBzAGkAdgBlACAARABhAHQAYQAgAEIAYQBzAGUAcwAgAGEAbgBkACAARABhAHQAYQAgAFMAdAByAGUAYQBtAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAQ1Vc2Vycy9meXUvTGlicmFyeS9Hcm91cCBDb250YWluZXJzL1VCRjhUMzQ2RzkuT25lRHJpdmVTdGFuZGFsb25lU3VpdGUvT25lRHJpdmUgLSBZb3VuZ3N0b3duIFN0YXRlIFVuaXZlcnNpdHkubm9pbmRleC9PbmVEcml2ZSAtIFlvdW5nc3Rvd24gU3RhdGUgVW5pdmVyc2l0eS9kb2MvbXlfbGlicmFyeS9wZGYvWmVuZyAtIDIwMTQgLSBBcHByb3hpbWF0aW9uIGFuZCBTZWFyY2ggT3B0aW1pemF0aW9uIG9uIE1hc3NpdmUgRGF0YSBCYXNlcyBhbmQgRGF0YSBTdHJlYW1zLnBkZgAAEwABLwAAFQACAAr//wAAAAgADQAaACQBLgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATm}}

@inproceedings{fan-sigmod14,
	author = {Wenfei Fan and Xin Wang and Yinghui Wu},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. SIGMOD'14},
	keywords = {bounded resource,graph querying,pattern matching; sampling; database},
	pages = {301-312},
	publisher = {ACM},
	title = {Querying Big Graphs Within Bounded Resources},
	year = {2014}}

@article{cohen2014uniform,
	abstract = {Random sampling has become a critical tool in solving massive matrix problems. For linear regression, a small, manageable set of data rows can be randomly selected to approximate a tall, skinny data matrix, improving processing time significantly. For theoretical performance guarantees, each row must be sampled with probability proportional to its statistical leverage score. Unfortunately, leverage scores are difficult to compute. A simple alternative is to sample rows uniformly at random. While this often works, uniform sampling will eliminate critical row information for many natural instances. We take a fresh look at uniform sampling by examining what information it does preserve. Specifically, we show that uniform sampling yields a matrix that, in some sense, well approximates a large fraction of the original. While this weak form of approximation is not enough for solving linear regression directly, it is enough to compute a better approximation. This observation leads to simple iterative row sampling algorithms for matrix approximation that run in input-sparsity time and preserve row structure and sparsity at all intermediate steps. In addition to an improved understanding of uniform sampling, our main proof introduces a structural result of independent interest: we show that every matrix can be made to have low coherence by reweighting a small subset of its rows.},
	author = {Michael B. Cohen and Yin Tat Lee and Christopher Cameron Musco and Christopher Cameron Musco and Richard Peng and Aaron Sidford},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2688073.2688113},
	isbn = {9781450333337},
	journal = {ITCS 2015 - Proceedings of the 6th Innovations in Theoretical Computer Science},
	keywords = {Leverage scores,Matrix sampling,Randomized numerical linear algebra,Regression; sampling; database},
	pages = {181-190},
	title = {Uniform Sampling for Matrix Approximation},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1145/2688073.2688113}}

@inproceedings{zeng-sigmod14,
	abstract = {UCLA CDS Technical Report 130028 with full proofs and tpc-h queries},
	author = {Kai Zeng and Shi Gao and Barzan Mozafari and Carlo Zaniolo},
	booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
	date-modified = {2022-01-14 10:14:33 -0500},
	doi = {10.1145/2588555.2588579},
	keywords = {bootstrap; sampling; aqp; database},
	pages = {277--288},
	title = {The Analytical Bootstrap: a New Method for Fast Error Estimation in Approximate Query Processing},
	url = {https://doi.org/10.1145/2588555.2588579},
	year = {2014},
	bdsk-file-1 = {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},
	bdsk-url-1 = {http://web.eecs.umich.edu/~mozafari/papers/sigmod_2014_abm.pdf%20http://dl.acm.org/citation.cfm?id=2588555.2588579},
	bdsk-url-2 = {https://doi.org/10.1145/2588555.2588579}}

@inproceedings{Sch,
	author = {Marc Sch and Johannes Schildgen and Stefan De{\ss}loch},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Datenbanksysteme f{\"u}r Business, Technologie und Web (BTW)},
	keywords = {sampling; database},
	title = {Sampling with Incremental MapReduce},
	year = {2015}}

@article{Leis2015-JOB,
	abstract = {Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and exper-imentally revisit the main components in the classic query opti-mizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using an-other set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumera-tion techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.},
	author = {Viktor Leis and Andrey Gubichev and Atanas Mirchev and Peter Boncz and Alfons Kemper and Thomas Neumann},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/2850583.2850594},
	issn = {2150-8097},
	issue = {3},
	journal = {Proc VLDB'15},
	keywords = {sampling; database},
	month = {11},
	pages = {204-215},
	publisher = {VLDB Endowment},
	title = {How good are query optimizers, really?},
	url = {http://dx.doi.org/10.14778/2850583.2850594 http://dl.acm.org/citation.cfm?doid=2850583.2850594},
	volume = {9},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.14778/2850583.2850594%20http://dl.acm.org/citation.cfm?doid=2850583.2850594},
	bdsk-url-2 = {http://dx.doi.org/10.14778/2850583.2850594}}

@article{Shin2015,
	author = {Jaeho Shin and Sen Wu and Feiran Wang and Christopher De Sa and Ce Zhang and Christopher R{\'e}},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {11},
	keywords = {PVLDB8,VLDB15; sampling; database},
	pages = {1310-1321},
	title = {Incremental \{Knowledge\} \{Base\} \{Construction\} \{Using\} \{DeepDive\}},
	url = {http://www.vldb.org/pvldb/vol8/p1310-shin.pdf},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {http://www.vldb.org/pvldb/vol8/p1310-shin.pdf}}

@article{Chen2015,
	abstract = {Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms.},
	author = {Changyou Chen and David Carlson and Zhe Gan and Chunyuan Li and Lawrence Carin},
	date-modified = {2022-01-10 13:26:52 -0500},
	isbn = {1512.07962},
	keywords = {sampling; database},
	title = {Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization},
	url = {http://arxiv.org/abs/1512.07962},
	volume = {41},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1512.07962}}

@article{vengerov2015join,
	author = {David Vengerov and Winterberry Way and San Jose and Andre Cavalheiro Menck and Mohamed Zait and Sunil P Chakkappen},
	city = {Kohala Coast, Hawaii, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/2824032.2824051},
	issn = {21508097},
	issue = {12},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {sampling; aqp; database},
	pages = {1530-1541},
	publisher = {VLDB Endowment},
	title = {Join size estimation subject to filter conditions},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.14778/2824032.2824051}}

@article{Chung2016,
	author = {Yung-Yu Chung and Srikanta Tirthapura and David P. Woodruff},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1109/TKDE.2016.2518679},
	isbn = {9788578110796},
	issn = {1041-4347},
	issue = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Algorithm design and analysis,Complexity theory,Distributed databases,Distributed stream,Electronic mail,Protocols,Reservoirs,Silicon,distributed stream,random sampling,reservoir sampling,skew; sampling; database},
	pages = {1356-1368},
	pmid = {25246403},
	title = {A Simple Message-Optimal Algorithm for Random Sampling from a Distributed Stream},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7384501},
	volume = {28},
	year = {2016},
	bdsk-url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7384501},
	bdsk-url-2 = {https://doi.org/10.1109/TKDE.2016.2518679}}

@inproceedings{Wu2016,
	abstract = {Despite of decades of work, query optimizers still make mistakes on "difficult" queries because of bad cardinality estimates, often due to the interaction of multiple predicates and correlations in the data. In this paper, we propose a low-cost post-processing step that can take a plan produced by the optimizer, detect when it is likely to have made such a mistake, and take steps to fix it. Specifically, our solution is a sampling-based iterative procedure that requires almost no changes to the original query optimizer or query evaluation mechanism of the system. We show that this indeed imposes low overhead and catches cases where three widely used optimizers (PostgreSQL and two commercial systems) make large errors.},
	author = {Wentao Wu and Jeffrey F Naughton and Harneet Singh},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. SIGMOD'16},
	keywords = {sampling; aqp; database},
	pages = {1721-1736},
	title = {Sampling-Based Query Re-Optimization},
	year = {2016}}

@inproceedings{Ting:2016:TOC:2939672.2939772,
	author = {Daniel Ting},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2939672.2939772},
	isbn = {978-1-4503-4232-2},
	journal = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	keywords = {cardinality estimation,data sketching,randomized algorithms; sampling; database},
	pages = {1195-1204},
	publisher = {ACM},
	title = {Towards Optimal Cardinality Estimation of Unions and Intersections with Sketches},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2939672.2939772}}

@article{chung16-tods,
	abstract = {It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) is the integrated data set complete and (2) what is the impact of any unknown (i.e., unobserved) data on query results? In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.},
	author = {Yeounoh Chung and Michael Lind Mortensen and Carsten Binnig and Tim Kraska},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/2882903.2882909},
	institution = {ACM},
	isbn = {9781450335317},
	issn = {07308078},
	issue = {1},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	keywords = {sampling; aqp; database},
	pages = {861-876},
	title = {Estimating the impact of unknown unknowns on aggregate query results},
	volume = {26-June-20},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2882903.2882909}}

@inproceedings{Leis2017,
	abstract = {After four decades of research, today's database systems still suf-fer from poor query execution plans. Bad plans are usually caused by poor cardinality estimates, which have been called the " Achilles Heel " of modern query optimizers. In this work we propose index-based join sampling, a novel cardinality estimation technique for main-memory databases that relies on sampling and existing in-dex structures to obtain accurate estimates. Results on a real-world data set show that this approach significantly improves estimation as well as overall plan quality. The additional sampling effort is quite low and can be configured to match the desired application profile. The technique can be easily integrated into most systems.},
	author = {Viktor Leis and Bernhard Radke and Andrey Gubichev and Alfons Kemper and Thomas Neumann},
	city = {Chaminade, California, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Proc. CIDR'17},
	keywords = {sampling; database},
	title = {Cardinality Estimation Done Right : Index-Based Join Sampling},
	year = {2017}}

@article{Chen2017,
	abstract = {We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and the John walk, for generating samples from the uniform distribution over a polytope. Both random walks are sampling algorithms derived from interior point methods. The former is based on volumetric-logarithmic barrier introduced by Vaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk mixes in significantly fewer steps than the logarithmic-barrier based Dikin walk studied in past work. For a polytope in $\mathbb\{R\}^d$ defined by $n >d$ linear constraints, we show that the mixing time from a warm start is bounded as $\mathcal\{O\}(n^\{0.5\}d^\{1.5\})$, compared to the $\mathcal\{O\}(nd)$ mixing time bound for the Dikin walk. The cost of each step of the Vaidya walk is of the same order as the Dikin walk, and at most twice as large in terms of constant pre-factors. For the John walk, we prove an $\mathcal\{O\}(d^\{2.5\}\cdot\log^4(n/d))$ bound on its mixing time and conjecture that an improved variant of it could achieve a mixing time of $\mathcal\{O\}(d^2\cdot\text\{polylog\}(n/d))$. Additionally, we propose variants of the Vaidya and John walks that mix in polynomial time from a deterministic starting point. We illustrate the speed-up of the Vaidya walk over the Dikin walk via several numerical examples.},
	author = {Yuansi Chen and Raaz Dwivedi and Martin J. Wainwright and Bin Yu},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {sampling; database},
	title = {Fast MCMC sampling algorithms on polytopes},
	url = {http://arxiv.org/abs/1710.08165},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1710.08165}}

@article{Ahn2017,
	abstract = {Molecular dynamics simulations are useful in obtaining thermodynamic and kinetic properties of bio-molecules, but they are limited by the time scale barrier. That is, we may not obtain properties' efficiently because we need to run microseconds or longer simulations using femtosecond time steps. To overcome this time scale barrier, we can use the weighted ensemble (WE) method, a powerful enhanced sampling method that efficiently samples thermodynamic and kinetic properties. However, the WE method requires an appropriate partitioning of phase space into discrete macrostates, which can be problematic when we have a high-dimensional collective space or when little is known a priori about the molecular system. Hence, we developed a new WE-based method, called the "Concurrent Adaptive Sampling (CAS) algorithm," to tackle these issues. The CAS algorithm is not constrained to use only one or two collective variables, unlike most reaction coordinate-dependent methods. Instead, it can use a large number of collective variables and adaptive macrostates to enhance the sampling in the high-dimensional space. This is especially useful for systems in which we do not know what the right reaction coordinates are, in which case we can use many collective variables to sample conformations and pathways. In addition, a clustering technique based on the committor function is used to accelerate sampling the slowest process in the molecular system. In this paper, we introduce the new method and show results from two-dimensional models and bio-molecules, specifically penta-alanine and a triazine trimer.},
	author = {Surl Hee Ahn and Jay W. Grate and Eric F. Darve},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1063/1.4999097},
	issn = {00219606},
	issue = {7},
	journal = {Journal of Chemical Physics},
	keywords = {chemistry,sampling,simulation; database},
	pages = {1-33},
	pmid = {28830168},
	title = {Efficiently sampling conformations and pathways using the concurrent adaptive sampling (CAS) algorithm},
	volume = {147},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1063/1.4999097}}

@inproceedings{ChenY2017-sigmod,
	abstract = {Join size estimation is a critical step in query optimization, and has been extensively studied in the literature. Among the many techniques, sampling based approaches are partic-ularly appealing, due to their ability to handle arbitrary se-lection predicates. In this paper, we propose a new sampling algorithm for join size estimation, called two-level sampling, which combines the advantages of three previous sampling methods while making further improvements. Both ana-lytical and empirical comparisons show that the new algo-rithm outperforms all the previous algorithms on a variety of joins, including primary key-foreign key joins, many-to-many joins, and multi-table joins. The new sampling algo-rithm is also very easy to implement, requiring just one pass over the data. It only relies on some basic statistical infor-mation about the data, such as the k -norms and the heavy hitters.},
	author = {Yu Chen and Ke Yi},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/3035918.3035921},
	isbn = {978-1-4503-4197-4},
	issue = {2},
	journal = {Proc. ICDE'17},
	keywords = {Joins,joins,sampling; aqp; database},
	pages = {759-774},
	publisher = {ACM},
	title = {Two-Level Sampling for Join Size Estimation},
	url = {http://doi.acm.org/10.1145/3035918.3035921},
	year = {2017},
	bdsk-url-1 = {http://doi.acm.org/10.1145/3035918.3035921},
	bdsk-url-2 = {https://doi.org/10.1145/3035918.3035921}}

@article{boicea2018sampling,
	abstract = {Getting information from large volumes of data is very expensive in terms of resources like CPU and memory, as well as computation time. The analysis of a small data set extracted from the original set is preferred. From this small set, called sample, approximate results can be obtained. The errors are acceptable given the reduced cost necessary for processing the data. Using sampling algorithms with small errors saves execution time and resources. This paper presents comparisons between sampling algorithms in order to determine which one performs better when taking into account set operations such as intersect, union and difference. The comparison focuses on the errors introduced by each algorithm for different sample sizes and on execution times.},
	author = {Alexandru Boicea and Ciprian-Octavian Truicua and Florin Ruadulescu and Elena-Cristina Bucse},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {Data & Knowledge Engineering},
	keywords = {sampling; database},
	pages = {1-15},
	publisher = {Elsevier},
	title = {Sampling strategies for extracting information from large data sets},
	url = {https://www.evernote.com/shard/s13/nl/1480559/46437b41-1f9b-47a2-8778-fd31f9705f11/},
	volume = {115},
	year = {2018},
	bdsk-url-1 = {https://www.evernote.com/shard/s13/nl/1480559/46437b41-1f9b-47a2-8778-fd31f9705f11/}}

@article{2018arXiv181201823H,
	abstract = {We introduce a sampling framework to support approximate computing with estimated error bounds in Spark. Our framework allows sampling to be performed at the beginning of a sequence of multiple transformations ending in an aggregation operation. The framework constructs a data provenance graph as the computation proceeds, then combines the graph with multi-stage sampling and population estimation theories to compute error bounds for the aggregation. When information about output keys are available early, the framework can also use adaptive stratified reservoir sampling to avoid (or reduce) key losses in the final output and to achieve more consistent error bounds across popular and rare keys. Finally, the framework includes an algorithm to dynamically choose sampling rates to meet user specified constraints on the CDF of error bounds in the outputs. We have implemented a prototype of our framework called ApproxSpark, and used it to implement five approximate applications from different domains. Evaluation results show that ApproxSpark can (a) significantly reduce execution time if users can tolerate small amounts of uncertainties and, in many cases, loss of rare keys, and (b) automatically find sampling rates to meet user specified constraints on error bounds. We also explore and discuss extensively trade-offs between sampling rates, execution time, accuracy and key loss.},
	author = {Guangyan Hu and Desheng Zhang and Sandro Rigo and Thu D. Nguyen},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.7282/T3CN77JS},
	journal = {arXiv e-prints},
	keywords = {Computer Science - Databases,Computer Science - Distributed,Parallel,and Cluster Computing; sampling; database},
	month = {12},
	pages = {arXiv:1812.01823},
	title = {Approximation with Error Bounds in Spark},
	url = {http://arxiv.org/abs/1812.01823%0Ahttp://dx.doi.org/10.7282/T3CN77JS},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1812.01823%0Ahttp://dx.doi.org/10.7282/T3CN77JS},
	bdsk-url-2 = {https://doi.org/10.7282/T3CN77JS}}

@phdthesis{Nirkhiwale2018,
	author = {Supriya Nirkhiwale},
	date-modified = {2022-01-10 13:26:52 -0500},
	institution = {University of Florida},
	keywords = {sampling; database},
	title = {A Sampling Algebra for Scalable Approximate Query Processing},
	year = {2018}}

@inproceedings{kipf2018learned,
	author = {Andreas Kipf and Thomas Kipf and Bernhard Radke and Viktor Leis and Peter Boncz and Alfons Kemper},
	date-modified = {2022-01-10 13:26:52 -0500},
	keywords = {sampling; database},
	title = {Learned Cardinalities: Estimating Correlated Joins with Deep Learning},
	year = {2018}}

@inproceedings{Stefanoni:2018:ECC:3178876.3186003,
	abstract = {Estimating the cardinality (i.e., the number of answers) of conjunctive queries is particularly difficult in RDF systems: queries over RDF data are navigational and thus tend to involve many joins. We present a new, principled cardinality estimation technique based on graph summarisation. We interpret a summary of an RDF graph using a possible world semantics and formalise the estimation problem as computing the expected cardinality over all RDF graphs represented by the summary, and we present a closed-form formula for computing the expectation of arbitrary queries. We also discuss approaches to RDF graph summarisation. Finally, we show empirically that our cardinality technique is more accurate and more consistent, often by orders of magnitude, than the state of the art.},
	author = {Giorgio Stefanoni and Boris Motik and Egor V. Kostylev},
	city = {Republic and Canton of Geneva, Switzerland},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/3178876.3186003},
	isbn = {978-1-4503-5639-8},
	journal = {Proceedings of the 2018 World Wide Web Conference},
	keywords = {RDF,databases,graph summarisation,query cardinality estimation; sampling; database},
	pages = {1043-1052},
	publisher = {International World Wide Web Conferences Steering Committee},
	title = {Estimating the Cardinality of Conjunctive Queries over RDF Data Using Graph Summarisation},
	url = {http://arxiv.org/abs/1801.09619%0Ahttp://dx.doi.org/10.1145/3178876.3186003 https://doi.org/10.1145/3178876.3186003},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1801.09619%0Ahttp://dx.doi.org/10.1145/3178876.3186003%20https://doi.org/10.1145/3178876.3186003},
	bdsk-url-2 = {https://doi.org/10.1145/3178876.3186003}}

@article{li-tods2019,
	author = {Feifei Li and Bin Wu and Ke Yi and Zhuoyue Zhao},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	issue = {1},
	journal = {ACM Trans. Database Syst.},
	keywords = {Join,online aggregation,random walk; sampling; aqp; database},
	month = {1},
	note = {<br/>},
	pages = {2:1--2:41},
	publisher = {ACM},
	title = {Wander Join and XDB: Online Aggregation via Random Walks},
	volume = {44},
	year = {2019}}

@inproceedings{kipf19-deep,
	author = {Andreas Kipf and Dimitri Vorona and Jonas M{\"u}ller and Thomas Kipf and Bernhard Radke and Viktor Leis and Peter Boncz and Thomas Neumann and Alfons Kemper},
	city = {New York, NY, USA},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.1145/3299869.3320218},
	isbn = {978-1-4503-5643-5},
	journal = {arXiv e-prints},
	keywords = {Computer Science - Databases,cardinality estimation,ml for databases; sampling; database},
	month = {4},
	pages = {arXiv:1904.08223},
	publisher = {ACM},
	title = {Estimating Cardinalities with Deep Sketches},
	url = {http://doi.acm.org/10.1145/3299869.3320218},
	year = {2019},
	bdsk-url-1 = {http://doi.acm.org/10.1145/3299869.3320218},
	bdsk-url-2 = {https://doi.org/10.1145/3299869.3320218}}

@article{Yang2019,
	author = {Zongheng Yang and Eric Liang and Amog Kamsetty and Chenggang Wu and Yan Duan and Xi Chen and Pieter Abbeel and Joseph M. Hellerstein and Sanjay Krishnan and Ion Stoica},
	date-modified = {2022-01-10 13:26:52 -0500},
	journal = {CoRR},
	keywords = {sampling; database},
	title = {Selectivity Estimation with Deep Likelihood Models},
	url = {http://arxiv.org/abs/1905.04278},
	volume = {abs/1905.0},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1905.04278}}

@article{Huang2019,
	abstract = {Despite decades of research on AQP (approximate query processing), our understanding of sample-based joins has remained limited and, to some extent, even superficial. The common belief in the community is that joining random samples is futile. This belief is largely based on an early result showing that the join of two uniform samples is not an independent sample of the original join, and that it leads to quadratically fewer output tuples. Unfortunately, this early result has little applicability to the key questions practitioners face. For example, the success metric is often the final approximation's accuracy, rather than output cardinality. Moreover, there are many non-uniform sampling strategies that one can employ. Is sampling for joins still futile in all of these settings? If not, what is the best sampling strategy in each case? To the best of our knowledge, there is no formal study answering these questions. This paper aims to improve our understanding of sample-based joins and offer a guideline for practitioners building and using realworld AQP systems. We study limitations of offline samples in approximating join queries: given an offline sampling budget, how well can one approximate the join of two tables? We answer this question for two success metrics: output size and estimator variance. We show that maximizing output size is easy, while there is an information-theoretical lower bound on the lowest variance achievable by any sampling strategy. We then define a hybrid sampling scheme that captures all combinations of stratified, universe, and Bernoulli sampling, and show that this scheme with our optimal parameters achieves the theoretical lower bound within a constant factor. Since computing these optimal parameters requires shuffling statistics across the network, we also propose a decentralized variant in which each node acts autonomously using minimal statistics. We also empirically validate our findings on popular SQL and AQP engines.},
	author = {Dawei Huang and Dong Young Yoon and Seth Pettie and Barzan Mozafari},
	date-modified = {2022-01-10 13:26:52 -0500},
	doi = {10.14778/3372716.3372726},
	issn = {21508097},
	issue = {4},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {sampling; database},
	pages = {547-560},
	title = {Joins on samples: A theoretical guide for practitioners},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.14778/3372716.3372726}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>!read:sigmod21</string>
		<key>keys</key>
		<string>Dai_2021,shi2021time</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>db</string>
		<key>keys</key>
		<string>fdblp,tpch-skew,tpch,Conway,Unknowna,Unknown,Peng,dblp,Bre,jbirch,Kaczmarski,Bhargava,hoe63,smit75,wong76,coch77,Comer1979,kooi80,ross80,Bernstein1981,Yu1984,piat84,chri84,Jarke1984,vitt85,Lehman:1986,Ioannidis1987a,valduriez1987,mura88,Swama1988,kersten89,swam89,ullman1988principles,57061,ioan90,lipton90-practical,ioan91,haas92,Mishra1992,haas93,Graefe1993a,Steinbrunn1993a,250116,Graefe1995,zhan96,Ioannidis1996,alon96,o1996log,poos97,Steinbrunn1997,gibb97,Karger1997,Evrendilek1997,Torp1998,mati98,Chaudhuri1998,simkovics1998enhancement,Alon1999-pods,Rao1999,lava95,lee99,li1999fast,mati00,rao00,LiJZ01-codas,wu01,desh01,Lane2003,rama03,mark04,Bender-siam2005,Bender2005,corm05,pol-sigmod05,Bender2006,spie06,Jermaine2006,Araujo2006a,estan2006end-biased,chau07,Stonebraker2007,Bender2007,rashid07,lava07,fang2007,Aguilera2008,garc08,Nickolls2008,He2008,Ilyas2008,Agrawal:2009:CRD:1516046.1516062,He2009,kim2009,MendezMediavilla2010,Taylor2010,beav10,Sun2010,Bakkum2010a,Sewall2011,Tudorica2011,Pokorny2011,Bakkum2011,Blanas2011,Stonebraker2011,Catania2011,Nevarez2011,Han2011,Wu2011b,Cattell2011a,Unknown2011,Jin2011,Braginsky2012,Liu2012,Kaldewey2012,Jin2012,Han2012,Aiyer2012,Jin2012a,Babu2012b,Ngo2012,Augustyn2012,Heimel2012,Cheng2013,RomanD.;Villaverde2014,Barahmand,Moniruzzaman2013,Sellam2013,Abramova2013,Grolinger2013,Sitaridi2013,Balkesen2013,rauh13,Bednar2013a,Ngo2013a,Guo2013b,Hsu2013,Teodoro2014,Faleiro2014,Model2014,Zhang2014,Ntarmos2014,Kllapi2014,Hu2014a,Ma2014,Peng2014,Wu2014a,Polychroniou2014,Walters2014a,Mirzoev2014a,zeng-sigmod14,Meister2015,faleiro2015fit,Trummer2015,bernstein2015scaling,Leis2015-JOB,Fegaras2015,Faleiro2015,Commons2015a,Melorose2015e,Chestnut2016,Abiteboul2017,YongjooParkAhmadShahabTajikMichaelCafarella2017,trummer2017solving,baca2017xml,ChenY2017-sigmod,Faerber2017,Leis2018,Makreshanski2018,Stefanoni:2018:ECC:3178876.3186003,Neumann2020,kleiner-sigkdd13,horvitz1952ht,Ling1999,Newberg2007,Owen2005,nirkhiwale-vldb13,Nirkhiwale2018,Chung2016,Doulkeridis2014a,zeng2014-abs,seli79,chen1994adaptive,Al-Kateb2007,Andrieu2002,efron94bootstrap,Neal2001,Quoc2018-spark,Bernardino2001,poosala99-DEB,encyc-aqp,chaudhuri2017approximate,10.5555/767141.767147,li2018approximate,astrahan1987,zeng2014approximation,2018arXiv181201823H,Quoc2018-approxjoin,wang2018aqp,peng2018aqp,gang96-biofocal,Berkeley2012,Agarwal2013,chernick2008a,efron1979,li2018-baq,Chen2015,Leis2017,Strobl2008,yu13-cs2,he2018demonstration,Haas2009,GibbonsP2001,laptev2012earl,Lu2002,zhao2020efficient,Jiang2009,dutt13efficiently,han2018efficiently,Ahn2017,archer08,Zhang2009,hou91-error,kipf19-deep,heltshe1983estimating,chung16-tods,chao1992estimating,bunge1993estimating,burnham1978estimation,grover2012extending,Chen2017,Au2001a,Hamze2004,Yu2015b,zhou-hermes18,Melchers1989,wang2019-improved,zong2018iht,Shin2015,Hu2014,vengerov2015join,acha99js,Huang2019,Agarwal2014,Albers2013,haas-97-sigmod,kipf2018learned,Zliobaite2010,Stillger2001,breidt2000local,Dean2004,su2018miss,kulessa2018,Walker2008,Huang2010,Borgs2013,Jin2006,gibb98,chao1984nonparametric,Yu2018,Yu2017,Chaudhuri2005,goodman1949estimation,gardy1984sizes,hansen1943hhest,Thomas1998,Hellerstein1997,Nath2010,Hasan2009,Wlodarczyk,Cai2019,hellerstein1993predicate,Tipping1999,Hou1989,dobra2002processing,van2017query,lipton90-query,Mccallum2011,fan-sigmod14,Olken1993,Zhao2018,KentanMu1991,Mykland1995,haas1999ripple,Wei2011,burnham1979robust,Joshi08,Vojnovic2012a,lohr-2000book,Diaconis2012,Leskovec2006,boicea2018sampling,Cochran1977,Sch,Haas1995,xu2012sampling,Wu2016,Jermaine2008,Wilson2019,Wick2010,Buck2011,efron2003,haas1996selectivity,Lynch1988,zhang2020selectivity,Yang2019,Haas1992-strat,Dobra,Zhang1989,Olken1986,olke86,Cohen1997,Dobra2004,Rusu2008-tods,Cormode2005-vldb,wang-skew-aware-2018,hou88,casella2002statistical,Levin2014,Cohen2011,cormode2011synopses,kleiner2012-blb,Alexandrov2014,Feng2012,Zhang2013a,Ting:2016:TOC:2939672.2939772,Richter2014,spie09tug,cohen2014uniform,Division2005,Gromping2009,park2018verdictdb,li-tods2019,LiFF16-wander,Efraimidis2006,Ringner2008a,Das2013</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>db:aqp</string>
		<key>keys</key>
		<string>Dobra,horvitz1952ht,Lynch1988,Hou1989,Hellerstein1997,poosala99-DEB,Ling1999,acha99js,breidt2000local,10.5555/767141.767147,Bernardino2001,Lu2002,dobra2002processing,Jin2006,Joshi08,Jermaine2008,cormode2011synopses,Berkeley2012,yu13-cs2,Agarwal2014,LiFF16-wander,van2017query,chaudhuri2017approximate,Quoc2018-approxjoin,kulessa2018,Zhao2018,su2018miss,han2018efficiently,wang-skew-aware-2018,zhou-hermes18,wang2018aqp,peng2018aqp,park2018verdictdb,he2018demonstration,li2018approximate,zong2018iht,li2018-baq,Quoc2018-spark,Cai2019,wang2019-improved,zhao2020efficient,zhang2020selectivity,dutt13efficiently,Agarwal2013,ChenY2017-sigmod,chung16-tods,encyc-aqp,estan2006end-biased,GibbonsP2001,Haas2009,li-tods2019,MendezMediavilla2010,Olken1993,Polychroniou2014,vengerov2015join,Wu2016,zeng-sigmod14</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>db:sampling</string>
		<key>keys</key>
		<string>kleiner-sigkdd13,Newberg2007,Owen2005,nirkhiwale-vldb13,Nirkhiwale2018,Chung2016,Doulkeridis2014a,zeng2014-abs,seli79,chen1994adaptive,Al-Kateb2007,Andrieu2002,efron94bootstrap,Neal2001,encyc-aqp,astrahan1987,zeng2014approximation,2018arXiv181201823H,gang96-biofocal,Agarwal2013,chernick2008a,efron1979,Chen2015,Leis2017,Strobl2008,Samplers2014,Haas2009,GibbonsP2001,laptev2012earl,Jiang2009,Ahn2017,archer08,estan2006end-biased,hou91-error,kipf19-deep,heltshe1983estimating,Stefanoni:2018:ECC:3178876.3186003,chung16-tods,chao1992estimating,bunge1993estimating,burnham1978estimation,grover2012extending,Chen2017,Au2001a,Hamze2004,Leis2015-JOB,Melchers1989,Shin2015,Hu2014,vengerov2015join,Huang2019,Albers2013,kipf2018learned,Zliobaite2010,Stillger2001,Dean2004,Walker2008,Huang2010,Borgs2013,gibb98,chao1984nonparametric,Chaudhuri2005,goodman1949estimation,gardy1984sizes,hansen1943hhest,Thomas1998,Nath2010,Hasan2009,Wlodarczyk,lipton90-practical,hellerstein1993predicate,Tipping1999,lipton90-query,Mccallum2011,fan-sigmod14,Olken1993,KentanMu1991,Mykland1995,pol-sigmod05,haas1999ripple,Wei2011,burnham1979robust,Vojnovic2012a,lohr-2000book,Diaconis2012,Leskovec2006,boicea2018sampling,Cochran1977,Sch,Haas1995,xu2012sampling,Wu2016,Wick2010,Buck2011,efron2003,haas1996selectivity,Yang2019,Haas1992-strat,Zhang1989,Olken1986,olke86,Cohen1997,Dobra2004,Rusu2008-tods,Cormode2005-vldb,hou88,casella2002statistical,Levin2014,Cohen2011,zeng-sigmod14,kleiner2012-blb,Alexandrov2014,Feng2012,Zhang2013a,Ting:2016:TOC:2939672.2939772,Richter2014,Alon1999-pods,spie09tug,ChenY2017-sigmod,cohen2014uniform,Division2005,Gromping2009,li-tods2019,Efraimidis2006,Ringner2008a,Das2013</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>db:sampling:big data</string>
		<key>keys</key>
		<string>Doulkeridis2014a,laptev2012earl,Vojnovic2012a,kleiner2012-blb,Alexandrov2014,Richter2014</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>db:sampling:book</string>
		<key>keys</key>
		<string></string>
	</dict>
	<dict>
		<key>group name</key>
		<string>paper:bs:ijca22-si</string>
		<key>keys</key>
		<string>efron1979,efron94bootstrap,efron2003,pol-sigmod05,chernick2008a,archer08,laptev2012earl,kleiner2012-blb,kleiner-sigkdd13,zeng2014-abs,zeng2014approximation,zeng-sigmod14,zhou-hermes18,haas-97-sigmod</string>
	</dict>
</array>
</plist>
}}

@comment{BibDesk Smart Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Author</string>
				<key>value</key>
				<string>Feng Yu</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Author</string>
				<key>value</key>
				<string>F. Yu</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>!!our_publications</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>bootstrap</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Title</string>
				<key>value</key>
				<string>bootstrap</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Abstract</string>
				<key>value</key>
				<string>bootstrap</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>bootstrap</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>sampling</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>sampling</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>aqp</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>sampling:aqp</string>
	</dict>
</array>
</plist>
}}
